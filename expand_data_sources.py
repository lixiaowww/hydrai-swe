#!/usr/bin/env python3
"""
æ•°æ®æ‰©å±•ä¸»è„šæœ¬
åè°ƒæ‰€æœ‰æ•°æ®æºçš„ä¸‹è½½å’Œåˆå¹¶ï¼Œè§£å†³æ•°æ®ä¸è¶³é—®é¢˜
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import time
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

class DataSourceExpander:
    """æ•°æ®æºæ‰©å±•å™¨"""
    
    def __init__(self):
        self.data_dir = "data"
        self.raw_dir = os.path.join(self.data_dir, "raw")
        self.processed_dir = os.path.join(self.data_dir, "processed")
        self.extended_dir = os.path.join(self.processed_dir, "extended")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.extended_dir, exist_ok=True)
        
        # æ•°æ®æºé…ç½®
        self.data_sources = {
            'nasa': {
                'name': 'NASAé›ªæ•°æ®',
                'script': 'download_nasa_snow_data.py',
                'description': 'SMAPã€AMSR2ã€GlobSnowç­‰å«æ˜Ÿé›ªæ°´å½“é‡æ•°æ®',
                'priority': 'high'
            },
            'era5': {
                'name': 'ERA5å†åˆ†ææ•°æ®',
                'script': 'download_era5_extended.py',
                'description': 'ECMWFå†åˆ†ææ°”è±¡ã€é›ªã€åœŸå£¤æ•°æ®',
                'priority': 'high'
            },
            'noaa': {
                'name': 'NOAAæ°”è±¡æ•°æ®',
                'script': 'download_noaa_extended.py',
                'description': 'ç¾å›½å›½å®¶æµ·æ´‹å’Œå¤§æ°”ç®¡ç†å±€æ°”è±¡æ•°æ®',
                'priority': 'medium'
            },
            'canada': {
                'name': 'åŠ æ‹¿å¤§ç¯å¢ƒæ•°æ®',
                'script': 'download_canada_environment.py',
                'description': 'åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨é›ªã€æ°”è±¡ã€æ°´æ–‡æ•°æ®',
                'priority': 'high'
            }
        }
        
        # ç›®æ ‡åŒºåŸŸï¼ˆManitobaé™„è¿‘ï¼‰
        self.target_region = {
            'name': 'Manitoba Region',
            'bbox': [-102.0, 49.0, -88.0, 60.0],  # [è¥¿, å—, ä¸œ, åŒ—]
            'center': [54.5, -95.0],  # [çº¬åº¦, ç»åº¦]
            'area_km2': 647797  # Manitobaé¢ç§¯
        }
    
    def analyze_current_data(self) -> Dict[str, Any]:
        """åˆ†æå½“å‰æ•°æ®çŠ¶å†µ"""
        print("ğŸ” åˆ†æå½“å‰æ•°æ®çŠ¶å†µ")
        print("=" * 50)
        
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'current_data_sources': [],
            'data_volume': {},
            'time_coverage': {},
            'quality_issues': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥ç°æœ‰æ•°æ®
        existing_datasets = [
            'eccc_manitoba_snow_processed.csv',
            'hydat_streamflow_processed.csv',
            'comprehensive_training_dataset.csv',
            'standardized_training_dataset.csv'
        ]
        
        total_records = 0
        total_size_mb = 0
        
        for dataset in existing_datasets:
            filepath = os.path.join(self.processed_dir, dataset)
            if os.path.exists(filepath):
                try:
                    # è¯»å–æ•°æ®
                    df = pd.read_csv(filepath, index_col=0, parse_dates=True)
                    records = len(df)
                    size_mb = os.path.getsize(filepath) / 1024 / 1024
                    
                    # æ—¶é—´èŒƒå›´
                    if 'date' in df.columns:
                        date_col = 'date'
                    else:
                        date_col = df.index.name if df.index.name else df.index[0]
                    
                    if pd.api.types.is_datetime64_any_dtype(df[date_col]):
                        time_range = f"{df[date_col].min()} - {df[date_col].max()}"
                        days_covered = (df[date_col].max() - df[date_col].min()).days
                    else:
                        time_range = "Unknown"
                        days_covered = 0
                    
                    dataset_info = {
                        'name': dataset,
                        'records': records,
                        'size_mb': size_mb,
                        'time_range': time_range,
                        'days_covered': days_covered
                    }
                    
                    analysis['current_data_sources'].append(dataset_info)
                    total_records += records
                    total_size_mb += size_mb
                    
                    print(f"âœ… {dataset}: {records:,} æ¡è®°å½•, {size_mb:.2f} MB, {time_range}")
                    
                except Exception as e:
                    print(f"âŒ è¯»å– {dataset} å¤±è´¥: {e}")
                    analysis['quality_issues'].append(f"æ— æ³•è¯»å– {dataset}: {e}")
        
        # æ€»ä½“ç»Ÿè®¡
        analysis['data_volume'] = {
            'total_records': total_records,
            'total_size_mb': total_size_mb,
            'datasets_count': len(analysis['current_data_sources'])
        }
        
        # æ•°æ®è´¨é‡è¯„ä¼°
        if total_records < 10000:
            analysis['quality_issues'].append("æ•°æ®é‡ä¸è¶³ï¼šæ€»è®°å½•æ•°å°‘äº10,000æ¡")
            analysis['recommendations'].append("éœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®æº")
        
        if total_records < 50000:
            analysis['quality_issues'].append("æ•°æ®é‡æœ‰é™ï¼šæ€»è®°å½•æ•°å°‘äº50,000æ¡")
            analysis['recommendations'].append("å»ºè®®æ‰©å±•åˆ°æ›´å¤šå¹´ä»½å’Œåœ°åŒº")
        
        # æ—¶é—´è¦†ç›–è¯„ä¼°
        if analysis['current_data_sources']:
            max_days = max([ds['days_covered'] for ds in analysis['current_data_sources']])
            if max_days < 365 * 10:  # å°‘äº10å¹´
                analysis['quality_issues'].append("æ—¶é—´è¦†ç›–ä¸è¶³ï¼šå°‘äº10å¹´çš„æ•°æ®")
                analysis['recommendations'].append("éœ€è¦æ›´é•¿æœŸçš„å†å²æ•°æ®")
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {total_records:,}")
        print(f"   æ€»å¤§å°: {total_size_mb:.2f} MB")
        print(f"   æ•°æ®é›†æ•°é‡: {len(analysis['current_data_sources'])}")
        
        return analysis
    
    def check_data_source_availability(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®æºå¯ç”¨æ€§"""
        print("\nğŸ” æ£€æŸ¥æ•°æ®æºå¯ç”¨æ€§")
        print("=" * 50)
        
        availability = {
            'timestamp': datetime.now().isoformat(),
            'sources': {}
        }
        
        for source_key, source_info in self.data_sources.items():
            print(f"\nğŸ¯ æ£€æŸ¥æ•°æ®æº: {source_info['name']}")
            
            # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
            script_path = source_info['script']
            script_exists = os.path.exists(script_path)
            
            # æ£€æŸ¥ä¾èµ–
            dependencies = self._check_dependencies(source_key)
            
            # æ£€æŸ¥é…ç½®
            config_status = self._check_configuration(source_key)
            
            source_status = {
                'name': source_info['name'],
                'priority': source_info['priority'],
                'script_exists': script_exists,
                'dependencies_met': dependencies['all_met'],
                'config_ready': config_status['ready'],
                'dependencies': dependencies,
                'config': config_status,
                'status': 'ready' if script_exists and dependencies['all_met'] and config_status['ready'] else 'not_ready'
            }
            
            availability['sources'][source_key] = source_status
            
            # æ˜¾ç¤ºçŠ¶æ€
            status_emoji = "âœ…" if source_status['status'] == 'ready' else "âŒ"
            print(f"   {status_emoji} çŠ¶æ€: {source_status['status']}")
            print(f"   è„šæœ¬: {'âœ…' if script_exists else 'âŒ'} {script_path}")
            print(f"   ä¾èµ–: {'âœ…' if dependencies['all_met'] else 'âŒ'} {dependencies['missing'] if dependencies['missing'] else 'å…¨éƒ¨æ»¡è¶³'}")
            print(f"   é…ç½®: {'âœ…' if config_status['ready'] else 'âŒ'} {config_status['issues'] if config_status['issues'] else 'é…ç½®æ­£ç¡®'}")
        
        return availability
    
    def _check_dependencies(self, source_key: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®æºä¾èµ–"""
        dependencies = {
            'all_met': True,
            'missing': [],
            'details': {}
        }
        
        if source_key == 'nasa':
            # NASAæ•°æ®æºä¾èµ–
            required_packages = ['requests', 'netCDF4', 'xarray']
            for package in required_packages:
                try:
                    __import__(package)
                    dependencies['details'][package] = 'installed'
                except ImportError:
                    dependencies['details'][package] = 'missing'
                    dependencies['missing'].append(package)
                    dependencies['all_met'] = False
        
        elif source_key == 'era5':
            # ERA5æ•°æ®æºä¾èµ–
            required_packages = ['cdsapi', 'netCDF4', 'xarray']
            for package in required_packages:
                try:
                    __import__(package)
                    dependencies['details'][package] = 'installed'
                except ImportError:
                    dependencies['details'][package] = 'missing'
                    dependencies['missing'].append(package)
                    dependencies['all_met'] = False
        
        return dependencies
    
    def _check_configuration(self, source_key: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®æºé…ç½®"""
        config = {
            'ready': False,
            'issues': [],
            'details': {}
        }
        
        if source_key == 'nasa':
            # NASAæ•°æ®æºé…ç½®æ£€æŸ¥
            config['ready'] = True  # æš‚æ—¶ä¸éœ€è¦ç‰¹æ®Šé…ç½®
            config['details']['api_key'] = 'not_required'
        
        elif source_key == 'era5':
            # ERA5æ•°æ®æºé…ç½®æ£€æŸ¥
            cds_config_path = os.path.expanduser("~/.cdsapirc")
            if os.path.exists(cds_config_path):
                config['ready'] = True
                config['details']['cds_config'] = 'found'
            else:
                config['ready'] = False
                config['issues'].append("ç¼ºå°‘CDS APIé…ç½®æ–‡ä»¶")
                config['details']['cds_config'] = 'missing'
        
        return config
    
    def download_data_source(self, source_key: str) -> bool:
        """ä¸‹è½½æŒ‡å®šæ•°æ®æº"""
        if source_key not in self.data_sources:
            print(f"âŒ æœªçŸ¥æ•°æ®æº: {source_key}")
            return False
        
        source_info = self.data_sources[source_key]
        script_path = source_info['script']
        
        if not os.path.exists(script_path):
            print(f"âŒ è„šæœ¬ä¸å­˜åœ¨: {script_path}")
            return False
        
        print(f"ğŸš€ å¼€å§‹ä¸‹è½½æ•°æ®æº: {source_info['name']}")
        print("=" * 50)
        
        try:
            # æ‰§è¡Œä¸‹è½½è„šæœ¬
            import subprocess
            result = subprocess.run([sys.executable, script_path], 
                                 capture_output=True, text=True, timeout=3600)
            
            if result.returncode == 0:
                print(f"âœ… æ•°æ®æºä¸‹è½½æˆåŠŸ: {source_info['name']}")
                print(f"   è¾“å‡º: {result.stdout}")
                return True
            else:
                print(f"âŒ æ•°æ®æºä¸‹è½½å¤±è´¥: {source_info['name']}")
                print(f"   é”™è¯¯: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"âŒ æ•°æ®æºä¸‹è½½è¶…æ—¶: {source_info['name']}")
            return False
        except Exception as e:
            print(f"âŒ æ•°æ®æºä¸‹è½½å¼‚å¸¸: {source_info['name']}: {e}")
            return False
    
    def merge_all_extended_data(self) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰æ‰©å±•æ•°æ®"""
        print("\nğŸ”„ åˆå¹¶æ‰€æœ‰æ‰©å±•æ•°æ®")
        print("=" * 50)
        
        all_data = []
        
        # è¯»å–ç°æœ‰æ•°æ®
        existing_datasets = [
            'eccc_manitoba_snow_processed.csv',
            'hydat_streamflow_processed.csv'
        ]
        
        for dataset in existing_datasets:
            filepath = os.path.join(self.processed_dir, dataset)
            if os.path.exists(filepath):
                try:
                    df = pd.read_csv(filepath, index_col=0, parse_dates=True)
                    df['data_source'] = dataset.replace('_processed.csv', '').upper()
                    all_data.append(df)
                    print(f"âœ… åŠ è½½ç°æœ‰æ•°æ®: {dataset} ({len(df)} æ¡è®°å½•)")
                except Exception as e:
                    print(f"âŒ åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {dataset}: {e}")
        
        # è¯»å–æ‰©å±•æ•°æ®
        extended_sources = ['nasa_snow', 'era5_extended']
        
        for source in extended_sources:
            source_dir = os.path.join(self.processed_dir, source)
            if os.path.exists(source_dir):
                csv_files = [f for f in os.listdir(source_dir) if f.endswith('.csv')]
                for csv_file in csv_files:
                    filepath = os.path.join(source_dir, csv_file)
                    try:
                        df = pd.read_csv(filepath, index_col=0, parse_dates=True)
                        df['data_source'] = source.upper()
                        all_data.append(df)
                        print(f"âœ… åŠ è½½æ‰©å±•æ•°æ®: {csv_file} ({len(df)} æ¡è®°å½•)")
                    except Exception as e:
                        print(f"âŒ åŠ è½½æ‰©å±•æ•°æ®å¤±è´¥: {csv_file}: {e}")
        
        if not all_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ•°æ®
        print(f"\nğŸ”„ å¼€å§‹åˆå¹¶ {len(all_data)} ä¸ªæ•°æ®é›†...")
        
        # æ ‡å‡†åŒ–åˆ—å
        standardized_data = []
        for df in all_data:
            # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
            column_mapping = {
                'snow_depth_mm': 'snow_depth_mm',
                'snow_fall_mm': 'snow_fall_mm',
                'snow_water_equivalent_mm': 'snow_water_equivalent_mm',
                'snow_depth': 'snow_depth_mm',
                'snowfall': 'snow_fall_mm',
                'snow_depth_water_equivalent': 'snow_water_equivalent_mm'
            }
            
            df_renamed = df.rename(columns=column_mapping)
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_columns = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm']
            for col in required_columns:
                if col not in df_renamed.columns:
                    df_renamed[col] = np.nan
            
            standardized_data.append(df_renamed)
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_data = pd.concat(standardized_data, ignore_index=True)
        
        # å»é‡å’Œæ’åº
        if 'date' in merged_data.columns:
            merged_data = merged_data.drop_duplicates(subset=['date']).sort_values('date')
        
        print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_data)} æ¡è®°å½•")
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"extended_comprehensive_dataset_{timestamp}.csv"
        output_path = os.path.join(self.extended_dir, output_file)
        
        merged_data.to_csv(output_path, index=False)
        print(f"âœ… æ‰©å±•æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
        
        return merged_data
    
    def generate_expansion_report(self, analysis: Dict[str, Any], 
                                availability: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‰©å±•æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'target_region': self.target_region,
            'current_data_analysis': analysis,
            'data_source_availability': availability,
            'expansion_plan': self._generate_expansion_plan(analysis, availability),
            'recommendations': []
        }
        
        # ç”Ÿæˆå»ºè®®
        if analysis['data_volume']['total_records'] < 50000:
            report['recommendations'].append("ç«‹å³å¼€å§‹ä¸‹è½½é«˜ä¼˜å…ˆçº§æ•°æ®æº")
        
        if not any(source['status'] == 'ready' for source in availability['sources'].values()):
            report['recommendations'].append("ä¼˜å…ˆè§£å†³ä¾èµ–å’Œé…ç½®é—®é¢˜")
        
        report['recommendations'].extend(analysis['recommendations'])
        
        return report
    
    def _generate_expansion_plan(self, analysis: Dict[str, Any], 
                               availability: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‰©å±•è®¡åˆ’"""
        plan = {
            'immediate_actions': [],
            'short_term_goals': [],
            'long_term_goals': [],
            'priority_order': []
        }
        
        # ç«‹å³è¡ŒåŠ¨
        ready_sources = [key for key, source in availability['sources'].items() 
                        if source['status'] == 'ready']
        
        if ready_sources:
            plan['immediate_actions'].append(f"å¼€å§‹ä¸‹è½½å°±ç»ªçš„æ•°æ®æº: {', '.join(ready_sources)}")
        
        # çŸ­æœŸç›®æ ‡
        if analysis['data_volume']['total_records'] < 100000:
            plan['short_term_goals'].append("å°†æ•°æ®é‡æ‰©å±•åˆ°100,000æ¡è®°å½•ä»¥ä¸Š")
        
        # é•¿æœŸç›®æ ‡
        plan['long_term_goals'].append("å»ºç«‹å¤šæºã€é•¿æœŸã€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†")
        plan['long_term_goals'].append("å®ç°æ•°æ®çš„è‡ªåŠ¨æ›´æ–°å’Œç»´æŠ¤")
        
        # ä¼˜å…ˆçº§é¡ºåº
        high_priority = [key for key, source in availability['sources'].items() 
                        if source['priority'] == 'high' and source['status'] == 'ready']
        medium_priority = [key for key, source in availability['sources'].items() 
                          if source['priority'] == 'medium' and source['status'] == 'ready']
        
        plan['priority_order'] = high_priority + medium_priority
        
        return plan

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ•°æ®æºæ‰©å±•å™¨å¯åŠ¨")
    print("=" * 50)
    
    expander = DataSourceExpander()
    
    # åˆ†æå½“å‰æ•°æ®çŠ¶å†µ
    analysis = expander.analyze_current_data()
    
    # æ£€æŸ¥æ•°æ®æºå¯ç”¨æ€§
    availability = expander.check_data_source_availability()
    
    # ç”Ÿæˆæ‰©å±•æŠ¥å‘Š
    report = expander.generate_expansion_report(analysis, availability)
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(expander.extended_dir, f"expansion_report_{timestamp}.json")
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    print(f"\nğŸ“Š æ‰©å±•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æ˜¾ç¤ºæ‰©å±•è®¡åˆ’
    print(f"\nğŸ“‹ æ•°æ®æ‰©å±•è®¡åˆ’:")
    print(f"   ç«‹å³è¡ŒåŠ¨: {report['expansion_plan']['immediate_actions']}")
    print(f"   çŸ­æœŸç›®æ ‡: {report['expansion_plan']['short_term_goals']}")
    print(f"   é•¿æœŸç›®æ ‡: {report['expansion_plan']['long_term_goals']}")
    print(f"   ä¼˜å…ˆçº§é¡ºåº: {report['expansion_plan']['priority_order']}")
    
    # è¯¢é—®æ˜¯å¦å¼€å§‹ä¸‹è½½
    if report['expansion_plan']['immediate_actions']:
        print(f"\nâ“ æ˜¯å¦å¼€å§‹ä¸‹è½½æ•°æ®æºï¼Ÿ")
        print(f"   å°±ç»ªçš„æ•°æ®æº: {[key for key, source in availability['sources'].items() if source['status'] == 'ready']}")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç”¨æˆ·äº¤äº’é€»è¾‘
        # æš‚æ—¶è‡ªåŠ¨å¼€å§‹ä¸‹è½½é«˜ä¼˜å…ˆçº§æ•°æ®æº
        
        print(f"\nğŸš€ è‡ªåŠ¨å¼€å§‹ä¸‹è½½é«˜ä¼˜å…ˆçº§æ•°æ®æº...")
        
        for source_key in report['expansion_plan']['priority_order'][:2]:  # é™åˆ¶ä¸ºå‰2ä¸ª
            print(f"\nğŸ¯ ä¸‹è½½æ•°æ®æº: {source_key}")
            success = expander.download_data_source(source_key)
            if success:
                print(f"âœ… {source_key} ä¸‹è½½å®Œæˆ")
            else:
                print(f"âŒ {source_key} ä¸‹è½½å¤±è´¥")
        
        # åˆå¹¶æ‰©å±•æ•°æ®
        print(f"\nğŸ”„ å¼€å§‹åˆå¹¶æ‰©å±•æ•°æ®...")
        merged_data = expander.merge_all_extended_data()
        
        if not merged_data.empty:
            print(f"ğŸ‰ æ•°æ®æ‰©å±•å®Œæˆï¼")
            print(f"   æœ€ç»ˆæ•°æ®é›†å¤§å°: {len(merged_data):,} æ¡è®°å½•")
        else:
            print(f"âŒ æ•°æ®æ‰©å±•å¤±è´¥")
    
    else:
        print(f"\nâš ï¸ æ²¡æœ‰å°±ç»ªçš„æ•°æ®æºï¼Œè¯·å…ˆè§£å†³ä¾èµ–å’Œé…ç½®é—®é¢˜")

if __name__ == "__main__":
    main()

