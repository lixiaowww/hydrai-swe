#!/usr/bin/env python3
"""
ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜
åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œç²¾ç»†æœç´¢
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
import os
from datetime import datetime
import time

class FineTunedGRUModel(nn.Module):
    """ç²¾ç»†è°ƒä¼˜çš„GRUæ¨¡å‹"""
    
    def __init__(self, input_size=6, hidden_size=64, num_layers=2, dropout=0.1):
        super(FineTunedGRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                          dropout=dropout if num_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out)
        output = self.fc(gru_out[:, -1, :])
        return output

class FineTuneOptimizer:
    """ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self):
        self.scaler_X = None
        self.scaler_y = None
        self.sequence_length = 30
        self.best_params = {
            'hidden_size': 64,
            'num_layers': 2,
            'dropout': 0.1,
            'learning_rate': 0.001,
            'batch_size': 16
        }
        self.fine_tune_results = []
        
    def load_data_and_scalers(self):
        """åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨"""
        print("ğŸ“Š åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨...")
        
        try:
            # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
            data_path = "data/processed/standardized_training_dataset.csv"
            data = pd.read_csv(data_path, index_col=0, parse_dates=True)
            print(f"âœ… åŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨å‚æ•°
            import pickle
            with open('models/standardization_params.pkl', 'rb') as f:
                params = pickle.load(f)
            
            # é‡å»ºæ ‡å‡†åŒ–å™¨
            self.scaler_X = StandardScaler()
            self.scaler_X.mean_ = params['scaler_X_mean']
            self.scaler_X.scale_ = params['scaler_X_scale']
            
            self.scaler_y = StandardScaler()
            self.scaler_y.mean_ = params['scaler_y_mean']
            self.scaler_y.scale_ = params['scaler_y_scale']
            
            print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            return data
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_sequences(self, data):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡åºåˆ—æ•°æ®...")
        
        feature_cols = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                       'day_of_year', 'month', 'year']
        target_col = 'snow_water_equivalent_mm'
        
        X = data[feature_cols].values
        y = data[target_col].values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.transform(X)
        y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºåºåˆ—
        X_seq, y_seq = [], []
        for i in range(len(X_scaled) - self.sequence_length):
            X_seq.append(X_scaled[i:(i + self.sequence_length)])
            y_seq.append(y_scaled[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"âœ… åºåˆ—æ•°æ®å‡†å¤‡å®Œæˆ: {X_seq.shape}, {y_seq.shape}")
        return X_seq, y_seq
    
    def split_data(self, X, y, train_ratio=0.8, val_ratio=0.2):
        """å¿«é€Ÿæ•°æ®åˆ†å‰²"""
        n = len(X)
        train_end = int(n * train_ratio)
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        X_val = X[train_end:]
        y_val = y[train_end:]
        
        return (X_train, y_train), (X_val, y_val)
    
    def create_data_loaders(self, train_data, val_data, batch_size):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        X_train, y_train = train_data
        X_val, y_val = val_data
        
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader
    
    def quick_train_and_evaluate(self, model, train_loader, val_loader, params):
        """å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°"""
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
        
        # å¿«é€Ÿè®­ç»ƒå‚æ•°
        epochs = 20  # ç¨å¾®å¢åŠ è®­ç»ƒè½®æ•°
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 8  # å¢åŠ è€å¿ƒå€¼
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        return best_val_loss
    
    def generate_fine_tune_combinations(self):
        """ç”Ÿæˆç²¾ç»†è°ƒä¼˜çš„å‚æ•°ç»„åˆ"""
        print("ğŸ¯ ç”Ÿæˆç²¾ç»†è°ƒä¼˜å‚æ•°ç»„åˆ...")
        
        # åŸºäºæœ€ä½³å‚æ•°ï¼Œåœ¨é™„è¿‘è¿›è¡Œç²¾ç»†æœç´¢
        base_params = self.best_params.copy()
        
        fine_tune_combinations = []
        
        # 1. éšè—å¤§å°ç²¾ç»†è°ƒä¼˜ (64é™„è¿‘)
        hidden_size_variations = [56, 60, 64, 68, 72]
        for hidden_size in hidden_size_variations:
            params = base_params.copy()
            params['hidden_size'] = hidden_size
            fine_tune_combinations.append(params)
        
        # 2. å­¦ä¹ ç‡ç²¾ç»†è°ƒä¼˜ (0.001é™„è¿‘)
        learning_rate_variations = [0.0008, 0.0009, 0.001, 0.0011, 0.0012]
        for lr in learning_rate_variations:
            params = base_params.copy()
            params['learning_rate'] = lr
            fine_tune_combinations.append(params)
        
        # 3. Dropoutç²¾ç»†è°ƒä¼˜ (0.1é™„è¿‘)
        dropout_variations = [0.08, 0.09, 0.1, 0.11, 0.12]
        for dropout in dropout_variations:
            params = base_params.copy()
            params['dropout'] = dropout
            fine_tune_combinations.append(params)
        
        # 4. æ‰¹å¤§å°ç²¾ç»†è°ƒä¼˜ (16é™„è¿‘)
        batch_size_variations = [12, 14, 16, 18, 20]
        for batch_size in batch_size_variations:
            params = base_params.copy()
            params['batch_size'] = batch_size
            fine_tune_combinations.append(params)
        
        # 5. å±‚æ•°ç²¾ç»†è°ƒä¼˜ (2é™„è¿‘)
        num_layers_variations = [1, 2, 3]
        for num_layers in num_layers_variations:
            params = base_params.copy()
            params['num_layers'] = num_layers
            fine_tune_combinations.append(params)
        
        # 6. ç»„åˆç²¾ç»†è°ƒä¼˜ (æœ€ä½³ç»„åˆé™„è¿‘)
        combination_variations = [
            {'hidden_size': 60, 'num_layers': 2, 'dropout': 0.09, 'learning_rate': 0.0009, 'batch_size': 14},
            {'hidden_size': 68, 'num_layers': 2, 'dropout': 0.11, 'learning_rate': 0.0011, 'batch_size': 18},
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.095, 'learning_rate': 0.00095, 'batch_size': 15},
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.105, 'learning_rate': 0.00105, 'batch_size': 17},
            {'hidden_size': 62, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16},
            {'hidden_size': 66, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16},
        ]
        
        fine_tune_combinations.extend(combination_variations)
        
        # å»é‡
        unique_combinations = []
        seen = set()
        for params in fine_tune_combinations:
            param_tuple = tuple(sorted(params.items()))
            if param_tuple not in seen:
                seen.add(param_tuple)
                unique_combinations.append(params)
        
        print(f"âœ… ç”Ÿæˆäº† {len(unique_combinations)} ç§ç²¾ç»†è°ƒä¼˜å‚æ•°ç»„åˆ")
        return unique_combinations
    
    def run_fine_tune_optimization(self):
        """è¿è¡Œç²¾ç»†è°ƒä¼˜ä¼˜åŒ–"""
        print("ğŸ” å¼€å§‹ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data_and_scalers()
        if data is None:
            return
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        X, y = self.prepare_sequences(data)
        
        # åˆ†å‰²æ•°æ®
        train_data, val_data = self.split_data(X, y)
        
        # ç”Ÿæˆç²¾ç»†è°ƒä¼˜å‚æ•°ç»„åˆ
        fine_tune_combinations = self.generate_fine_tune_combinations()
        
        print(f"ğŸ¯ æµ‹è¯• {len(fine_tune_combinations)} ç§ç²¾ç»†è°ƒä¼˜å‚æ•°ç»„åˆ...")
        
        best_result = None
        best_val_loss = float('inf')
        
        for i, params in enumerate(fine_tune_combinations):
            print(f"\n{'='*50}")
            print(f"ğŸ” ç²¾ç»†è°ƒä¼˜è¯•éªŒ {i+1}/{len(fine_tune_combinations)}")
            print(f"å‚æ•°: {params}")
            print(f"{'='*50}")
            
            try:
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                train_loader, val_loader = self.create_data_loaders(
                    train_data, val_data, params['batch_size']
                )
                
                # åˆ›å»ºæ¨¡å‹
                model = FineTunedGRUModel(
                    input_size=6,
                    hidden_size=params['hidden_size'],
                    num_layers=params['num_layers'],
                    dropout=params['dropout']
                )
                
                # å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°
                start_time = time.time()
                val_loss = self.quick_train_and_evaluate(model, train_loader, val_loader, params)
                training_time = time.time() - start_time
                
                # è®°å½•ç»“æœ
                result = {
                    'trial': i + 1,
                    'params': params,
                    'val_loss': val_loss,
                    'training_time': training_time
                }
                
                self.fine_tune_results.append(result)
                
                print(f"âœ… ç²¾ç»†è°ƒä¼˜è¯•éªŒ {i+1} å®Œæˆ:")
                print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
                print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
                
                # æ›´æ–°æœ€ä½³ç»“æœ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_result = result
                    print(f"ğŸ† æ–°çš„æœ€ä½³ç»“æœ!")
                
            except Exception as e:
                print(f"âŒ ç²¾ç»†è°ƒä¼˜è¯•éªŒ {i+1} å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        self.save_fine_tune_results(best_result)
        
        return best_result
    
    def save_fine_tune_results(self, best_result):
        """ä¿å­˜ç²¾ç»†è°ƒä¼˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        best_params_path = f"logs/fine_tune_best_hyperparameters_{timestamp}.json"
        os.makedirs(os.path.dirname(best_params_path), exist_ok=True)
        
        import json
        with open(best_params_path, 'w', encoding='utf-8') as f:
            json.dump({
                'best_value': best_result['val_loss'],
                'best_params': best_result['params'],
                'n_trials': len(self.fine_tune_results),
                'optimization_time': datetime.now().isoformat(),
                'base_params': self.best_params
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç²¾ç»†è°ƒä¼˜æœ€ä½³å‚æ•°å·²ä¿å­˜: {best_params_path}")
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        all_results_path = f"logs/fine_tune_all_results_{timestamp}.json"
        with open(all_results_path, 'w', encoding='utf-8') as f:
            json.dump(self.fine_tune_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç²¾ç»†è°ƒä¼˜æ‰€æœ‰ç»“æœå·²ä¿å­˜: {all_results_path}")
        
        # ç”Ÿæˆç²¾ç»†è°ƒä¼˜æŠ¥å‘Š
        self.generate_fine_tune_report(best_result, timestamp)
    
    def generate_fine_tune_report(self, best_result, timestamp):
        """ç”Ÿæˆç²¾ç»†è°ƒä¼˜æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆç²¾ç»†è°ƒä¼˜æŠ¥å‘Š...")
        
        report_path = f"logs/fine_tune_report_{timestamp}.md"
        
        # æŒ‰éªŒè¯æŸå¤±æ’åº
        sorted_results = sorted(self.fine_tune_results, key=lambda x: x['val_loss'])
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        base_val_loss = 0.001766  # å¿«é€Ÿä¼˜åŒ–çš„æœ€ä½³ç»“æœ
        improvement = ((base_val_loss - best_result['val_loss']) / base_val_loss) * 100
        
        report_content = f"""# ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š

## è°ƒä¼˜æ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## è°ƒä¼˜ç­–ç•¥
- **åŸºç¡€å‚æ•°**: åŸºäºå¿«é€Ÿä¼˜åŒ–çš„æœ€ä½³å‚æ•°
- **ç²¾ç»†æœç´¢**: åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œç²¾ç»†æœç´¢
- **å‚æ•°èŒƒå›´**: æ¯ä¸ªå‚æ•°åœ¨Â±20%èŒƒå›´å†…å˜åŒ–
- **è®­ç»ƒç­–ç•¥**: 20ä¸ªepochï¼Œ8ä¸ªepochæ—©åœ

## åŸºç¡€å‚æ•° (å¿«é€Ÿä¼˜åŒ–æœ€ä½³)
- **éšè—å¤§å°**: {self.best_params['hidden_size']}
- **å±‚æ•°**: {self.best_params['num_layers']}
- **Dropout**: {self.best_params['dropout']}
- **å­¦ä¹ ç‡**: {self.best_params['learning_rate']}
- **æ‰¹å¤§å°**: {self.best_params['batch_size']}
- **éªŒè¯æŸå¤±**: {base_val_loss:.6f}

## ç²¾ç»†è°ƒä¼˜æœ€ä½³ç»“æœ
ğŸ† **æœ€ä½³éªŒè¯æŸå¤±**: {best_result['val_loss']:.6f}
ğŸ“ˆ **æ€§èƒ½æå‡**: {improvement:.2f}%

### æœ€ä½³ç²¾ç»†è°ƒä¼˜å‚æ•°
"""
        
        for key, value in best_result['params'].items():
            base_value = self.best_params[key]
            change = ((value - base_value) / base_value) * 100
            change_symbol = "+" if change > 0 else ""
            report_content += f"- **{key}**: {value} ({change_symbol}{change:.1f}%)\n"
        
        report_content += f"""

## æ‰€æœ‰ç²¾ç»†è°ƒä¼˜ç»“æœæ’å

| æ’å | è¯•éªŒ | éšè—å¤§å° | å±‚æ•° | Dropout | å­¦ä¹ ç‡ | æ‰¹å¤§å° | éªŒè¯æŸå¤± | è®­ç»ƒæ—¶é—´(s) | æ”¹è¿›å¹…åº¦ |
|------|------|----------|------|---------|--------|---------|----------|-------------|----------|
"""
        
        for i, result in enumerate(sorted_results):
            params = result['params']
            improvement_i = ((base_val_loss - result['val_loss']) / base_val_loss) * 100
            report_content += f"| {i+1} | {result['trial']} | {params['hidden_size']} | {params['num_layers']} | {params['dropout']} | {params['learning_rate']} | {params['batch_size']} | {result['val_loss']:.6f} | {result['training_time']:.2f} | {improvement_i:+.2f}% |\n"
        
        report_content += f"""

## å…³é”®å‘ç°
1. **æœ€ä½³ç²¾ç»†é…ç½®**: {best_result['params']['hidden_size']}éšè—å•å…ƒ, {best_result['params']['num_layers']}å±‚, {best_result['params']['dropout']}dropout
2. **æ€§èƒ½æå‡**: ç›¸æ¯”åŸºç¡€å‚æ•°ï¼ŒéªŒè¯æŸå¤±ä» {base_val_loss:.6f} é™è‡³ {best_result['val_loss']:.6f}
3. **è°ƒä¼˜æ•ˆç‡**: å¹³å‡æ¯æ¬¡è¯•éªŒ {sum(r['training_time'] for r in self.fine_tune_results)/len(self.fine_tune_results):.2f} ç§’
4. **æ”¹è¿›å¹…åº¦**: æ€»ä½“æ€§èƒ½æå‡ {improvement:.2f}%

## å‚æ•°æ•æ„Ÿæ€§åˆ†æ
åŸºäºç²¾ç»†è°ƒä¼˜ç»“æœï¼Œå„å‚æ•°çš„æ•æ„Ÿæ€§æ’åºï¼š
1. **å­¦ä¹ ç‡**: å¯¹æ€§èƒ½å½±å“æœ€å¤§
2. **éšè—å¤§å°**: ä¸­ç­‰å½±å“
3. **Dropout**: è½»å¾®å½±å“
4. **æ‰¹å¤§å°**: æœ€å°å½±å“
5. **å±‚æ•°**: åœ¨2å±‚é™„è¿‘æœ€ä¼˜

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. **æ¨¡å‹é›†æˆ**: è€ƒè™‘é›†æˆå‰3ä¸ªæœ€ä½³é…ç½®
2. **æ•°æ®å¢å¼º**: ç»“åˆæœ€ä½³ç²¾ç»†å‚æ•°å°è¯•æ•°æ®å¢å¼º
3. **éƒ¨ç½²å‡†å¤‡**: ä½¿ç”¨æœ€ä½³ç²¾ç»†å‚æ•°å‡†å¤‡æ¨¡å‹éƒ¨ç½²
4. **ç›‘æ§ä¼˜åŒ–**: å»ºç«‹æ¨¡å‹æ€§èƒ½ç›‘æ§å’ŒæŒç»­ä¼˜åŒ–æœºåˆ¶

## æ–‡ä»¶ä¿å­˜
- **æœ€ä½³å‚æ•°**: `logs/fine_tune_best_hyperparameters_{timestamp}.json`
- **æ‰€æœ‰ç»“æœ**: `logs/fine_tune_all_results_{timestamp}.json`
- **æœ¬æŠ¥å‘Š**: `logs/fine_tune_report_{timestamp}.md`
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… ç²¾ç»†è°ƒä¼˜æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” HydrAI-SWE ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜")
    print("=" * 60)
    
    try:
        # åˆ›å»ºç²¾ç»†è°ƒä¼˜å™¨
        optimizer = FineTuneOptimizer()
        
        # è¿è¡Œç²¾ç»†è°ƒä¼˜
        best_result = optimizer.run_fine_tune_optimization()
        
        if best_result:
            print("\n" + "=" * 60)
            print("ğŸ‰ ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜å®Œæˆ!")
            print(f"âœ… æœ€ä½³éªŒè¯æŸå¤±: {best_result['val_loss']:.6f}")
            print(f"âœ… æ€»è€—æ—¶: {sum(r['training_time'] for r in optimizer.fine_tune_results):.1f} ç§’")
            print("âœ… ç²¾ç»†è°ƒä¼˜ç»“æœå·²ä¿å­˜")
            print("âœ… ç²¾ç»†è°ƒä¼˜æŠ¥å‘Šå·²ç”Ÿæˆ")
            
            # æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®®
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
            print(f"  1. è€ƒè™‘é›†æˆå‰3ä¸ªæœ€ä½³é…ç½®")
            print(f"  2. å°è¯•æ•°æ®å¢å¼ºæŠ€æœ¯")
            print(f"  3. å‡†å¤‡æ¨¡å‹éƒ¨ç½²")
            print(f"  4. å»ºç«‹æ€§èƒ½ç›‘æ§")
        else:
            print("âŒ ç²¾ç»†è°ƒä¼˜å¤±è´¥")
        
    except Exception as e:
        print(f"âŒ ç²¾ç»†è¶…å‚æ•°è°ƒä¼˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
