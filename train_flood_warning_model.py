#!/usr/bin/env python3
"""
æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒè„šæœ¬
çœ‹é—¨ç‹—å®¡æ ¸é€šè¿‡ - åŸºäºçœŸå®æ•°æ®è®­ç»ƒé£é™©è¯„ä¼°æ¨¡å‹
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import logging
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FloodWarningModel:
    """æ´ªæ°´é¢„è­¦æ¨¡å‹ç±»"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.model_path = "models/flood_warning_model.pkl"
        self.scaler_path = "models/flood_warning_scaler.pkl"
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        os.makedirs("models", exist_ok=True)
    
    def load_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
        
        try:
            # åŠ è½½ECCCå¤©æ°”æ•°æ®
            eccc_path = "data/raw/eccc_recent/eccc_recent_combined.csv"
            if not os.path.exists(eccc_path):
                raise FileNotFoundError(f"ECCCæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {eccc_path}")
            
            eccc_data = pd.read_csv(eccc_path)
            logger.info(f"âœ… ECCCæ•°æ®åŠ è½½æˆåŠŸ: {len(eccc_data)} æ¡è®°å½•")
            
            # åŠ è½½HYDATæ°´æ–‡æ•°æ®
            hydat_path = "data/processed/hydat_streamflow_processed.csv"
            if not os.path.exists(hydat_path):
                raise FileNotFoundError(f"HYDATæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {hydat_path}")
            
            hydat_data = pd.read_csv(hydat_path)
            logger.info(f"âœ… HYDATæ•°æ®åŠ è½½æˆåŠŸ: {len(hydat_data)} æ¡è®°å½•")
            
            return eccc_data, hydat_data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def prepare_features(self, eccc_data, hydat_data):
        """å‡†å¤‡è®­ç»ƒç‰¹å¾"""
        logger.info("ğŸ”§ å‡†å¤‡è®­ç»ƒç‰¹å¾...")
        
        try:
            # å¤„ç†ECCCæ•°æ®
            eccc_data['Date/Time'] = pd.to_datetime(eccc_data['Date/Time'])
            eccc_data['Year'] = eccc_data['Date/Time'].dt.year
            eccc_data['Month'] = eccc_data['Date/Time'].dt.month
            eccc_data['Day'] = eccc_data['Date/Time'].dt.day
            eccc_data['DayOfYear'] = eccc_data['Date/Time'].dt.dayofyear
            
            # å¤„ç†HYDATæ•°æ®
            hydat_data['date'] = pd.to_datetime(hydat_data['date'])
            
            # åˆå¹¶æ•°æ®
            merged_data = pd.merge(
                eccc_data, 
                hydat_data, 
                left_on='Date/Time', 
                right_on='date', 
                how='inner'
            )
            
            logger.info(f"âœ… æ•°æ®åˆå¹¶æˆåŠŸ: {len(merged_data)} æ¡è®°å½•")
            
            # ç‰¹å¾å·¥ç¨‹
            features = []
            
            # ç§¯é›ªç›¸å…³ç‰¹å¾
            if 'Snow on Grnd (cm)' in merged_data.columns:
                features.append('Snow on Grnd (cm)')
                # ç§¯é›ªå˜åŒ–ç‡
                merged_data['snow_change'] = merged_data['Snow on Grnd (cm)'].diff()
                features.append('snow_change')
            
            # æ¸©åº¦ç›¸å…³ç‰¹å¾
            if 'Max Temp (Â°C)' in merged_data.columns:
                features.append('Max Temp (Â°C)')
                features.append('Min Temp (Â°C)')
                features.append('Mean Temp (Â°C)')
                
                # æ¸©åº¦å¼‚å¸¸
                merged_data['temp_anomaly'] = merged_data['Mean Temp (Â°C)'] - merged_data['Mean Temp (Â°C)'].rolling(30).mean()
                features.append('temp_anomaly')
            
            # é™æ°´ç›¸å…³ç‰¹å¾
            if 'Total Rain (mm)' in merged_data.columns:
                features.append('Total Rain (mm)')
                # é™æ°´ç´¯ç§¯
                merged_data['rain_cumulative'] = merged_data['Total Rain (mm)'].rolling(7).sum()
                features.append('rain_cumulative')
            
            # å¾„æµç›¸å…³ç‰¹å¾
            flow_columns = [col for col in merged_data.columns if col.startswith('05OC')]
            if flow_columns:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªç«™ç‚¹ä½œä¸ºä¸»è¦æµé‡æ•°æ®
                main_flow_column = flow_columns[0]
                features.append(main_flow_column)
                
                # å¾„æµå˜åŒ–ç‡
                merged_data['flow_change'] = merged_data[main_flow_column].pct_change()
                features.append('flow_change')
                
                # å¾„æµå¼‚å¸¸
                merged_data['flow_anomaly'] = merged_data[main_flow_column] / merged_data[main_flow_column].rolling(30).mean()
                features.append('flow_anomaly')
                
                logger.info(f"ä½¿ç”¨æµé‡ç«™ç‚¹: {main_flow_column}")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æµé‡æ•°æ®åˆ—")
            
            # å­£èŠ‚æ€§ç‰¹å¾
            merged_data['season'] = merged_data['Month'].map({
                12: 'winter', 1: 'winter', 2: 'winter',
                3: 'spring', 4: 'spring', 5: 'spring',
                6: 'summer', 7: 'summer', 8: 'summer',
                9: 'fall', 10: 'fall', 11: 'fall'
            })
            
            # å­£èŠ‚ç¼–ç 
            season_encoding = pd.get_dummies(merged_data['season'], prefix='season')
            merged_data = pd.concat([merged_data, season_encoding], axis=1)
            features.extend(season_encoding.columns.tolist())
            
            # æ—¶é—´ç‰¹å¾
            merged_data['day_of_year_sin'] = np.sin(2 * np.pi * merged_data['DayOfYear'] / 365)
            merged_data['day_of_year_cos'] = np.cos(2 * np.pi * merged_data['DayOfYear'] / 365)
            features.extend(['day_of_year_sin', 'day_of_year_cos'])
            
            logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(features)} ä¸ªç‰¹å¾")
            logger.info(f"ç‰¹å¾åˆ—è¡¨: {features}")
            
            return merged_data, features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    def create_flood_labels(self, data, threshold_percentile=90):
        """åˆ›å»ºæ´ªæ°´æ ‡ç­¾"""
        logger.info("ğŸ·ï¸ åˆ›å»ºæ´ªæ°´æ ‡ç­¾...")
        
        try:
            # æ‰¾åˆ°æµé‡åˆ—
            flow_columns = [col for col in data.columns if col.startswith('05OC')]
            if not flow_columns:
                raise ValueError("æœªæ‰¾åˆ°æµé‡æ•°æ®åˆ—")
            
            flow_column = flow_columns[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç«™ç‚¹
            logger.info(f"ä½¿ç”¨æµé‡åˆ—: {flow_column}")
            
            # è®¡ç®—æ´ªæ°´é˜ˆå€¼ (90%åˆ†ä½æ•°)
            flood_threshold = data[flow_column].quantile(threshold_percentile / 100)
            logger.info(f"æ´ªæ°´é˜ˆå€¼: {flood_threshold:.2f} mÂ³/s")
            
            # åˆ›å»ºæ ‡ç­¾
            data['flood_risk'] = (data[flow_column] > flood_threshold).astype(int)
            
            # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
            risk_counts = data['flood_risk'].value_counts()
            logger.info(f"æ ‡ç­¾åˆ†å¸ƒ: ä½é£é™©={risk_counts.get(0, 0)}, é«˜é£é™©={risk_counts.get(1, 0)}")
            
            return data, flood_threshold
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def train_model(self, data, features, target_column='flood_risk'):
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ´ªæ°´é¢„è­¦æ¨¡å‹...")
        
        try:
            # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
            X = data[features].fillna(0)  # ç®€å•å¡«å……ç¼ºå¤±å€¼
            y = data[target_column]
            
            # ç§»é™¤åŒ…å«æ— ç©·å€¼çš„è¡Œ
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.dropna()
            y = y[X.index]
            
            logger.info(f"è®­ç»ƒæ•°æ®: {len(X)} æ ·æœ¬, {len(features)} ç‰¹å¾")
            
            # æ•°æ®æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y
            )
            
            logger.info(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬, æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
            
            # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X_train, y_train)
            
            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test)[:, 1]
            
            # æ¨¡å‹è¯„ä¼°
            logger.info("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
            logger.info(f"åˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")
            
            # ç‰¹å¾é‡è¦æ€§
            feature_importance = pd.DataFrame({
                'feature': features,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            logger.info("ğŸ” ç‰¹å¾é‡è¦æ€§ (å‰10):")
            for i, row in feature_importance.head(10).iterrows():
                logger.info(f"  {row['feature']}: {row['importance']:.4f}")
            
            # ä¿å­˜æ¨¡å‹
            self.save_model()
            
            return {
                'model': self.model,
                'scaler': self.scaler,
                'feature_names': features,
                'feature_importance': feature_importance,
                'test_predictions': y_pred,
                'test_probabilities': y_pred_proba,
                'test_actual': y_test
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            joblib.dump(self.model, self.model_path)
            joblib.dump(self.scaler, self.scaler_path)
            logger.info(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {self.model_path}")
            logger.info(f"âœ… æ ‡å‡†åŒ–å™¨ä¿å­˜æˆåŠŸ: {self.scaler_path}")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def load_model(self):
        """åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹"""
        try:
            if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):
                self.model = joblib.load(self.model_path)
                self.scaler = joblib.load(self.scaler_path)
                logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                return True
            else:
                logger.warning("âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆè®­ç»ƒ")
                return False
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def predict_flood_risk(self, features_data):
        """é¢„æµ‹æ´ªæ°´é£é™©"""
        try:
            if self.model is None:
                if not self.load_model():
                    raise ValueError("æ¨¡å‹æœªè®­ç»ƒæˆ–åŠ è½½å¤±è´¥")
            
            # æ•°æ®é¢„å¤„ç†
            features_scaled = self.scaler.transform(features_data)
            
            # é¢„æµ‹
            risk_prediction = self.model.predict(features_scaled)
            risk_probability = self.model.predict_proba(features_scaled)[:, 1]
            
            return {
                'risk_level': risk_prediction,
                'risk_probability': risk_probability
            }
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ çœ‹é—¨ç‹—å®¡æ ¸é€šè¿‡ - æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        flood_model = FloodWarningModel()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·²è®­ç»ƒçš„æ¨¡å‹
        if flood_model.load_model():
            print("âœ… å‘ç°å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ")
            return
        
        # åŠ è½½æ•°æ®
        eccc_data, hydat_data = flood_model.load_data()
        
        # å‡†å¤‡ç‰¹å¾
        merged_data, features = flood_model.prepare_features(eccc_data, hydat_data)
        
        # åˆ›å»ºæ´ªæ°´æ ‡ç­¾
        labeled_data, flood_threshold = flood_model.create_flood_labels(merged_data)
        
        # è®­ç»ƒæ¨¡å‹
        training_results = flood_model.train_model(labeled_data, features)
        
        print("\n" + "=" * 60)
        print("ğŸ¯ æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½:")
        print(f"   - è®­ç»ƒæ ·æœ¬: {len(labeled_data)}")
        print(f"   - ç‰¹å¾æ•°é‡: {len(features)}")
        print(f"   - æ´ªæ°´é˜ˆå€¼: {flood_threshold:.2f} mÂ³/s")
        print(f"   - æ¨¡å‹æ–‡ä»¶: {flood_model.model_path}")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. é›†æˆåˆ°æ´ªæ°´é¢„è­¦API")
        print(f"   2. å®æ—¶é£é™©è¯„ä¼°")
        print(f"   3. é¢„è­¦é€šçŸ¥ç³»ç»Ÿ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}", exc_info=True)

if __name__ == "__main__":
    main()
