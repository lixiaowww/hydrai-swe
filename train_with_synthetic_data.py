#!/usr/bin/env python3
"""
ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒåœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹
è§£å†³æ•°æ®é‡ä¸è¶³çš„é—®é¢˜
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import logging
from datetime import datetime
import json
from typing import Dict, List, Tuple
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SyntheticDataSoilMoisturePredictor:
    """ä½¿ç”¨åˆæˆæ•°æ®çš„åœŸå£¤æ¹¿åº¦é¢„æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.scaler = StandardScaler()
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"âœ… åˆæˆæ•°æ®åœŸå£¤æ¹¿åº¦é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def load_and_prepare_data(self, data_path: str) -> Dict:
        """åŠ è½½å’Œå‡†å¤‡åˆæˆæ•°æ®"""
        try:
            logger.info(f"ğŸ“¥ åŠ è½½åˆæˆæ•°æ®: {data_path}")
            
            # åŠ è½½æ•°æ®
            data = pd.read_csv(data_path)
            logger.info(f"ğŸ“Š åˆæˆæ•°æ®: {data.shape}")
            
            # ç‰¹å¾å·¥ç¨‹
            features, target = self._engineer_features(data)
            
            # æ•°æ®åˆ†å‰²
            X_train, X_temp, y_train, y_temp = train_test_split(
                features, target, test_size=0.3, random_state=42
            )
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=0.5, random_state=42
            )
            
            # æ ‡å‡†åŒ–
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            X_test_scaled = self.scaler.transform(X_test)
            
            # è½¬æ¢ä¸ºå¼ é‡
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train_scaled), 
                torch.FloatTensor(y_train.values)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val_scaled), 
                torch.FloatTensor(y_val.values)
            )
            test_dataset = TensorDataset(
                torch.FloatTensor(X_test_scaled), 
                torch.FloatTensor(y_test.values)
            )
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))
            test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))
            
            data_loaders = {
                'train': train_loader,
                'val': val_loader,
                'test': test_loader
            }
            
            logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
            logger.info(f"  è®­ç»ƒé›†: {X_train.shape}")
            logger.info(f"  éªŒè¯é›†: {X_val.shape}")
            logger.info(f"  æµ‹è¯•é›†: {X_test.shape}")
            logger.info(f"  ç‰¹å¾æ•°: {X_train.shape[1]}")
            
            return {
                'status': 'success',
                'data_loaders': data_loaders,
                'data_shapes': {
                    'train': X_train.shape,
                    'val': X_val.shape,
                    'test': X_test.shape
                },
                'feature_names': features.columns.tolist()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _engineer_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """ç‰¹å¾å·¥ç¨‹"""
        try:
            # é€‰æ‹©æ•°å€¼ç‰¹å¾
            feature_cols = [
                'temperature', 'humidity', 'precipitation', 'snow',
                'pressure', 'wind_speed', 'wind_direction',
                'year', 'month', 'day', 'day_of_year', 'day_of_week'
            ]
            
            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å­˜åœ¨
            available_cols = [col for col in feature_cols if col in data.columns]
            features = data[available_cols].copy()
            
            # æ·»åŠ æ´¾ç”Ÿç‰¹å¾
            if 'temperature' in features.columns:
                features['temp_squared'] = features['temperature'] ** 2
                features['temp_cubed'] = features['temperature'] ** 3
            
            if 'humidity' in features.columns:
                features['humidity_squared'] = features['humidity'] ** 2
            
            if 'precipitation' in features.columns:
                features['precip_squared'] = features['precipitation'] ** 2
                features['precip_log'] = np.log1p(features['precipitation'])
            
            if 'month' in features.columns:
                features['is_winter'] = features['month'].isin([12, 1, 2]).astype(int)
                features['is_spring'] = features['month'].isin([3, 4, 5]).astype(int)
                features['is_summer'] = features['month'].isin([6, 7, 8]).astype(int)
                features['is_fall'] = features['month'].isin([9, 10, 11]).astype(int)
                
                # å­£èŠ‚æ€§æ­£å¼¦ç‰¹å¾
                features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)
                features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)
            
            if 'day_of_year' in features.columns:
                features['day_sin'] = np.sin(2 * np.pi * features['day_of_year'] / 365)
                features['day_cos'] = np.cos(2 * np.pi * features['day_of_year'] / 365)
            
            # äº¤äº’ç‰¹å¾
            if 'temperature' in features.columns and 'humidity' in features.columns:
                features['temp_humidity'] = features['temperature'] * features['humidity']
            
            if 'temperature' in features.columns and 'precipitation' in features.columns:
                features['temp_precip'] = features['temperature'] * features['precipitation']
            
            # ç›®æ ‡å˜é‡
            target = data['soil_moisture']
            
            logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {features.shape[1]} ä¸ªç‰¹å¾")
            return features, target
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    def build_model(self, input_size: int) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        try:
            model = nn.Sequential(
                nn.Linear(input_size, 128),
                nn.ReLU(),
                nn.BatchNorm1d(128),
                nn.Dropout(0.3),
                
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.BatchNorm1d(64),
                nn.Dropout(0.2),
                
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.BatchNorm1d(32),
                nn.Dropout(0.1),
                
                nn.Linear(32, 16),
                nn.ReLU(),
                nn.Linear(16, 1)
            )
            
            self.model = model.to(self.device)
            logger.info(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ: è¾“å…¥ç‰¹å¾æ•° {input_size}")
            return model
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            return None
    
    def train_model(self, data_loaders: Dict, epochs: int = 200) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model()")
            
            logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
            
            # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
            criterion = nn.MSELoss()
            optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15, factor=0.5)
            
            # è®­ç»ƒå†å²
            train_losses = []
            val_losses = []
            best_val_loss = float('inf')
            patience_counter = 0
            
            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                self.model.train()
                train_loss = 0
                for batch_X, batch_y in data_loaders['train']:
                    batch_X = batch_X.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = self.model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    train_loss += loss.item()
                
                train_loss /= len(data_loaders['train'])
                train_losses.append(train_loss)
                
                # éªŒè¯é˜¶æ®µ
                self.model.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch_X, batch_y in data_loaders['val']:
                        batch_X = batch_X.to(self.device)
                        batch_y = batch_y.to(self.device)
                        
                        outputs = self.model(batch_X).squeeze()
                        loss = criterion(outputs, batch_y)
                        val_loss += loss.item()
                
                val_loss /= len(data_loaders['val'])
                val_losses.append(val_loss)
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step(val_loss)
                
                # æ—©åœ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    torch.save(self.model.state_dict(), 'best_synthetic_model.pth')
                else:
                    patience_counter += 1
                
                if patience_counter >= 30:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒ")
                    break
                
                if (epoch + 1) % 20 == 0:
                    logger.info(f"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            return {
                'status': 'success',
                'epochs_trained': epoch + 1,
                'final_train_loss': train_loss,
                'final_val_loss': val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def evaluate_model(self, data_loaders: Dict) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model()")
            
            logger.info("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            self.model.load_state_dict(torch.load('best_synthetic_model.pth'))
            self.model.eval()
            
            # æµ‹è¯•é›†è¯„ä¼°
            test_predictions = []
            test_targets = []
            
            with torch.no_grad():
                for batch_X, batch_y in data_loaders['test']:
                    batch_X = batch_X.to(self.device)
                    outputs = self.model(batch_X).squeeze()
                    test_predictions.extend(outputs.cpu().numpy())
                    test_targets.extend(batch_y.numpy())
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            test_predictions = np.array(test_predictions)
            test_targets = np.array(test_targets)
            
            r2 = r2_score(test_targets, test_predictions)
            mae = mean_absolute_error(test_targets, test_predictions)
            rmse = np.sqrt(mean_squared_error(test_targets, test_predictions))
            
            performance = {
                'r2_score': r2,
                'mae': mae,
                'rmse': rmse,
                'status': 'overfitting' if r2 < 0 else 'normal',
                'test_samples': len(test_targets)
            }
            
            logger.info(f"ğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°å®Œæˆ:")
            logger.info(f"  RÂ²: {r2:.4f}")
            logger.info(f"  MAE: {mae:.4f}")
            logger.info(f"  RMSE: {rmse:.4f}")
            logger.info(f"  çŠ¶æ€: {'è¿‡æ‹Ÿåˆ' if r2 < 0 else 'æ­£å¸¸'}")
            
            return performance
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒåœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹...")
        
        # åˆ›å»ºé¢„æµ‹å™¨
        predictor = SyntheticDataSoilMoisturePredictor()
        
        # åŠ è½½å’Œå‡†å¤‡åˆæˆæ•°æ®
        data_path = "data/synthetic/synthetic_weather_20250821_183650.csv"
        data_result = predictor.load_and_prepare_data(data_path)
        
        if data_result['status'] != 'success':
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {data_result}")
            return
        
        # æ„å»ºæ¨¡å‹
        input_size = data_result['data_shapes']['train'][1]
        model = predictor.build_model(input_size)
        
        if model is None:
            logger.error("âŒ æ¨¡å‹æ„å»ºå¤±è´¥")
            return
        
        # è®­ç»ƒæ¨¡å‹
        training_result = predictor.train_model(data_result['data_loaders'])
        
        if training_result['status'] != 'success':
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {training_result}")
            return
        
        # è¯„ä¼°æ¨¡å‹
        performance = predictor.evaluate_model(data_result['data_loaders'])
        
        if 'status' not in performance:
            logger.info("ğŸ‰ ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹æˆåŠŸï¼")
            
            # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            report = {
                'timestamp': datetime.now().isoformat(),
                'data_info': {
                    'total_samples': sum(data_result['data_shapes'].values()),
                    'features': input_size,
                    'feature_names': data_result['feature_names']
                },
                'training_summary': training_result,
                'model_performance': performance
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"synthetic_data_training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # æ˜¾ç¤ºå…³é”®ç»“æœ
            if performance['r2_score'] > 0:
                logger.info("ğŸ¯ æˆåŠŸï¼RÂ²å·²è½¬ä¸ºæ­£å€¼ï¼Œåˆæˆæ•°æ®è§£å†³äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼")
                logger.info(f"ğŸ† æœ€ç»ˆRÂ²: {performance['r2_score']:.4f}")
            else:
                logger.info("âš ï¸ RÂ²ä»ä¸ºè´Ÿå€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
            
            return report
        else:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {performance}")
            return None
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()
