#!/usr/bin/env python3
"""
å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–è„šæœ¬
ä½¿ç”¨è½»é‡çº§ç­–ç•¥å¿«é€Ÿæ‰¾åˆ°å¥½çš„å‚æ•°ç»„åˆ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
import os
from datetime import datetime
import time

class QuickGRUModel(nn.Module):
    """å¿«é€Ÿä¼˜åŒ–çš„GRUæ¨¡å‹"""
    
    def __init__(self, input_size=6, hidden_size=64, num_layers=2, dropout=0.1):
        super(QuickGRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                          dropout=dropout if num_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out)
        output = self.fc(gru_out[:, -1, :])
        return output

class QuickHyperparameterOptimizer:
    """å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.scaler_X = None
        self.scaler_y = None
        self.sequence_length = 30
        self.optimization_results = []
        
    def load_data_and_scalers(self):
        """åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨"""
        print("ğŸ“Š åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨...")
        
        try:
            # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
            data_path = "data/processed/standardized_training_dataset.csv"
            data = pd.read_csv(data_path, index_col=0, parse_dates=True)
            print(f"âœ… åŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨å‚æ•°
            import pickle
            with open('models/standardization_params.pkl', 'rb') as f:
                params = pickle.load(f)
            
            # é‡å»ºæ ‡å‡†åŒ–å™¨
            self.scaler_X = StandardScaler()
            self.scaler_X.mean_ = params['scaler_X_mean']
            self.scaler_X.scale_ = params['scaler_X_scale']
            
            self.scaler_y = StandardScaler()
            self.scaler_y.mean_ = params['scaler_y_mean']
            self.scaler_y.scale_ = params['scaler_y_scale']
            
            print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            return data
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_sequences(self, data):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡åºåˆ—æ•°æ®...")
        
        feature_cols = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                       'day_of_year', 'month', 'year']
        target_col = 'snow_water_equivalent_mm'
        
        X = data[feature_cols].values
        y = data[target_col].values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.transform(X)
        y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºåºåˆ—
        X_seq, y_seq = [], []
        for i in range(len(X_scaled) - self.sequence_length):
            X_seq.append(X_scaled[i:(i + self.sequence_length)])
            y_seq.append(y_scaled[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"âœ… åºåˆ—æ•°æ®å‡†å¤‡å®Œæˆ: {X_seq.shape}, {y_seq.shape}")
        return X_seq, y_seq
    
    def split_data(self, X, y, train_ratio=0.8, val_ratio=0.2):
        """å¿«é€Ÿæ•°æ®åˆ†å‰²"""
        n = len(X)
        train_end = int(n * train_ratio)
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        X_val = X[train_end:]
        y_val = y[train_end:]
        
        return (X_train, y_train), (X_val, y_val)
    
    def create_data_loaders(self, train_data, val_data, batch_size):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        X_train, y_train = train_data
        X_val, y_val = val_data
        
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader
    
    def quick_train_and_evaluate(self, model, train_loader, val_loader, params):
        """å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°"""
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
        
        # å¿«é€Ÿè®­ç»ƒå‚æ•°
        epochs = 15  # å‡å°‘è®­ç»ƒè½®æ•°
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 5  # å‡å°‘è€å¿ƒå€¼
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        return best_val_loss
    
    def run_quick_optimization(self):
        """è¿è¡Œå¿«é€Ÿä¼˜åŒ–"""
        print("ğŸš€ å¼€å§‹å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data_and_scalers()
        if data is None:
            return
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        X, y = self.prepare_sequences(data)
        
        # åˆ†å‰²æ•°æ®
        train_data, val_data = self.split_data(X, y)
        
        # å®šä¹‰è¦æµ‹è¯•çš„å‚æ•°ç»„åˆï¼ˆå‡å°‘ç»„åˆæ•°é‡ï¼‰
        param_combinations = [
            # åŸºç¡€é…ç½®
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32},
            {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32},
            {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32},
            
            # å­¦ä¹ ç‡å˜åŒ–
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 32},
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.002, 'batch_size': 32},
            
            # Dropoutå˜åŒ–
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.05, 'learning_rate': 0.001, 'batch_size': 32},
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'batch_size': 32},
            
            # æ‰¹å¤§å°å˜åŒ–
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 16},
            {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 64},
            
            # ç»„åˆä¼˜åŒ–
            {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.15, 'learning_rate': 0.0008, 'batch_size': 48},
            {'hidden_size': 96, 'num_layers': 2, 'dropout': 0.12, 'learning_rate': 0.0012, 'batch_size': 40},
        ]
        
        print(f"ğŸ¯ æµ‹è¯• {len(param_combinations)} ç§å‚æ•°ç»„åˆ...")
        
        best_result = None
        best_val_loss = float('inf')
        
        for i, params in enumerate(param_combinations):
            print(f"\n{'='*50}")
            print(f"ğŸ” è¯•éªŒ {i+1}/{len(param_combinations)}")
            print(f"å‚æ•°: {params}")
            print(f"{'='*50}")
            
            try:
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                train_loader, val_loader = self.create_data_loaders(
                    train_data, val_data, params['batch_size']
                )
                
                # åˆ›å»ºæ¨¡å‹
                model = QuickGRUModel(
                    input_size=6,
                    hidden_size=params['hidden_size'],
                    num_layers=params['num_layers'],
                    dropout=params['dropout']
                )
                
                # å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°
                start_time = time.time()
                val_loss = self.quick_train_and_evaluate(model, train_loader, val_loader, params)
                training_time = time.time() - start_time
                
                # è®°å½•ç»“æœ
                result = {
                    'trial': i + 1,
                    'params': params,
                    'val_loss': val_loss,
                    'training_time': training_time
                }
                
                self.optimization_results.append(result)
                
                print(f"âœ… è¯•éªŒ {i+1} å®Œæˆ:")
                print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
                print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
                
                # æ›´æ–°æœ€ä½³ç»“æœ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_result = result
                    print(f"ğŸ† æ–°çš„æœ€ä½³ç»“æœ!")
                
            except Exception as e:
                print(f"âŒ è¯•éªŒ {i+1} å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        self.save_quick_optimization_results(best_result)
        
        return best_result
    
    def save_quick_optimization_results(self, best_result):
        """ä¿å­˜å¿«é€Ÿä¼˜åŒ–ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        best_params_path = f"logs/quick_best_hyperparameters_{timestamp}.json"
        os.makedirs(os.path.dirname(best_params_path), exist_ok=True)
        
        import json
        with open(best_params_path, 'w', encoding='utf-8') as f:
            json.dump({
                'best_value': best_result['val_loss'],
                'best_params': best_result['params'],
                'n_trials': len(self.optimization_results),
                'optimization_time': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æœ€ä½³å‚æ•°å·²ä¿å­˜: {best_params_path}")
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        all_results_path = f"logs/quick_optimization_all_results_{timestamp}.json"
        with open(all_results_path, 'w', encoding='utf-8') as f:
            json.dump(self.optimization_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜: {all_results_path}")
        
        # ç”Ÿæˆå¿«é€Ÿä¼˜åŒ–æŠ¥å‘Š
        self.generate_quick_optimization_report(best_result, timestamp)
    
    def generate_quick_optimization_report(self, best_result, timestamp):
        """ç”Ÿæˆå¿«é€Ÿä¼˜åŒ–æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆå¿«é€Ÿä¼˜åŒ–æŠ¥å‘Š...")
        
        report_path = f"logs/quick_optimization_report_{timestamp}.md"
        
        # æŒ‰éªŒè¯æŸå¤±æ’åº
        sorted_results = sorted(self.optimization_results, key=lambda x: x['val_loss'])
        
        report_content = f"""# å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–æŠ¥å‘Š

## ä¼˜åŒ–æ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ä¼˜åŒ–ç­–ç•¥
- **å¿«é€Ÿè®­ç»ƒ**: æ¯ä¸ªè¯•éªŒæœ€å¤š15ä¸ªepoch
- **æ—©åœæœºåˆ¶**: 5ä¸ªepochæ— æ”¹å–„å³åœæ­¢
- **å‚æ•°ç»„åˆ**: é¢„å®šä¹‰çš„10ç§å‚æ•°ç»„åˆ
- **æ€»è€—æ—¶**: çº¦ {sum(r['training_time'] for r in self.optimization_results):.1f} ç§’

## æœ€ä½³ç»“æœ
ğŸ† **æœ€ä½³éªŒè¯æŸå¤±**: {best_result['val_loss']:.6f}

### æœ€ä½³è¶…å‚æ•°
"""
        
        for key, value in best_result['params'].items():
            report_content += f"- **{key}**: {value}\n"
        
        report_content += f"""

## æ‰€æœ‰è¯•éªŒç»“æœæ’å

| æ’å | è¯•éªŒ | éšè—å¤§å° | å±‚æ•° | Dropout | å­¦ä¹ ç‡ | æ‰¹å¤§å° | éªŒè¯æŸå¤± | è®­ç»ƒæ—¶é—´(s) |
|------|------|----------|------|---------|--------|---------|----------|-------------|
"""
        
        for i, result in enumerate(sorted_results):
            params = result['params']
            report_content += f"| {i+1} | {result['trial']} | {params['hidden_size']} | {params['num_layers']} | {params['dropout']} | {params['learning_rate']} | {params['batch_size']} | {result['val_loss']:.6f} | {result['training_time']:.2f} |\n"
        
        report_content += f"""

## å…³é”®å‘ç°
1. **æœ€ä½³é…ç½®**: {best_result['params']['hidden_size']}éšè—å•å…ƒ, {best_result['params']['num_layers']}å±‚, {best_result['params']['dropout']}dropout
2. **è®­ç»ƒæ•ˆç‡**: å¹³å‡æ¯æ¬¡è¯•éªŒ {sum(r['training_time'] for r in self.optimization_results)/len(self.optimization_results):.2f} ç§’
3. **æ€§èƒ½æå‡**: ç›¸æ¯”é»˜è®¤å‚æ•°ï¼ŒéªŒè¯æŸå¤±ä» {max(r['val_loss'] for r in self.optimization_results):.6f} é™è‡³ {best_result['val_loss']:.6f}

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. **ä½¿ç”¨æœ€ä½³å‚æ•°**: ç”¨æœ€ä½³é…ç½®é‡æ–°è®­ç»ƒå®Œæ•´æ¨¡å‹
2. **ç²¾ç»†è°ƒä¼˜**: åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œæ›´ç²¾ç»†çš„æœç´¢
3. **æ¨¡å‹é›†æˆ**: è€ƒè™‘é›†æˆå‰3-5ä¸ªæœ€ä½³é…ç½®
4. **æ•°æ®å¢å¼º**: ç»“åˆæœ€ä½³è¶…å‚æ•°å°è¯•æ•°æ®å¢å¼º
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… å¿«é€Ÿä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ HydrAI-SWE å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–")
    print("=" * 60)
    
    try:
        # åˆ›å»ºå¿«é€Ÿä¼˜åŒ–å™¨
        optimizer = QuickHyperparameterOptimizer()
        
        # è¿è¡Œå¿«é€Ÿä¼˜åŒ–
        best_result = optimizer.run_quick_optimization()
        
        if best_result:
            print("\n" + "=" * 60)
            print("ğŸ‰ å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
            print(f"âœ… æœ€ä½³éªŒè¯æŸå¤±: {best_result['val_loss']:.6f}")
            print(f"âœ… æ€»è€—æ—¶: {sum(r['training_time'] for r in optimizer.optimization_results):.1f} ç§’")
            print("âœ… ä¼˜åŒ–ç»“æœå·²ä¿å­˜")
            print("âœ… å¿«é€Ÿä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ")
            
            # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
            print(f"\nğŸ’¡ ç«‹å³è¡ŒåŠ¨å»ºè®®:")
            print(f"  1. ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒå®Œæ•´æ¨¡å‹")
            print(f"  2. åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œç²¾ç»†æœç´¢")
            print(f"  3. è€ƒè™‘é›†æˆå‰3ä¸ªæœ€ä½³é…ç½®")
            print(f"  4. å°è¯•æ•°æ®å¢å¼ºæŠ€æœ¯")
        else:
            print("âŒ å¿«é€Ÿä¼˜åŒ–å¤±è´¥")
        
    except Exception as e:
        print(f"âŒ å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
