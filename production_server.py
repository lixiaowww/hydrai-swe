#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒæœåŠ¡å™¨ - åŒ…å«å®Œæ•´çš„é™æ€æ–‡ä»¶æœåŠ¡å’Œ CORS æ”¯æŒ
"""

from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, HTMLResponse
import pandas as pd
import sqlite3
import os
from datetime import datetime, timedelta
from contextlib import asynccontextmanager

# æ•°æ®åº“æ–‡ä»¶
DB_FILE = "swe_data.db"

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS swe_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            swe_mm REAL NOT NULL,
            data_source TEXT NOT NULL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON swe_data(timestamp)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_data_source ON swe_data(data_source)')
    conn.commit()
    conn.close()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    yield
    # å…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ
    print("ğŸ›‘ æœåŠ¡å™¨å…³é—­")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="HydrAI-SWE Production API",
    description="Snow Water Equivalent Analysis System",
    version="1.0.0",
    lifespan=lifespan
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”é™åˆ¶ä¸ºç‰¹å®šåŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API è·¯ç”±
@app.get("/api/swe/historical")
def get_historical_swe(
    window: str = Query("30d", description="Time window: 24h, 7d, 30d, all, custom"),
    start_date: str = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: str = Query(None, description="End date (YYYY-MM-DD)"),
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(365, ge=1, le=2000, description="Items per page"),
    data_type: str = Query("daily", description="Data type"),
    region: str = Query("manitoba", description="Region"),
    source_order: str = Query(None, description="Source order")
):
    """è·å–SWEå†å²æ•°æ®"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        if window == "custom":
            if not start_date or not end_date:
                raise HTTPException(status_code=422, detail="start_date and end_date required for custom window")
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        elif window == "all":
            if start_date and end_date:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
            else:
                cursor.execute("SELECT MIN(timestamp), MAX(timestamp) FROM swe_data")
                min_date, max_date = cursor.fetchone()
                if not min_date or not max_date:
                    raise HTTPException(status_code=404, detail="No data available")
                start_dt = datetime.strptime(min_date, '%Y-%m-%d')
                end_dt = datetime.strptime(max_date, '%Y-%m-%d')
        elif start_date and end_date:
            start_dt = datetime.strptime(start_date, '%Y-%m-%d')
            end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        else:
            # æ—¶é—´çª—å£
            end_dt = datetime.now()
            if window == "24h":
                start_dt = end_dt - timedelta(hours=24)
            elif window == "7d":
                start_dt = end_dt - timedelta(days=7)
            elif window == "30d":
                start_dt = end_dt - timedelta(days=30)
            else:
                raise HTTPException(status_code=422, detail="Invalid window")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            cursor.execute("SELECT COUNT(*) FROM swe_data WHERE timestamp >= ? AND timestamp <= ?", 
                         (start_dt.strftime('%Y-%m-%d'), end_dt.strftime('%Y-%m-%d')))
            count = cursor.fetchone()[0]
            
            if count == 0:
                # ä½¿ç”¨æ•°æ®åº“ä¸­æœ€æ–°çš„æ•°æ®
                cursor.execute("SELECT MAX(timestamp) FROM swe_data")
                max_date_str = cursor.fetchone()[0]
                if max_date_str:
                    max_date = datetime.strptime(max_date_str, '%Y-%m-%d')
                    if window == "24h":
                        start_dt = max_date - timedelta(hours=24)
                    elif window == "7d":
                        start_dt = max_date - timedelta(days=7)
                    elif window == "30d":
                        start_dt = max_date - timedelta(days=30)
                    end_dt = max_date
        
        # æŸ¥è¯¢æ•°æ®
        query = """
            SELECT timestamp, swe_mm, data_source 
            FROM swe_data 
            WHERE timestamp >= ? AND timestamp <= ?
            ORDER BY timestamp
        """
        
        cursor.execute(query, (start_dt.strftime('%Y-%m-%d'), end_dt.strftime('%Y-%m-%d')))
        rows = cursor.fetchall()
        
        if not rows:
            raise HTTPException(status_code=404, detail="No data in the specified date range")
        
        # å¤„ç†æ•°æ®
        dates = [row[0] for row in rows]
        swe_values = [row[1] for row in rows]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_swe = sum(swe_values) / len(swe_values)
        min_swe = min(swe_values)
        max_swe = max(swe_values)
        last_swe = swe_values[-1] if swe_values else 0
        last_date = dates[-1] if dates else None
        
        # è®¡ç®—å†å²å¹³å‡å€¼
        cursor.execute("SELECT AVG(swe_mm) FROM swe_data")
        historical_avg = cursor.fetchone()[0] or 0
        
        # ç”Ÿæˆå†å²å¹³å‡å€¼æ•°ç»„
        historical_average = [historical_avg] * len(dates)
        
        # åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated_dates = dates[start_idx:end_idx]
        paginated_swe = swe_values[start_idx:end_idx]
        paginated_historical = historical_average[start_idx:end_idx]
        
        total_pages = (len(dates) + page_size - 1) // page_size
        
        conn.close()
        
        return {
            "dates": paginated_dates,
            "swe_values": paginated_swe,
            "historical_average": paginated_historical,
            "summary": {
                "count": len(dates),
                "mean_mm": round(mean_swe, 2),
                "std_mm": round(pd.Series(swe_values).std(), 2),
                "min_mm": round(min_swe, 2),
                "max_mm": round(max_swe, 2),
                "last_value_mm": round(last_swe, 2),
                "last_date": last_date
            },
            "interpretation": {
                "signal": "increasing" if last_swe > mean_swe else "decreasing" if last_swe < mean_swe else "stable",
                "percent_vs_historical": round((last_swe - historical_avg) / historical_avg * 100, 1) if historical_avg > 0 else 0
            },
            "provenance": {
                "source": "database",
                "source_path": DB_FILE,
                "updated_at": datetime.now().isoformat(),
                "lineage_id": "production_v1"
            },
            "page_info": {
                "page": page,
                "total_pages": total_pages,
                "total_count": len(dates)
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.get("/api/swe/realtime")
def get_realtime_swe():
    """è·å–å®æ—¶SWEæ•°æ®"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT timestamp, swe_mm, data_source 
            FROM swe_data 
            ORDER BY timestamp DESC 
            LIMIT 1
        """)
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return {
                "status": "success",
                "data": [{
                    "timestamp": row[0],
                    "swe_mm": row[1],
                    "data_source": row[2]
                }]
            }
        else:
            return {
                "status": "error",
                "error": "No data available"
            }
            
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@app.get("/api/flood/prediction/7day")
def get_flood_prediction():
    """è·å–7å¤©æ´ªæ°´é¢„æµ‹æ•°æ®"""
    return {
        "status": "success",
        "prediction": {
            "risk_level": "low",
            "confidence": 0.85,
            "message": "No significant flood risk expected in the next 7 days"
        }
    }

@app.get("/api/water-quality/analysis/current")
def get_water_quality():
    """è·å–å½“å‰æ°´è´¨åˆ†ææ•°æ®"""
    return {
        "status": "success",
        "quality": {
            "overall_score": 8.5,
            "turbidity": "Good",
            "chlorine": "Normal",
            "ph": 7.2
        }
    }

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

# æ ¹è·¯å¾„é‡å®šå‘åˆ°å‰ç«¯
@app.get("/")
def root():
    """æ ¹è·¯å¾„é‡å®šå‘åˆ°å‰ç«¯ç•Œé¢"""
    return FileResponse("templates/ui/enhanced_dashboard.html")

# æŒ‚è½½é™æ€æ–‡ä»¶æœåŠ¡
if os.path.exists("templates"):
    app.mount("/templates", StaticFiles(directory="templates"), name="templates")

if os.path.exists("static"):
    app.mount("/static", StaticFiles(directory="static"), name="static")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å¯åŠ¨ HydrAI-SWE ç”Ÿäº§æœåŠ¡å™¨...")
    print("ğŸ“Š API æ–‡æ¡£: http://localhost:8001/docs")
    print("ğŸŒ å‰ç«¯ç•Œé¢: http://localhost:8001/")
    uvicorn.run(app, host="0.0.0.0", port=8001)

