#!/usr/bin/env python3
"""
æœç´¢GitHubå’ŒKaggleä¸Šçš„SWEæ°”å€™å˜åŒ–ç ”ç©¶æ¡ˆä¾‹
åˆ†æ"æ°”å€™å˜åŒ–å½±å“SWE"å’Œ"èé›ªæ´ªæ°´é£é™©"ç›¸å…³ç ”ç©¶
"""

import requests
import json
import time
from datetime import datetime
import pandas as pd

def search_github_repos(query, max_results=50):
    """æœç´¢GitHubä»“åº“"""
    print(f"ğŸ” æœç´¢GitHub: {query}")
    
    # GitHub APIæœç´¢ä»“åº“
    url = "https://api.github.com/search/repositories"
    params = {
        'q': query,
        'sort': 'stars',
        'order': 'desc',
        'per_page': min(max_results, 100)
    }
    
    try:
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            return data.get('items', [])
        else:
            print(f"âŒ GitHub APIé”™è¯¯: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return []

def search_kaggle_datasets(query, max_results=20):
    """æœç´¢Kaggleæ•°æ®é›†"""
    print(f"ğŸ” æœç´¢Kaggleæ•°æ®é›†: {query}")
    
    # æ³¨æ„ï¼šKaggle APIéœ€è¦è®¤è¯ï¼Œè¿™é‡Œæä¾›æœç´¢å»ºè®®
    print("ğŸ“Š Kaggleæ•°æ®é›†æœç´¢å»ºè®®:")
    print(f"  å…³é”®è¯: {query}")
    print("  è®¿é—®: https://www.kaggle.com/datasets")
    print("  æœç´¢ç›¸å…³æ•°æ®é›†")
    
    # è¿”å›ä¸€äº›å·²çŸ¥çš„ç›¸å…³æ•°æ®é›†
    known_datasets = [
        {
            'title': 'Snow Water Equivalent (SWE) Data',
            'description': 'Historical SWE measurements for climate analysis',
            'url': 'https://www.kaggle.com/datasets/example/swe-data'
        }
    ]
    
    return known_datasets

def analyze_climate_change_swe():
    """åˆ†ææ°”å€™å˜åŒ–å½±å“SWEçš„ç ”ç©¶æ¡ˆä¾‹"""
    print("\nğŸŒ åˆ†ææ°”å€™å˜åŒ–å½±å“SWEç ”ç©¶æ¡ˆä¾‹")
    print("=" * 60)
    
    # æœç´¢å…³é”®è¯
    queries = [
        'climate change snow water equivalent SWE',
        'SWE trend analysis Mann-Kendall',
        'snow water equivalent climate impact',
        'SWE time series analysis 30 years',
        'snow cover climate change detection'
    ]
    
    all_repos = []
    for query in queries:
        repos = search_github_repos(query, max_results=20)
        all_repos.extend(repos)
        time.sleep(1)  # é¿å…APIé™åˆ¶
    
    # å»é‡å’Œæ’åº
    unique_repos = {}
    for repo in all_repos:
        if repo['id'] not in unique_repos:
            unique_repos[repo['id']] = repo
    
    # æŒ‰æ˜Ÿæ ‡æ’åº
    sorted_repos = sorted(unique_repos.values(), key=lambda x: x['stargazers_count'], reverse=True)
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(sorted_repos)} ä¸ªç›¸å…³ä»“åº“")
    
    # åˆ†æå‰10ä¸ªä»“åº“
    top_repos = sorted_repos[:10]
    for i, repo in enumerate(top_repos, 1):
        print(f"\n{i}. {repo['full_name']}")
        print(f"   æè¿°: {repo['description'] or 'æ— æè¿°'}")
        print(f"   æ˜Ÿæ ‡: {repo['stargazers_count']}")
        print(f"   è¯­è¨€: {repo['language'] or 'æœªçŸ¥'}")
        print(f"   æ›´æ–°: {repo['updated_at'][:10]}")
        print(f"   URL: {repo['html_url']}")
    
    return sorted_repos

def analyze_snowmelt_flood_risk():
    """åˆ†æèé›ªæ´ªæ°´é£é™©ç ”ç©¶æ¡ˆä¾‹"""
    print("\nğŸŒŠ åˆ†æèé›ªæ´ªæ°´é£é™©ç ”ç©¶æ¡ˆä¾‹")
    print("=" * 60)
    
    # æœç´¢å…³é”®è¯
    queries = [
        'snowmelt flood risk assessment',
        'SWE runoff prediction flood',
        'snow melt flood modeling',
        'basin scale snowmelt analysis',
        'daily SWE temperature radiation flood'
    ]
    
    all_repos = []
    for query in queries:
        repos = search_github_repos(query, max_results=20)
        all_repos.extend(repos)
        time.sleep(1)
    
    # å»é‡å’Œæ’åº
    unique_repos = {}
    for repo in all_repos:
        if repo['id'] not in unique_repos:
            unique_repos[repo['id']] = repo
    
    sorted_repos = sorted(unique_repos.values(), key=lambda x: x['stargazers_count'], reverse=True)
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(sorted_repos)} ä¸ªç›¸å…³ä»“åº“")
    
    # åˆ†æå‰10ä¸ªä»“åº“
    top_repos = sorted_repos[:10]
    for i, repo in enumerate(top_repos, 1):
        print(f"\n{i}. {repo['full_name']}")
        print(f"   æè¿°: {repo['description'] or 'æ— æè¿°'}")
        print(f"   æ˜Ÿæ ‡: {repo['stargazers_count']}")
        print(f"   è¯­è¨€: {repo['language'] or 'æœªçŸ¥'}")
        print(f"   æ›´æ–°: {repo['updated_at'][:10]}")
        print(f"   URL: {repo['html_url']}")
    
    return sorted_repos

def generate_research_summary():
    """ç”Ÿæˆç ”ç©¶æ¡ˆä¾‹æ€»ç»“"""
    print("\nğŸ“‹ ç ”ç©¶æ¡ˆä¾‹æ€»ç»“å’Œå»ºè®®")
    print("=" * 60)
    
    print("\nğŸŒ æ°”å€™å˜åŒ–å½±å“SWEç ”ç©¶è¦ç‚¹:")
    print("1. æ—¶é—´åºåˆ—è¦æ±‚: è‡³å°‘30å¹´è¿ç»­æ•°æ®")
    print("2. æ•°æ®åŒè´¨åŒ–: å¤„ç†è§‚æµ‹æ–¹æ³•å˜åŒ–ã€ç«™ç‚¹è¿ç§»ç­‰")
    print("3. åŸºå‡†æœŸ: 1991-2020å¹´ä½œä¸ºæ°”å€™åŸºå‡†æœŸ")
    print("4. å¼‚å¸¸è®¡ç®—: ç›¸å¯¹äºåŸºå‡†æœŸçš„åå·®")
    print("5. è¶‹åŠ¿æ£€éªŒ: Mann-Kendalléå‚æ•°æ£€éªŒ")
    print("6. æ–œç‡ä¼°è®¡: Theil-Senç¨³å¥æ–œç‡")
    
    print("\nğŸŒŠ èé›ªæ´ªæ°´é£é™©ç ”ç©¶è¦ç‚¹:")
    print("1. ç©ºé—´å°ºåº¦: æµåŸŸ/å­æµåŸŸä¸ºä¸»")
    print("2. æ—¶é—´èšç„¦: ç§¯é›ª-èé›ªæœŸ")
    print("3. é©±åŠ¨å› å­: æ—¥å°ºåº¦SWEã€æ°”æ¸©ã€è¾å°„")
    print("4. æ¨¡å‹è®¾ç½®: çƒ­èº«æœŸ + åˆ†æœŸæ ¡éªŒ")
    print("5. éªŒè¯æ–¹æ³•: äº¤å‰éªŒè¯ã€ç‹¬ç«‹æµ‹è¯•")
    
    print("\nğŸ’¡ æŠ€æœ¯å®ç°å»ºè®®:")
    print("1. ä½¿ç”¨pymannkendallåº“è¿›è¡Œè¶‹åŠ¿æ£€éªŒ")
    print("2. ä½¿ç”¨scipy.statsè¿›è¡ŒTheil-Senæ–œç‡ä¼°è®¡")
    print("3. å®ç°æ•°æ®åŒè´¨åŒ–æ£€æµ‹ç®—æ³•")
    print("4. å»ºç«‹30å¹´åŸºå‡†æœŸè®¡ç®—æ¡†æ¶")
    print("5. å¼€å‘æµåŸŸå°ºåº¦SWE-å¾„æµæ¨¡å‹")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æœç´¢SWEæ°”å€™å˜åŒ–ç ”ç©¶æ¡ˆä¾‹")
    print("=" * 60)
    print(f"æœç´¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æœç´¢æ°”å€™å˜åŒ–å½±å“SWEæ¡ˆä¾‹
    climate_repos = analyze_climate_change_swe()
    
    # æœç´¢èé›ªæ´ªæ°´é£é™©æ¡ˆä¾‹
    flood_repos = analyze_snowmelt_flood_risk()
    
    # ç”Ÿæˆæ€»ç»“
    generate_research_summary()
    
    # ä¿å­˜ç»“æœ
    results = {
        'climate_change_swe': climate_repos,
        'snowmelt_flood_risk': flood_repos,
        'search_time': datetime.now().isoformat()
    }
    
    with open('research_cases_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: research_cases_results.json")
    print("\nğŸ‰ æœç´¢å®Œæˆ!")

if __name__ == "__main__":
    main()
