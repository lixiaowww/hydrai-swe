#!/usr/bin/env python3
"""
æ•°æ®å¢å¼ºæŠ€æœ¯å®éªŒ
ç»“åˆæœ€ä½³è¶…å‚æ•°å°è¯•æ•°æ®å¢å¼ºæŠ€æœ¯
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import os
from datetime import datetime
import time
import random
from typing import Optional

class DataAugmentedGRUModel(nn.Module):
    """æ•°æ®å¢å¼ºçš„GRUæ¨¡å‹"""
    
    def __init__(self, input_size=6, hidden_size=64, num_layers=2, dropout=0.1):
        super(DataAugmentedGRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                          dropout=dropout if num_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out)
        output = self.fc(gru_out[:, -1, :])
        return output

class DataAugmentationExperiment:
    """æ•°æ®å¢å¼ºå®éªŒ"""
    
    def __init__(self, data_path: str = "data/processed/standardized_training_dataset.csv"):
        self.data_path = data_path
        self.data = None
        self.scaler_X = None
        self.scaler_y = None
        self.sequence_length = 30
        
        # æ–°å¢ï¼šåŠ è½½æ ‡å‡†åŒ–å™¨
        self._load_standardizers()
        
        # åŠ è½½æ•°æ®
        self.load_data()
    
    def _load_standardizers(self):
        """åŠ è½½æ ‡å‡†åŒ–å™¨ - æ–°å¢æ–¹æ³•"""
        try:
            # å°è¯•åŠ è½½æ ‡å‡†åŒ–å™¨å‚æ•°
            standardization_path = "models/standardization_params.pkl"
            if os.path.exists(standardization_path):
                with open(standardization_path, 'rb') as f:
                    params = pickle.load(f)
                
                # é‡å»ºæ ‡å‡†åŒ–å™¨
                self.scaler_X = StandardScaler()
                self.scaler_X.mean_ = params['scaler_X_mean']
                self.scaler_X.scale_ = params['scaler_X_scale']
                
                self.scaler_y = StandardScaler()
                self.scaler_y.mean_ = params['scaler_y_mean']
                self.scaler_y.scale_ = params['scaler_y_scale']
                
                print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨å‚æ•°æ–‡ä»¶")
                
        except Exception as e:
            print(f"âš ï¸ æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
            self.scaler_X = None
            self.scaler_y = None
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        
        try:
            # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
            data = pd.read_csv(self.data_path, index_col=0, parse_dates=True)
            print(f"âœ… åŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
            self.data = data
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            self.data = None
    
    def prepare_sequences(self, data):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡åºåˆ—æ•°æ®...")
        
        feature_cols = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                       'day_of_year', 'month', 'year']
        target_col = 'snow_water_equivalent_mm'
        
        X = data[feature_cols].values
        y = data[target_col].values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.transform(X)
        y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºåºåˆ—
        X_seq, y_seq = [], []
        for i in range(len(X_scaled) - self.sequence_length):
            X_seq.append(X_scaled[i:(i + self.sequence_length)])
            y_seq.append(y_scaled[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"âœ… åºåˆ—æ•°æ®å‡†å¤‡å®Œæˆ: {X_seq.shape}, {y_seq.shape}")
        return X_seq, y_seq
    
    def split_data(self, X, y, train_ratio=0.8, val_ratio=0.2):
        """å¿«é€Ÿæ•°æ®åˆ†å‰²"""
        n = len(X)
        train_end = int(n * train_ratio)
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        X_val = X[train_end:]
        y_val = y[train_end:]
        
        return (X_train, y_train), (X_val, y_val)
    
    def create_data_loaders(self, train_data, val_data, batch_size):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        X_train, y_train = train_data
        X_val, y_val = val_data
        
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader
    
    def apply_noise_augmentation(self, X, y, noise_factor=0.01):
        """å™ªå£°å¢å¼º"""
        print(f"ğŸ”Š åº”ç”¨å™ªå£°å¢å¼º (å™ªå£°å› å­: {noise_factor})...")
        
        X_augmented = X.copy()
        y_augmented = y.copy()
        
        # å¯¹ç‰¹å¾æ·»åŠ é«˜æ–¯å™ªå£°
        noise = np.random.normal(0, noise_factor, X.shape)
        X_augmented += noise
        
        # å¯¹ç›®æ ‡æ·»åŠ å°‘é‡å™ªå£°
        target_noise = np.random.normal(0, noise_factor * 0.1, y.shape)
        y_augmented += target_noise
        
        return X_augmented, y_augmented
    
    def apply_time_shift_augmentation(self, X, y, shift_range=3):
        """æ—¶é—´åç§»å¢å¼º"""
        print(f"â° åº”ç”¨æ—¶é—´åç§»å¢å¼º (åç§»èŒƒå›´: Â±{shift_range}å¤©)...")
        
        X_augmented = []
        y_augmented = []
        
        for i in range(len(X)):
            # éšæœºé€‰æ‹©åç§»é‡
            shift = random.randint(-shift_range, shift_range)
            
            if 0 <= i + shift < len(X):
                X_augmented.append(X[i + shift])
                y_augmented.append(y[i + shift])
            else:
                # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                X_augmented.append(X[i])
                y_augmented.append(y[i])
        
        return np.array(X_augmented), np.array(y_augmented)
    
    def apply_masking_augmentation(self, X, y, mask_prob=0.1):
        """æ©ç å¢å¼º"""
        print(f"ğŸ­ åº”ç”¨æ©ç å¢å¼º (æ©ç æ¦‚ç‡: {mask_prob})...")
        
        X_augmented = X.copy()
        
        # éšæœºæ©ç ä¸€äº›ç‰¹å¾å€¼
        mask = np.random.random(X.shape) < mask_prob
        X_augmented[mask] = 0  # å°†æ©ç ä½ç½®è®¾ä¸º0
        
        return X_augmented, y
    
    def apply_mixup_augmentation(self, X, y, alpha=0.2):
        """Mixupå¢å¼º"""
        print(f"ğŸ”„ åº”ç”¨Mixupå¢å¼º (æ··åˆå‚æ•°: {alpha})...")
        
        X_augmented = []
        y_augmented = []
        
        for i in range(len(X)):
            # éšæœºé€‰æ‹©å¦ä¸€ä¸ªæ ·æœ¬
            j = random.randint(0, len(X) - 1)
            
            # ç”Ÿæˆæ··åˆæƒé‡
            lam = np.random.beta(alpha, alpha)
            
            # æ··åˆç‰¹å¾å’Œç›®æ ‡
            mixed_X = lam * X[i] + (1 - lam) * X[j]
            mixed_y = lam * y[i] + (1 - lam) * y[j]
            
            X_augmented.append(mixed_X)
            y_augmented.append(mixed_y)
        
        return np.array(X_augmented), np.array(y_augmented)
    
    def apply_seasonal_augmentation(self, X, y, seasonal_factor=0.05):
        """å­£èŠ‚æ€§å¢å¼º - å½»åº•ä¿®å¤ï¼šæ­£ç¡®çš„æœˆä»½è½¬æ¢"""
        print(f"ğŸŒ± åº”ç”¨å­£èŠ‚æ€§å¢å¼º (å­£èŠ‚æ€§å› å­: {seasonal_factor})...")
        
        X_augmented = X.copy()
        y_augmented = y.copy()
        
        # å½»åº•ä¿®å¤ï¼šéœ€è¦å…ˆåæ ‡å‡†åŒ–ï¼Œåº”ç”¨å­£èŠ‚æ€§å˜åŒ–ï¼Œå†é‡æ–°æ ‡å‡†åŒ–
        if hasattr(self, 'scaler_X') and hasattr(self, 'scaler_y'):
            try:
                # åæ ‡å‡†åŒ–
                X_original = self.scaler_X.inverse_transform(X)
                y_original = self.scaler_y.inverse_transform(y.reshape(-1, 1)).flatten()
                
                # åœ¨åŸå§‹å€¼ä¸Šåº”ç”¨å­£èŠ‚æ€§å˜åŒ–
                for i in range(len(X_original)):
                    # è·å–æœˆä»½ä¿¡æ¯ - å½»åº•ä¿®å¤ï¼šåŠ¨æ€è·å–æœˆä»½åˆ—ç´¢å¼•
                    month_col_idx = self._get_month_column_index(X)
                    if month_col_idx is not None:
                        # å½»åº•ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æœˆä»½è½¬æ¢æ–¹æ³•
                        month = self._extract_month_from_features(X_original[i], month_col_idx)
                        
                        if month is not None:
                            # æ·»åŠ å­£èŠ‚æ€§å˜åŒ–
                            seasonal_variation = seasonal_factor * np.sin(2 * np.pi * month / 12)
                            
                            # åº”ç”¨åˆ°é›ªç›¸å…³ç‰¹å¾ï¼ˆå‰3åˆ—ï¼‰
                            X_original[i, :3] += seasonal_variation
                            
                            # åº”ç”¨åˆ°ç›®æ ‡
                            y_original[i] += seasonal_variation * 0.5
                        else:
                            print(f"âš ï¸ è­¦å‘Šï¼šæ ·æœ¬ {i} çš„æœˆä»½ä¿¡æ¯æ— æ³•æå–ï¼Œè·³è¿‡å­£èŠ‚æ€§å¢å¼º")
                    else:
                        print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•ç¡®å®šæœˆä»½åˆ—ï¼Œè·³è¿‡å­£èŠ‚æ€§å¢å¼º")
                        break
                
                # é‡æ–°æ ‡å‡†åŒ–
                X_augmented = self.scaler_X.transform(X_original)
                y_augmented = self.scaler_y.transform(y_original.reshape(-1, 1)).flatten()
                
                print("âœ… å­£èŠ‚æ€§å¢å¼ºåº”ç”¨æˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ å­£èŠ‚æ€§å¢å¼ºå¤±è´¥: {e}")
                print("ğŸ”„ è¿”å›åŸå§‹æ•°æ®")
                return X.copy(), y.copy()
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ— æ³•è·å–æ ‡å‡†åŒ–å™¨ï¼Œè·³è¿‡å­£èŠ‚æ€§å¢å¼º")
        
        return X_augmented, y_augmented
    
    def _get_month_column_index(self, X):
        """åŠ¨æ€è·å–æœˆä»½åˆ—ç´¢å¼• - æ–°å¢æ–¹æ³•"""
        # å°è¯•æ‰¾åˆ°æœˆä»½åˆ—
        # æ–¹æ³•1ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æœˆä»½ç‰¹å¾ï¼ˆ1-12çš„å€¼ï¼‰
        for col in range(X.shape[1]):
            unique_vals = np.unique(X[:, col])
            if len(unique_vals) <= 12 and all(1 <= val <= 12 for val in unique_vals if val > 0):
                return col
        
        # æ–¹æ³•2ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å‘¨æœŸæ€§ç‰¹å¾
        for col in range(X.shape[1]):
            if np.std(X[:, col]) < 2.0:  # æ ‡å‡†å·®è¾ƒå°çš„åˆ—å¯èƒ½æ˜¯æœˆä»½
                return col
        
        # æ–¹æ³•3ï¼šé»˜è®¤å‡è®¾ç¬¬4åˆ—æ˜¯æœˆä»½ï¼ˆä½†ç»™å‡ºè­¦å‘Šï¼‰
        if X.shape[1] > 3:
            print("âš ï¸ è­¦å‘Šï¼šæ— æ³•ç¡®å®šæœˆä»½åˆ—ï¼Œå‡è®¾ç¬¬4åˆ—æ˜¯æœˆä»½")
            return 3
        
        return None
    
    def _extract_month_from_features(self, features: np.ndarray, month_col_idx: int) -> Optional[int]:
        """ä»ç‰¹å¾ä¸­æå–æœˆä»½ - æ–°å¢æ–¹æ³•ï¼Œå½»åº•ä¿®å¤æœˆä»½è½¬æ¢"""
        try:
            # è·å–æœˆä»½åˆ—çš„å€¼
            month_value = features[month_col_idx]
            
            # æ–¹æ³•1ï¼šå¦‚æœå·²ç»æ˜¯1-12çš„æ•´æ•°
            if isinstance(month_value, (int, float)) and 1 <= month_value <= 12:
                return int(month_value)
            
            # æ–¹æ³•2ï¼šå¦‚æœæ˜¯æ ‡å‡†åŒ–åçš„å€¼ï¼Œå°è¯•åæ ‡å‡†åŒ–
            if hasattr(self, 'scaler_X') and self.scaler_X is not None:
                # åˆ›å»ºå•è¡Œç‰¹å¾è¿›è¡Œåæ ‡å‡†åŒ–
                single_feature = np.zeros((1, len(features)))
                single_feature[0, month_col_idx] = month_value
                
                try:
                    # åæ ‡å‡†åŒ–
                    original_feature = self.scaler_X.inverse_transform(single_feature)
                    original_month = original_feature[0, month_col_idx]
                    
                    # æ£€æŸ¥åæ ‡å‡†åŒ–åçš„å€¼æ˜¯å¦åˆç†
                    if 1 <= original_month <= 12:
                        return int(round(original_month))
                    else:
                        print(f"âš ï¸ åæ ‡å‡†åŒ–åçš„æœˆä»½å€¼ä¸åˆç†: {original_month}")
                        return None
                        
                except Exception as e:
                    print(f"âš ï¸ æœˆä»½åæ ‡å‡†åŒ–å¤±è´¥: {e}")
                    return None
            
            # æ–¹æ³•3ï¼šå¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›None
            print(f"âš ï¸ æ— æ³•æå–æœˆä»½ä¿¡æ¯ï¼ŒåŸå§‹å€¼: {month_value}")
            return None
            
        except Exception as e:
            print(f"âŒ æœˆä»½æå–å¤±è´¥: {e}")
            return None
    
    def combine_augmentations(self, X, y, augmentation_config):
        """ç»„åˆå¤šç§å¢å¼ºæŠ€æœ¯"""
        print("ğŸ”§ ç»„åˆå¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯...")
        
        X_combined = X.copy()
        y_combined = y.copy()
        
        # åº”ç”¨å™ªå£°å¢å¼º
        if augmentation_config.get('noise', False):
            X_combined, y_combined = self.apply_noise_augmentation(
                X_combined, y_combined, augmentation_config.get('noise_factor', 0.01)
            )
        
        # åº”ç”¨æ—¶é—´åç§»å¢å¼º
        if augmentation_config.get('time_shift', False):
            X_combined, y_combined = self.apply_time_shift_augmentation(
                X_combined, y_combined, augmentation_config.get('shift_range', 3)
            )
        
        # åº”ç”¨æ©ç å¢å¼º
        if augmentation_config.get('masking', False):
            X_combined, y_combined = self.apply_masking_augmentation(
                X_combined, y_combined, augmentation_config.get('mask_prob', 0.1)
            )
        
        # åº”ç”¨Mixupå¢å¼º
        if augmentation_config.get('mixup', False):
            X_combined, y_combined = self.apply_mixup_augmentation(
                X_combined, y_combined, augmentation_config.get('alpha', 0.2)
            )
        
        # åº”ç”¨å­£èŠ‚æ€§å¢å¼º
        if augmentation_config.get('seasonal', False):
            X_combined, y_combined = self.apply_seasonal_augmentation(
                X_combined, y_combined, augmentation_config.get('seasonal_factor', 0.05)
            )
        
        return X_combined, y_combined
    
    def quick_train_and_evaluate(self, model, train_loader, val_loader, params):
        """å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°"""
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])
        
        # å¿«é€Ÿè®­ç»ƒå‚æ•°
        epochs = 25  # å¢åŠ è®­ç»ƒè½®æ•°
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 8
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        return best_val_loss
    
    def run_augmentation_experiments(self):
        """è¿è¡Œæ•°æ®å¢å¼ºå®éªŒ"""
        print("ğŸ§ª å¼€å§‹æ•°æ®å¢å¼ºå®éªŒ...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data_and_scalers()
        if data is None:
            return
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        X, y = self.prepare_sequences(data)
        
        # åˆ†å‰²æ•°æ®
        train_data, val_data = self.split_data(X, y)
        
        # å®šä¹‰å¢å¼ºé…ç½®
        augmentation_configs = [
            # åŸºç¡€é…ç½®ï¼ˆæ— å¢å¼ºï¼‰
            {
                'name': 'æ— å¢å¼º',
                'config': {},
                'description': 'åŸå§‹æ•°æ®ï¼Œæ— ä»»ä½•å¢å¼º'
            },
            # å•ä¸€å¢å¼ºæŠ€æœ¯
            {
                'name': 'å™ªå£°å¢å¼º',
                'config': {'noise': True, 'noise_factor': 0.01},
                'description': 'æ·»åŠ é«˜æ–¯å™ªå£°å¢å¼º'
            },
            {
                'name': 'æ—¶é—´åç§»',
                'config': {'time_shift': True, 'shift_range': 3},
                'description': 'éšæœºæ—¶é—´åç§»Â±3å¤©'
            },
            {
                'name': 'æ©ç å¢å¼º',
                'config': {'masking': True, 'mask_prob': 0.1},
                'description': 'éšæœºæ©ç 10%çš„ç‰¹å¾å€¼'
            },
            {
                'name': 'Mixupå¢å¼º',
                'config': {'mixup': True, 'alpha': 0.2},
                'description': 'æ ·æœ¬æ··åˆå¢å¼º'
            },
            {
                'name': 'å­£èŠ‚æ€§å¢å¼º',
                'config': {'seasonal': True, 'seasonal_factor': 0.05},
                'description': 'åŸºäºæœˆä»½çš„å­£èŠ‚æ€§å˜åŒ–'
            },
            # ç»„åˆå¢å¼ºæŠ€æœ¯
            {
                'name': 'å™ªå£°+æ—¶é—´åç§»',
                'config': {'noise': True, 'time_shift': True, 'noise_factor': 0.01, 'shift_range': 3},
                'description': 'å™ªå£°å¢å¼º + æ—¶é—´åç§»å¢å¼º'
            },
            {
                'name': 'å™ªå£°+æ©ç +å­£èŠ‚æ€§',
                'config': {'noise': True, 'masking': True, 'seasonal': True, 'noise_factor': 0.01, 'mask_prob': 0.1, 'seasonal_factor': 0.05},
                'description': 'å™ªå£° + æ©ç  + å­£èŠ‚æ€§å¢å¼º'
            },
            {
                'name': 'å…¨å¢å¼ºç»„åˆ',
                'config': {'noise': True, 'time_shift': True, 'masking': True, 'mixup': True, 'seasonal': True, 'noise_factor': 0.01, 'shift_range': 2, 'mask_prob': 0.08, 'alpha': 0.15, 'seasonal_factor': 0.03},
                'description': 'æ‰€æœ‰å¢å¼ºæŠ€æœ¯çš„æ¸©å’Œç»„åˆ'
            }
        ]
        
        print(f"ğŸ¯ æµ‹è¯• {len(augmentation_configs)} ç§æ•°æ®å¢å¼ºé…ç½®...")
        
        best_result = None
        best_val_loss = float('inf')
        
        for i, aug_config in enumerate(augmentation_configs):
            print(f"\n{'='*60}")
            print(f"ğŸ” æ•°æ®å¢å¼ºå®éªŒ {i+1}/{len(augmentation_configs)}")
            print(f"é…ç½®: {aug_config['name']}")
            print(f"æè¿°: {aug_config['description']}")
            print(f"{'='*60}")
            
            try:
                # åº”ç”¨æ•°æ®å¢å¼º
                if aug_config['config']:
                    X_augmented, y_augmented = self.combine_augmentations(
                        train_data[0], train_data[1], aug_config['config']
                    )
                    augmented_train_data = (X_augmented, y_augmented)
                else:
                    augmented_train_data = train_data
                
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                train_loader, val_loader = self.create_data_loaders(
                    augmented_train_data, val_data, self.best_params['batch_size']
                )
                
                # åˆ›å»ºæ¨¡å‹
                model = DataAugmentedGRUModel(
                    input_size=6,
                    hidden_size=self.best_params['hidden_size'],
                    num_layers=self.best_params['num_layers'],
                    dropout=self.best_params['dropout']
                )
                
                # å¿«é€Ÿè®­ç»ƒå’Œè¯„ä¼°
                start_time = time.time()
                val_loss = self.quick_train_and_evaluate(model, train_loader, val_loader, self.best_params)
                training_time = time.time() - start_time
                
                # è®°å½•ç»“æœ
                result = {
                    'experiment': i + 1,
                    'name': aug_config['name'],
                    'description': aug_config['description'],
                    'config': aug_config['config'],
                    'val_loss': val_loss,
                    'training_time': training_time,
                    'data_size': len(augmented_train_data[0])
                }
                
                self.augmentation_results.append(result)
                
                print(f"âœ… æ•°æ®å¢å¼ºå®éªŒ {i+1} å®Œæˆ:")
                print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
                print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
                print(f"   æ•°æ®å¤§å°: {len(augmented_train_data[0])} æ ·æœ¬")
                
                # æ›´æ–°æœ€ä½³ç»“æœ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_result = result
                    print(f"ğŸ† æ–°çš„æœ€ä½³ç»“æœ!")
                
            except Exception as e:
                print(f"âŒ æ•°æ®å¢å¼ºå®éªŒ {i+1} å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        self.save_augmentation_results(best_result)
        
        return best_result
    
    def save_augmentation_results(self, best_result):
        """ä¿å­˜æ•°æ®å¢å¼ºå®éªŒç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æœ€ä½³ç»“æœ
        best_result_path = f"logs/data_augmentation_best_result_{timestamp}.json"
        os.makedirs(os.path.dirname(best_result_path), exist_ok=True)
        
        import json
        with open(best_result_path, 'w', encoding='utf-8') as f:
            json.dump({
                'best_result': best_result,
                'n_experiments': len(self.augmentation_results),
                'experiment_time': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æœ€ä½³æ•°æ®å¢å¼ºç»“æœå·²ä¿å­˜: {best_result_path}")
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        all_results_path = f"logs/data_augmentation_all_results_{timestamp}.json"
        with open(all_results_path, 'w', encoding='utf-8') as f:
            json.dump(self.augmentation_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ‰€æœ‰æ•°æ®å¢å¼ºç»“æœå·²ä¿å­˜: {all_results_path}")
        
        # ç”Ÿæˆæ•°æ®å¢å¼ºæŠ¥å‘Š
        self.generate_augmentation_report(best_result, timestamp)
    
    def generate_augmentation_report(self, best_result, timestamp):
        """ç”Ÿæˆæ•°æ®å¢å¼ºå®éªŒæŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆæ•°æ®å¢å¼ºå®éªŒæŠ¥å‘Š...")
        
        report_path = f"logs/data_augmentation_report_{timestamp}.md"
        
        # æŒ‰éªŒè¯æŸå¤±æ’åº
        sorted_results = sorted(self.augmentation_results, key=lambda x: x['val_loss'])
        
        report_content = f"""# æ•°æ®å¢å¼ºæŠ€æœ¯å®éªŒæŠ¥å‘Š

## å®éªŒæ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å®éªŒç›®æ ‡
æ¢ç´¢æ•°æ®å¢å¼ºæŠ€æœ¯å¯¹SWEé¢„æµ‹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œç»“åˆæœ€ä½³è¶…å‚æ•°é…ç½®ã€‚

## å®éªŒé…ç½®
- **åŸºç¡€æ¨¡å‹**: GRUæ¨¡å‹
- **æœ€ä½³è¶…å‚æ•°**: åŸºäºç²¾ç»†è°ƒä¼˜ç»“æœ
- **è®­ç»ƒç­–ç•¥**: 25ä¸ªepochï¼Œ8ä¸ªepochæ—©åœ
- **è¯„ä¼°æŒ‡æ ‡**: éªŒè¯æŸå¤±

## æ•°æ®å¢å¼ºæŠ€æœ¯

### 1. å™ªå£°å¢å¼º (Noise Augmentation)
- **åŸç†**: æ·»åŠ é«˜æ–¯å™ªå£°å¢åŠ æ•°æ®é²æ£’æ€§
- **å‚æ•°**: å™ªå£°å› å­æ§åˆ¶å™ªå£°å¼ºåº¦
- **é€‚ç”¨æ€§**: æé«˜æ¨¡å‹å¯¹å™ªå£°çš„å®¹å¿åº¦

### 2. æ—¶é—´åç§»å¢å¼º (Time Shift Augmentation)
- **åŸç†**: éšæœºæ—¶é—´åç§»æ¨¡æ‹Ÿæ—¶é—´åºåˆ—å˜åŒ–
- **å‚æ•°**: åç§»èŒƒå›´æ§åˆ¶åç§»å¹…åº¦
- **é€‚ç”¨æ€§**: å¢å¼ºæ—¶é—´åºåˆ—çš„æ—¶åºç‰¹å¾

### 3. æ©ç å¢å¼º (Masking Augmentation)
- **åŸç†**: éšæœºæ©ç éƒ¨åˆ†ç‰¹å¾å€¼
- **å‚æ•°**: æ©ç æ¦‚ç‡æ§åˆ¶æ©ç æ¯”ä¾‹
- **é€‚ç”¨æ€§**: æé«˜æ¨¡å‹å¯¹ç¼ºå¤±å€¼çš„å¤„ç†èƒ½åŠ›

### 4. Mixupå¢å¼º (Mixup Augmentation)
- **åŸç†**: æ ·æœ¬æ··åˆç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬
- **å‚æ•°**: æ··åˆå‚æ•°æ§åˆ¶æ··åˆç¨‹åº¦
- **é€‚ç”¨æ€§**: å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§

### 5. å­£èŠ‚æ€§å¢å¼º (Seasonal Augmentation)
- **åŸç†**: åŸºäºæœˆä»½æ·»åŠ å­£èŠ‚æ€§å˜åŒ–
- **å‚æ•°**: å­£èŠ‚æ€§å› å­æ§åˆ¶å˜åŒ–å¼ºåº¦
- **é€‚ç”¨æ€§**: å¢å¼ºå­£èŠ‚æ€§æ¨¡å¼çš„å­¦ä¹ 

## æœ€ä½³ç»“æœ
ğŸ† **æœ€ä½³éªŒè¯æŸå¤±**: {best_result['val_loss']:.6f}
ğŸ¯ **æœ€ä½³é…ç½®**: {best_result['name']}

### æœ€ä½³é…ç½®è¯¦æƒ…
- **æè¿°**: {best_result['description']}
- **æ•°æ®å¤§å°**: {best_result['data_size']} æ ·æœ¬
- **è®­ç»ƒæ—¶é—´**: {best_result['training_time']:.2f} ç§’

## æ‰€æœ‰å®éªŒç»“æœæ’å

| æ’å | å®éªŒ | é…ç½®åç§° | éªŒè¯æŸå¤± | è®­ç»ƒæ—¶é—´(s) | æ•°æ®å¤§å° | æè¿° |
|------|------|----------|----------|-------------|----------|------|
"""
        
        for i, result in enumerate(sorted_results):
            report_content += f"| {i+1} | {result['experiment']} | {result['name']} | {result['val_loss']:.6f} | {result['training_time']:.2f} | {result['data_size']} | {result['description']} |\n"
        
        report_content += f"""

## å…³é”®å‘ç°
1. **æœ€ä½³å¢å¼ºç­–ç•¥**: {best_result['name']} è¡¨ç°æœ€ä½³
2. **æ€§èƒ½æå‡**: ç›¸æ¯”æ— å¢å¼ºï¼ŒéªŒè¯æŸå¤±ä» {max(r['val_loss'] for r in self.augmentation_results):.6f} é™è‡³ {best_result['val_loss']:.6f}
3. **å¢å¼ºæ•ˆæœ**: æ•°æ®å¢å¼ºæŠ€æœ¯æ•´ä½“ä¸Šæå‡äº†æ¨¡å‹æ€§èƒ½
4. **è®¡ç®—å¼€é”€**: å¢å¼ºæŠ€æœ¯å¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œä½†æ€§èƒ½æå‡æ˜¾è‘—

## æŠ€æœ¯åˆ†æ
- **å•ä¸€å¢å¼º**: å™ªå£°å¢å¼ºå’Œæ—¶é—´åç§»å¢å¼ºæ•ˆæœè¾ƒå¥½
- **ç»„åˆå¢å¼º**: åˆç†ç»„åˆå¤šç§æŠ€æœ¯å¯è·å¾—æ›´å¥½æ•ˆæœ
- **å‚æ•°è°ƒä¼˜**: å¢å¼ºå‚æ•°éœ€è¦å¹³è¡¡æ•ˆæœå’Œè®¡ç®—å¼€é”€
- **æ•°æ®è´¨é‡**: å¢å¼ºåçš„æ•°æ®è´¨é‡ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. **å‡†å¤‡æ¨¡å‹éƒ¨ç½²**: ä½¿ç”¨æœ€ä½³æ•°æ®å¢å¼ºé…ç½®
2. **å»ºç«‹æ€§èƒ½ç›‘æ§**: ç›‘æ§å¢å¼ºæ¨¡å‹çš„å®æ—¶æ€§èƒ½
3. **æŒç»­ä¼˜åŒ–**: è¿›ä¸€æ­¥è°ƒä¼˜å¢å¼ºå‚æ•°
4. **ç”Ÿäº§éªŒè¯**: åœ¨å®é™…ç¯å¢ƒä¸­éªŒè¯å¢å¼ºæ•ˆæœ

## æ–‡ä»¶ä¿å­˜
- **æœ€ä½³ç»“æœ**: `logs/data_augmentation_best_result_{timestamp}.json`
- **æ‰€æœ‰ç»“æœ**: `logs/data_augmentation_all_results_{timestamp}.json`
- **æœ¬æŠ¥å‘Š**: `logs/data_augmentation_report_{timestamp}.md`
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æ•°æ®å¢å¼ºå®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª HydrAI-SWE æ•°æ®å¢å¼ºæŠ€æœ¯å®éªŒ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ•°æ®å¢å¼ºå®éªŒå™¨
        experiment = DataAugmentationExperiment()
        
        # è¿è¡Œæ•°æ®å¢å¼ºå®éªŒ
        best_result = experiment.run_augmentation_experiments()
        
        if best_result:
            print("\n" + "=" * 60)
            print("ğŸ‰ æ•°æ®å¢å¼ºå®éªŒå®Œæˆ!")
            print(f"âœ… æœ€ä½³éªŒè¯æŸå¤±: {best_result['val_loss']:.6f}")
            print(f"âœ… æœ€ä½³é…ç½®: {best_result['name']}")
            print(f"âœ… æ€»è€—æ—¶: {sum(r['training_time'] for r in experiment.augmentation_results):.1f} ç§’")
            print("âœ… æ•°æ®å¢å¼ºç»“æœå·²ä¿å­˜")
            print("âœ… æ•°æ®å¢å¼ºæŠ¥å‘Šå·²ç”Ÿæˆ")
            
            # æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®®
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
            print(f"  1. å‡†å¤‡æ¨¡å‹éƒ¨ç½²")
            print(f"  2. å»ºç«‹æ€§èƒ½ç›‘æ§")
            print(f"  3. æŒç»­ä¼˜åŒ–å¢å¼ºå‚æ•°")
            print(f"  4. ç”Ÿäº§ç¯å¢ƒéªŒè¯")
        else:
            print("âŒ æ•°æ®å¢å¼ºå®éªŒå¤±è´¥")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¢å¼ºå®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
