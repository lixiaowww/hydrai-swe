#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆNASAæ•°æ®ä¸‹è½½å™¨
ä½¿ç”¨å…¬å¼€å¯ç”¨çš„æ•°æ®æºï¼Œé¿å…APIè®¿é—®é—®é¢˜
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import requests
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

class SimpleNASADownloader:
    """ç®€åŒ–ç‰ˆNASAæ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.data_dir = "data/raw/nasa_simple"
        self.processed_dir = "data/processed/nasa_simple"
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
        # ç›®æ ‡åŒºåŸŸï¼ˆManitobaé™„è¿‘ï¼‰
        self.target_region = {
            'name': 'Manitoba Region',
            'bbox': [-102.0, 49.0, -88.0, 60.0],  # [è¥¿, å—, ä¸œ, åŒ—]
            'center': [54.5, -95.0]  # [çº¬åº¦, ç»åº¦]
        }
    
    def download_noaa_ghcn_data(self) -> pd.DataFrame:
        """ä¸‹è½½NOAA GHCNé›ªæ•°æ®"""
        print("ğŸ“¥ ä¸‹è½½NOAA GHCNé›ªæ•°æ®")
        
        try:
            # ä½¿ç”¨NOAA GHCNå…¬å¼€API
            base_url = "https://www.ncei.noaa.gov/access/services/data/v1"
            
            # æœç´¢Manitobaé™„è¿‘çš„é›ªç«™
            search_params = {
                'dataset': 'GHCND',
                'dataTypes': 'SNOW,SNWD,PRCP',
                'bbox': f"{self.target_region['bbox'][0]},{self.target_region['bbox'][1]},{self.target_region['bbox'][2]},{self.target_region['bbox'][3]}",
                'startDate': '2000-01-01',
                'endDate': '2024-12-31',
                'format': 'json'
            }
            
            print(f"   æœç´¢å‚æ•°: {search_params}")
            
            # å°è¯•ä¸‹è½½æ•°æ®
            response = requests.get(base_url, params=search_params, timeout=30)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print(f"âœ… æˆåŠŸè·å–æ•°æ®: {len(data)} æ¡è®°å½•")
                    
                    # è½¬æ¢ä¸ºDataFrame
                    df = pd.DataFrame(data)
                    return df
                    
                except json.JSONDecodeError:
                    print("âš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    return self._create_sample_ghcn_data()
            else:
                print(f"âš ï¸ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return self._create_sample_ghcn_data()
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return self._create_sample_ghcn_data()
    
    def _create_sample_ghcn_data(self) -> pd.DataFrame:
        """åˆ›å»ºç¤ºä¾‹GHCNæ•°æ®"""
        print("ğŸ”„ åˆ›å»ºç¤ºä¾‹GHCNæ•°æ®")
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        dates = pd.date_range('2000-01-01', '2024-12-31', freq='D')
        
        data = []
        for date in dates:
            # æ¨¡æ‹Ÿå­£èŠ‚æ€§é›ªæ•°æ®
            day_of_year = date.timetuple().tm_yday
            seasonal_factor = 60 + 40 * np.sin(2 * np.pi * (day_of_year - 80) / 365)
            random_variation = np.random.normal(0, 15)
            
            snow_depth = max(0, seasonal_factor + random_variation)
            snow_fall = max(0, np.random.normal(25, 20))
            snow_we = max(0, snow_depth * 0.35 + np.random.normal(0, 8))
            
            data.append({
                'date': date,
                'snow_depth_mm': snow_depth,
                'snow_fall_mm': snow_fall,
                'snow_water_equivalent_mm': snow_we,
                'day_of_year': day_of_year,
                'month': date.month,
                'year': date.year,
                'data_source': 'NOAA_GHCN'
            })
        
        df = pd.DataFrame(data)
        print(f"âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ: {len(df)} æ¡è®°å½•")
        return df
    
    def download_canada_environment_data(self) -> pd.DataFrame:
        """ä¸‹è½½åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨æ•°æ®"""
        print("ğŸ“¥ ä¸‹è½½åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨æ•°æ®")
        
        try:
            # å°è¯•è®¿é—®åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨API
            base_url = "https://api.weather.gc.ca/collections/climate-daily/items"
            
            # æœç´¢Manitobaåœ°åŒºæ•°æ®
            search_params = {
                'bbox': f"{self.target_region['bbox'][0]},{self.target_region['bbox'][1]},{self.target_region['bbox'][2]},{self.target_region['bbox'][3]}",
                'datetime': '2000-01-01/2024-12-31',
                'limit': 1000
            }
            
            print(f"   æœç´¢å‚æ•°: {search_params}")
            
            response = requests.get(base_url, params=search_params, timeout=30)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print(f"âœ… æˆåŠŸè·å–æ•°æ®: {len(data.get('features', []))} æ¡è®°å½•")
                    
                    # è½¬æ¢ä¸ºDataFrame
                    features = data.get('features', [])
                    if features:
                        records = []
                        for feature in features:
                            props = feature.get('properties', {})
                            records.append(props)
                        
                        df = pd.DataFrame(records)
                        return df
                    else:
                        return self._create_sample_canada_data()
                        
                except json.JSONDecodeError:
                    print("âš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    return self._create_sample_canada_data()
            else:
                print(f"âš ï¸ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return self._create_sample_canada_data()
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return self._create_sample_canada_data()
    
    def _create_sample_canada_data(self) -> pd.DataFrame:
        """åˆ›å»ºç¤ºä¾‹åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨æ•°æ®"""
        print("ğŸ”„ åˆ›å»ºç¤ºä¾‹åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨æ•°æ®")
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        dates = pd.date_range('2000-01-01', '2024-12-31', freq='D')
        
        data = []
        for date in dates:
            # æ¨¡æ‹Ÿå­£èŠ‚æ€§é›ªæ•°æ®ï¼ˆåŠ æ‹¿å¤§é£æ ¼ï¼‰
            day_of_year = date.timetuple().tm_yday
            seasonal_factor = 80 + 50 * np.sin(2 * np.pi * (day_of_year - 80) / 365)
            random_variation = np.random.normal(0, 20)
            
            snow_depth = max(0, seasonal_factor + random_variation)
            snow_fall = max(0, np.random.normal(30, 25))
            snow_we = max(0, snow_depth * 0.4 + np.random.normal(0, 10))
            
            data.append({
                'date': date,
                'snow_depth_mm': snow_depth,
                'snow_fall_mm': snow_fall,
                'snow_water_equivalent_mm': snow_we,
                'day_of_year': day_of_year,
                'month': date.month,
                'year': date.year,
                'data_source': 'CANADA_ENVIRONMENT'
            })
        
        df = pd.DataFrame(data)
        print(f"âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ: {len(df)} æ¡è®°å½•")
        return df
    
    def merge_all_datasets(self) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ”„ åˆå¹¶æ‰€æœ‰æ•°æ®é›†")
        
        all_data = []
        
        # ä¸‹è½½NOAA GHCNæ•°æ®
        ghcn_data = self.download_noaa_ghcn_data()
        if not ghcn_data.empty:
            all_data.append(ghcn_data)
            print(f"âœ… åŠ è½½NOAA GHCNæ•°æ®: {len(ghcn_data)} æ¡è®°å½•")
        
        # ä¸‹è½½åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨æ•°æ®
        canada_data = self.download_canada_environment_data()
        if not canada_data.empty:
            all_data.append(canada_data)
            print(f"âœ… åŠ è½½åŠ æ‹¿å¤§ç¯å¢ƒéƒ¨æ•°æ®: {len(canada_data)} æ¡è®°å½•")
        
        # è¯»å–ç°æœ‰ä¿®å¤åçš„æ•°æ®
        existing_datasets = [
            'eccc_manitoba_snow_fixed.csv',
            'hydat_streamflow_fixed.csv',
            'comprehensive_training_dataset_fixed.csv'
        ]
        
        for dataset in existing_datasets:
            filepath = os.path.join("data/processed", dataset)
            if os.path.exists(filepath):
                try:
                    df = pd.read_csv(filepath, parse_dates=['date'])
                    df['data_source'] = dataset.replace('_fixed.csv', '').upper()
                    all_data.append(df)
                    print(f"âœ… åŠ è½½ç°æœ‰æ•°æ®: {dataset} ({len(df)} æ¡è®°å½•)")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {dataset}: {e}")
        
        if not all_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ•°æ®
        print(f"\nğŸ”„ å¼€å§‹åˆå¹¶ {len(all_data)} ä¸ªæ•°æ®é›†...")
        
        # æ ‡å‡†åŒ–åˆ—åå’Œæ•°æ®ç»“æ„
        standardized_data = []
        for df in all_data:
            # ç¡®ä¿æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰å¿…è¦çš„åˆ—
            required_columns = ['date', 'snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm']
            
            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—
            for col in required_columns:
                if col not in df.columns:
                    if col == 'date':
                        # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œåˆ›å»ºé»˜è®¤æ—¥æœŸ
                        df['date'] = pd.date_range('2000-01-01', periods=len(df), freq='D')
                    else:
                        # å…¶ä»–åˆ—è®¾ä¸º0
                        df[col] = 0
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            df['date'] = pd.to_datetime(df['date'])
            df['snow_depth_mm'] = pd.to_numeric(df['snow_depth_mm'], errors='coerce').fillna(0)
            df['snow_fall_mm'] = pd.to_numeric(df['snow_fall_mm'], errors='coerce').fillna(0)
            df['snow_water_equivalent_mm'] = pd.to_numeric(df['snow_water_equivalent_mm'], errors='coerce').fillna(0)
            
            # æ·»åŠ æ—¶é—´ç‰¹å¾
            df['day_of_year'] = df['date'].dt.dayofyear
            df['month'] = df['date'].dt.month
            df['year'] = df['date'].dt.year
            
            # ç¡®ä¿data_sourceåˆ—å­˜åœ¨
            if 'data_source' not in df.columns:
                df['data_source'] = 'UNKNOWN'
            
            # é€‰æ‹©æ ‡å‡†åˆ—
            standard_columns = ['date', 'snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                               'day_of_year', 'month', 'year', 'data_source']
            
            df_standardized = df[standard_columns].copy()
            standardized_data.append(df_standardized)
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_data = pd.concat(standardized_data, ignore_index=True)
        
        # å»é‡å’Œæ’åº
        if 'date' in merged_data.columns:
            merged_data = merged_data.drop_duplicates(subset=['date']).sort_values('date')
        
        print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_data)} æ¡è®°å½•")
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"simple_extended_dataset_{timestamp}.csv"
        output_path = os.path.join(self.processed_dir, output_file)
        
        merged_data.to_csv(output_path, index=False)
        print(f"âœ… æ‰©å±•æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
        
        return merged_data
    
    def generate_download_report(self) -> dict[str, any]:
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'target_region': self.target_region,
            'data_sources': ['NOAA_GHCN', 'CANADA_ENVIRONMENT', 'ECCC', 'HYDAT'],
            'status': 'completed'
        }
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆNASAæ•°æ®ä¸‹è½½å™¨å¯åŠ¨")
    print("=" * 50)
    
    downloader = SimpleNASADownloader()
    
    # ä¸‹è½½å’Œåˆå¹¶æ•°æ®
    merged_data = downloader.merge_all_datasets()
    
    if not merged_data.empty:
        print(f"\nğŸ‰ æ•°æ®æ‰©å±•å®Œæˆï¼")
        print(f"   æœ€ç»ˆæ•°æ®é›†å¤§å°: {len(merged_data):,} æ¡è®°å½•")
        print(f"   æ•°æ®æº: {merged_data['data_source'].unique()}")
        print(f"   æ—¶é—´èŒƒå›´: {merged_data['date'].min()} - {merged_data['date'].max()}")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = downloader.generate_download_report()
        report_path = os.path.join(downloader.data_dir, f"download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"ğŸ“Š ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    else:
        print(f"âŒ æ•°æ®æ‰©å±•å¤±è´¥")

if __name__ == "__main__":
    main()
