#!/usr/bin/env python3
"""
ä¸‹è½½çœŸå®æ•°æ®
ä»æœç´¢åˆ°çš„å¯ç”¨æ•°æ®æºä¸‹è½½å®é™…æ•°æ®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import requests
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
import json
from typing import Dict, List, Optional
import time
import zipfile
import io
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealDataDownloader:
    """çœŸå®æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def download_noaa_daily_summaries(self) -> Optional[str]:
        """ä¸‹è½½NOAAæ¯æ—¥æ‘˜è¦æ•°æ®"""
        try:
            logger.info("ğŸ“¥ ä¸‹è½½NOAAæ¯æ—¥æ‘˜è¦æ•°æ®...")
            
            # åˆ›å»ºä¸‹è½½ç›®å½•
            download_dir = "data/real/noaa_daily"
            os.makedirs(download_dir, exist_ok=True)
            
            # å°è¯•ä¸‹è½½æœ€è¿‘çš„æ¯æ—¥æ‘˜è¦æ•°æ®
            base_url = "https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/"
            
            # è·å–å¯ç”¨å¹´ä»½åˆ—è¡¨
            response = self.session.get(base_url, timeout=10)
            if response.status_code != 200:
                logger.warning("âš ï¸ æ— æ³•è®¿é—®NOAAåŸºç¡€URL")
                return None
            
            # å°è¯•ä¸‹è½½2024å¹´çš„æ•°æ®
            year = 2024
            year_url = f"{base_url}{year}/"
            
            response = self.session.get(year_url, timeout=10)
            if response.status_code == 200:
                # å°è¯•ä¸‹è½½ä¸€ä¸ªå…·ä½“æ–‡ä»¶
                sample_file = f"{year_url}01001099999.csv"
                response = self.session.get(sample_file, timeout=30)
                
                if response.status_code == 200:
                    # ä¿å­˜æ•°æ®
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filename = f"noaa_daily_{year}_sample_{timestamp}.csv"
                    filepath = os.path.join(download_dir, filename)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    logger.info(f"âœ… NOAAæ¯æ—¥æ‘˜è¦æ•°æ®å·²ä¿å­˜: {filepath}")
                    return filepath
                else:
                    logger.warning(f"âš ï¸ æ— æ³•ä¸‹è½½NOAA {year}å¹´æ•°æ®: HTTP {response.status_code}")
            else:
                logger.warning(f"âš ï¸ æ— æ³•è®¿é—®NOAA {year}å¹´ç›®å½•: HTTP {response.status_code}")
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½NOAAæ¯æ—¥æ‘˜è¦æ•°æ®å¤±è´¥: {e}")
            return None
    
    def download_noaa_hourly_data(self) -> Optional[str]:
        """ä¸‹è½½NOAAå°æ—¶æ•°æ®"""
        try:
            logger.info("ğŸ“¥ ä¸‹è½½NOAAå°æ—¶æ•°æ®...")
            
            # åˆ›å»ºä¸‹è½½ç›®å½•
            download_dir = "data/real/noaa_hourly"
            os.makedirs(download_dir, exist_ok=True)
            
            # å°è¯•ä¸‹è½½æœ€è¿‘çš„å°æ—¶æ•°æ®
            base_url = "https://www.ncei.noaa.gov/data/global-hourly/access/"
            
            # å°è¯•ä¸‹è½½2024å¹´çš„æ•°æ®
            year = 2024
            year_url = f"{base_url}{year}/"
            
            response = self.session.get(year_url, timeout=10)
            if response.status_code == 200:
                # å°è¯•ä¸‹è½½ä¸€ä¸ªå…·ä½“æ–‡ä»¶
                sample_file = f"{year_url}01001099999.csv"
                response = self.session.get(sample_file, timeout=30)
                
                if response.status_code == 200:
                    # ä¿å­˜æ•°æ®
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filename = f"noaa_hourly_{year}_sample_{timestamp}.csv"
                    filepath = os.path.join(download_dir, filename)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    logger.info(f"âœ… NOAAå°æ—¶æ•°æ®å·²ä¿å­˜: {filepath}")
                    return filepath
                else:
                    logger.warning(f"âš ï¸ æ— æ³•ä¸‹è½½NOAA {year}å¹´å°æ—¶æ•°æ®: HTTP {response.status_code}")
            else:
                logger.warning(f"âš ï¸ æ— æ³•è®¿é—®NOAA {year}å¹´å°æ—¶ç›®å½•: HTTP {response.status_code}")
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½NOAAå°æ—¶æ•°æ®å¤±è´¥: {e}")
            return None
    
    def download_eobs_daily_data(self) -> Optional[str]:
        """ä¸‹è½½EOBSæ¯æ—¥æ•°æ®"""
        try:
            logger.info("ğŸ“¥ ä¸‹è½½EOBSæ¯æ—¥æ•°æ®...")
            
            # åˆ›å»ºä¸‹è½½ç›®å½•
            download_dir = "data/real/eobs_daily"
            os.makedirs(download_dir, exist_ok=True)
            
            # EOBSæ•°æ®ä¸‹è½½é¡µé¢
            eobs_url = "https://www.ecad.eu/download/ensembles/download.php"
            
            response = self.session.get(eobs_url, timeout=10)
            if response.status_code == 200:
                # å°è¯•ä¸‹è½½ä¸€ä¸ªå…·ä½“çš„æ•°æ®æ–‡ä»¶
                # EOBSé€šå¸¸æä¾›æ¸©åº¦ã€é™æ°´ç­‰æ•°æ®
                sample_url = "https://www.ecad.eu/download/ensembles/data/Grid_0.1deg_reg_ensemble/tg_0.1deg_reg_2024.01.nc"
                
                response = self.session.get(sample_url, timeout=30)
                if response.status_code == 200:
                    # ä¿å­˜æ•°æ®
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filename = f"eobs_daily_sample_{timestamp}.nc"
                    filepath = os.path.join(download_dir, filename)
                    
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    
                    logger.info(f"âœ… EOBSæ¯æ—¥æ•°æ®å·²ä¿å­˜: {filepath}")
                    return filepath
                else:
                    logger.warning(f"âš ï¸ æ— æ³•ä¸‹è½½EOBSæ•°æ®æ–‡ä»¶: HTTP {response.status_code}")
            else:
                logger.warning(f"âš ï¸ æ— æ³•è®¿é—®EOBSä¸‹è½½é¡µé¢: HTTP {response.status_code}")
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½EOBSæ¯æ—¥æ•°æ®å¤±è´¥: {e}")
            return None
    
    def download_openmeteo_data(self) -> Optional[str]:
        """ä¸‹è½½Open-Meteoæ•°æ®"""
        try:
            logger.info("ğŸ“¥ ä¸‹è½½Open-Meteoæ•°æ®...")
            
            # åˆ›å»ºä¸‹è½½ç›®å½•
            download_dir = "data/real/openmeteo"
            os.makedirs(download_dir, exist_ok=True)
            
            # Open-Meteo API (å…è´¹ï¼Œæ— éœ€APIå¯†é’¥)
            # è·å–åŠ æ‹¿å¤§å‡ ä¸ªåŸå¸‚çš„å¤©æ°”æ•°æ®
            cities = [
                {'name': 'Winnipeg', 'lat': 49.8951, 'lon': -97.1384},
                {'name': 'Toronto', 'lat': 43.6532, 'lon': -79.3832},
                {'name': 'Vancouver', 'lat': 49.2827, 'lon': -123.1207}
            ]
            
            all_data = []
            
            for city in cities:
                try:
                    # è·å–å†å²å¤©æ°”æ•°æ®
                    url = f"https://archive-api.open-meteo.com/v1/archive"
                    params = {
                        'latitude': city['lat'],
                        'longitude': city['lon'],
                        'start_date': '2024-01-01',
                        'end_date': '2024-12-31',
                        'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum,soil_moisture_0_to_7cm',
                        'timezone': 'America/Winnipeg'
                    }
                    
                    response = self.session.get(url, params=params, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if 'daily' in data:
                            # è½¬æ¢ä¸ºDataFrame
                            df = pd.DataFrame(data['daily'])
                            df['city'] = city['name']
                            df['latitude'] = city['lat']
                            df['longitude'] = city['lon']
                            all_data.append(df)
                            
                            logger.info(f"âœ… æˆåŠŸè·å– {city['name']} æ•°æ®: {len(df)} æ¡è®°å½•")
                        else:
                            logger.warning(f"âš ï¸ {city['name']} æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•è·å– {city['name']} æ•°æ®: HTTP {response.status_code}")
                    
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å– {city['name']} æ•°æ®å¤±è´¥: {e}")
            
            if all_data:
                # åˆå¹¶æ‰€æœ‰åŸå¸‚æ•°æ®
                combined_df = pd.concat(all_data, ignore_index=True)
                
                # ä¿å­˜æ•°æ®
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"openmeteo_canada_{timestamp}.csv"
                filepath = os.path.join(download_dir, filename)
                
                combined_df.to_csv(filepath, index=False)
                
                logger.info(f"âœ… Open-Meteoæ•°æ®å·²ä¿å­˜: {filepath}")
                logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)}")
                logger.info(f"ğŸ™ï¸ åŸå¸‚æ•°: {len(cities)}")
                
                return filepath
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•Open-Meteoæ•°æ®")
                return None
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½Open-Meteoæ•°æ®å¤±è´¥: {e}")
            return None
    
    def download_visualcrossing_data(self) -> Optional[str]:
        """ä¸‹è½½Visual Crossingæ•°æ®"""
        try:
            logger.info("ğŸ“¥ ä¸‹è½½Visual Crossingæ•°æ®...")
            
            # åˆ›å»ºä¸‹è½½ç›®å½•
            download_dir = "data/real/visualcrossing"
            os.makedirs(download_dir, exist_ok=True)
            
            # Visual Crossingæä¾›å…è´¹çš„å†å²å¤©æ°”æ•°æ®
            # è·å–åŠ æ‹¿å¤§å‡ ä¸ªåŸå¸‚çš„æ•°æ®
            cities = [
                {'name': 'Edmonton', 'lat': 53.5461, 'lon': -113.4938},
                {'name': 'Calgary', 'lat': 51.0447, 'lon': -114.0719},
                {'name': 'Montreal', 'lat': 45.5017, 'lon': -73.5673}
            ]
            
            all_data = []
            
            for city in cities:
                try:
                    # ä½¿ç”¨å…è´¹çš„å†å²å¤©æ°”API
                    url = f"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{city['lat']},{city['lon']}/2024-01-01/2024-12-31"
                    params = {
                        'unitGroup': 'metric',
                        'include': 'days',
                        'key': 'demo',  # ä½¿ç”¨æ¼”ç¤ºå¯†é’¥
                        'contentType': 'json'
                    }
                    
                    response = self.session.get(url, params=params, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if 'days' in data:
                            # è½¬æ¢ä¸ºDataFrame
                            df = pd.DataFrame(data['days'])
                            df['city'] = city['name']
                            df['latitude'] = city['lat']
                            df['longitude'] = city['lon']
                            all_data.append(df)
                            
                            logger.info(f"âœ… æˆåŠŸè·å– {city['name']} æ•°æ®: {len(df)} æ¡è®°å½•")
                        else:
                            logger.warning(f"âš ï¸ {city['name']} æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•è·å– {city['name']} æ•°æ®: HTTP {response.status_code}")
                    
                    time.sleep(1)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å– {city['name']} æ•°æ®å¤±è´¥: {e}")
            
            if all_data:
                # åˆå¹¶æ‰€æœ‰åŸå¸‚æ•°æ®
                combined_df = pd.concat(all_data, ignore_index=True)
                
                # ä¿å­˜æ•°æ®
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"visualcrossing_canada_{timestamp}.csv"
                filepath = os.path.join(download_dir, filename)
                
                combined_df.to_csv(filepath, index=False)
                
                logger.info(f"âœ… Visual Crossingæ•°æ®å·²ä¿å­˜: {filepath}")
                logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)}")
                logger.info(f"ğŸ™ï¸ åŸå¸‚æ•°: {len(cities)}")
                
                return filepath
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•Visual Crossingæ•°æ®")
                return None
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½Visual Crossingæ•°æ®å¤±è´¥: {e}")
            return None
    
    def generate_download_report(self, download_results: Dict) -> Dict:
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        try:
            report = {
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_sources': len(download_results),
                    'successful_downloads': 0,
                    'failed_downloads': 0,
                    'total_records': 0
                },
                'download_details': download_results,
                'recommendations': []
            }
            
            # ç»Ÿè®¡ç»“æœ
            for source, result in download_results.items():
                if result and os.path.exists(result):
                    report['summary']['successful_downloads'] += 1
                    
                    # å°è¯•ç»Ÿè®¡è®°å½•æ•°
                    try:
                        if result.endswith('.csv'):
                            df = pd.read_csv(result)
                            report['summary']['total_records'] += len(df)
                    except:
                        pass
                else:
                    report['summary']['failed_downloads'] += 1
            
            # ç”Ÿæˆå»ºè®®
            if report['summary']['successful_downloads'] > 0:
                report['recommendations'].append(f"æˆåŠŸä¸‹è½½ {report['summary']['successful_downloads']} ä¸ªæ•°æ®æº")
                report['recommendations'].append(f"æ€»è®°å½•æ•°: {report['summary']['total_records']}")
                report['recommendations'].append("å»ºè®®ä½¿ç”¨è¿™äº›çœŸå®æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹")
            else:
                report['recommendations'].append("ä¸‹è½½å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIè®¿é—®")
                report['recommendations'].append("è€ƒè™‘ä½¿ç”¨å…¶ä»–æ•°æ®æºæˆ–APIå¯†é’¥")
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä¸‹è½½æŠ¥å‘Šå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨çœŸå®æ•°æ®ä¸‹è½½...")
        
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = RealDataDownloader()
        
        # ä¸‹è½½å„ç§æ•°æ®æº
        download_results = {}
        
        # 1. NOAAæ¯æ—¥æ‘˜è¦æ•°æ®
        logger.info("ğŸ“¥ 1/5 ä¸‹è½½NOAAæ¯æ—¥æ‘˜è¦æ•°æ®...")
        noaa_daily = downloader.download_noaa_daily_summaries()
        download_results['noaa_daily'] = noaa_daily
        
        # 2. NOAAå°æ—¶æ•°æ®
        logger.info("ğŸ“¥ 2/5 ä¸‹è½½NOAAå°æ—¶æ•°æ®...")
        noaa_hourly = downloader.download_noaa_hourly_data()
        download_results['noaa_hourly'] = noaa_hourly
        
        # 3. EOBSæ¯æ—¥æ•°æ®
        logger.info("ğŸ“¥ 3/5 ä¸‹è½½EOBSæ¯æ—¥æ•°æ®...")
        eobs_daily = downloader.download_eobs_daily_data()
        download_results['eobs_daily'] = eobs_daily
        
        # 4. Open-Meteoæ•°æ®
        logger.info("ğŸ“¥ 4/5 ä¸‹è½½Open-Meteoæ•°æ®...")
        openmeteo = downloader.download_openmeteo_data()
        download_results['openmeteo'] = openmeteo
        
        # 5. Visual Crossingæ•°æ®
        logger.info("ğŸ“¥ 5/5 ä¸‹è½½Visual Crossingæ•°æ®...")
        visualcrossing = downloader.download_visualcrossing_data()
        download_results['visualcrossing'] = visualcrossing
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        logger.info("ğŸ“Š ç”Ÿæˆä¸‹è½½æŠ¥å‘Š...")
        report = downloader.generate_download_report(download_results)
        
        # ä¿å­˜æŠ¥å‘Š
        output_dir = "data/download_reports"
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = os.path.join(output_dir, f"real_data_download_report_{timestamp}.json")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"âœ… ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        logger.info("ğŸ‰ çœŸå®æ•°æ®ä¸‹è½½å®Œæˆï¼")
        logger.info(f"ğŸ“Š ä¸‹è½½æ‘˜è¦:")
        logger.info(f"  æ€»æ•°æ®æº: {report['summary']['total_sources']}")
        logger.info(f"  æˆåŠŸä¸‹è½½: {report['summary']['successful_downloads']}")
        logger.info(f"  ä¸‹è½½å¤±è´¥: {report['summary']['failed_downloads']}")
        logger.info(f"  æ€»è®°å½•æ•°: {report['summary']['total_records']}")
        
        # æ˜¾ç¤ºå»ºè®®
        for i, rec in enumerate(report['recommendations'], 1):
            logger.info(f"ğŸ’¡ å»ºè®® {i}: {rec}")
        
        return report
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()
