#!/usr/bin/env python3
"""
é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯ç³»ç»Ÿçš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import logging
from datetime import datetime

# å¯¼å…¥é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ
from src.models.anti_overfitting_core import AntiOverfittingCore
from src.data.data_quality_detector import DataQualityDetector
from src.models.training_fixer import TrainingFixer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_data(n_samples: int = 200, n_features: int = 10) -> tuple:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    try:
        logger.info(f"ğŸ”§ åˆ›å»ºæµ‹è¯•æ•°æ®: {n_samples} æ ·æœ¬, {n_features} ç‰¹å¾")
        
        # ç”Ÿæˆç‰¹å¾æ•°æ®
        np.random.seed(42)
        X = np.random.randn(n_samples, n_features)
        
        # ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆæ·»åŠ ä¸€äº›å™ªå£°ï¼‰
        y = np.sum(X[:, :3], axis=1) + np.random.normal(0, 0.1, n_samples)
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(n_samples * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        logger.info(f"âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ: è®­ç»ƒé›† {len(X_train)}, éªŒè¯é›† {len(X_val)}")
        
        return X_train, y_train, X_val, y_val
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        raise

def create_test_model(input_size: int = 10) -> nn.Module:
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    try:
        class TestLSTM(nn.Module):
            def __init__(self, input_size: int):
                super(TestLSTM, self).__init__()
                
                # æ•…æ„åˆ›å»ºè¿‡å¤æ‚çš„æ¨¡å‹æ¥æµ‹è¯•è¿‡æ‹Ÿåˆæ£€æµ‹
                self.lstm = nn.LSTM(
                    input_size=input_size,
                    hidden_size=128,  # è¿‡å¤§çš„éšè—å±‚
                    num_layers=4,     # è¿‡å¤šçš„å±‚æ•°
                    batch_first=True,
                    dropout=0.0       # æ— æ­£åˆ™åŒ–
                )
                
                self.fc = nn.Linear(128, 1)
                
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                last_output = lstm_out[:, -1, :]
                return self.fc(last_output)
        
        model = TestLSTM(input_size)
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"âœ… æµ‹è¯•æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°æ•°é‡: {total_params}")
        
        return model
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºæµ‹è¯•æ¨¡å‹å¤±è´¥: {e}")
        raise

def simulate_training(model: nn.Module, X_train: np.ndarray, y_train: np.ndarray,
                     X_val: np.ndarray, y_val: np.ndarray) -> tuple:
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹"""
    try:
        logger.info("ğŸš€ å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ® - æ·»åŠ åºåˆ—ç»´åº¦
        X_train_tensor = torch.FloatTensor(X_train).unsqueeze(1)  # (batch, seq_len=1, features)
        y_train_tensor = torch.FloatTensor(y_train)
        X_val_tensor = torch.FloatTensor(X_val).unsqueeze(1)      # (batch, seq_len=1, features)
        y_val_tensor = torch.FloatTensor(y_val)
        
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=len(X_val))
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # è®­ç»ƒå‚æ•°
        epochs = 30
        train_losses = []
        val_losses = []
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            # è®°å½•æŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # æ¨¡æ‹Ÿè¿‡æ‹Ÿåˆï¼šè®­ç»ƒæŸå¤±ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
            if epoch > 15:
                # æ•…æ„å¢åŠ éªŒè¯æŸå¤±æ¥æ¨¡æ‹Ÿè¿‡æ‹Ÿåˆ
                val_losses[-1] += 0.01 * (epoch - 15)
            
            if (epoch + 1) % 5 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")
        
        logger.info("âœ… æ¨¡æ‹Ÿè®­ç»ƒå®Œæˆ")
        return train_losses, val_losses
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡æ‹Ÿè®­ç»ƒå¤±è´¥: {e}")
        raise

def test_anti_overfitting_system():
    """æµ‹è¯•é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ"""
    try:
        logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ...")
        
        # æ­¥éª¤1: åˆ›å»ºæµ‹è¯•æ•°æ®
        X_train, y_train, X_val, y_val = create_test_data(200, 10)
        
        # æ­¥éª¤2: åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = create_test_model(10)
        
        # æ­¥éª¤3: æ¨¡æ‹Ÿè®­ç»ƒ
        train_losses, val_losses = simulate_training(model, X_train, y_train, X_val, y_val)
        
        # æ­¥éª¤4: æµ‹è¯•æ•°æ®è´¨é‡æ£€æµ‹å™¨
        logger.info("\nğŸ“Š æµ‹è¯•æ•°æ®è´¨é‡æ£€æµ‹å™¨...")
        data_quality = DataQualityDetector()
        quality_result = data_quality.detect_data_issues(X_train, y_train)
        
        if quality_result['status'] == 'success':
            logger.info(f"æ•°æ®è´¨é‡å¾—åˆ†: {quality_result['quality_score']:.3f}")
            logger.info(f"å‘ç°é—®é¢˜æ•°é‡: {quality_result['total_issues']}")
            
            for issue in quality_result['issues']:
                logger.info(f"  - {issue['severity'].upper()}: {issue['description']}")
        
        # æ­¥éª¤5: æµ‹è¯•è¿‡æ‹Ÿåˆæ£€æµ‹
        logger.info("\nğŸ” æµ‹è¯•è¿‡æ‹Ÿåˆæ£€æµ‹...")
        anti_overfitting = AntiOverfittingCore()
        overfitting_result = anti_overfitting.detect_overfitting(train_losses, val_losses)
        
        if overfitting_result['status'] == 'success':
            logger.info(f"è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ: {'æ˜¯' if overfitting_result['overfitting'] else 'å¦'}")
            if overfitting_result['overfitting']:
                logger.info(f"ä¸¥é‡ç¨‹åº¦: {overfitting_result['severity']:.3f}")
                logger.info(f"å»ºè®®: {overfitting_result['recommendation']}")
        
        # æ­¥éª¤6: æµ‹è¯•è®­ç»ƒä¿®å¤å™¨
        logger.info("\nğŸ”§ æµ‹è¯•è®­ç»ƒä¿®å¤å™¨...")
        training_fixer = TrainingFixer()
        fix_result = training_fixer.diagnose_and_fix(
            model, X_train, y_train, X_val, y_val, train_losses, val_losses
        )
        
        if fix_result.get('status') == 'success':
            logger.info(f"ä¿®å¤çŠ¶æ€: {fix_result.get('final_status', 'unknown')}")
            logger.info(f"åº”ç”¨ä¿®å¤æ•°é‡: {len(fix_result.get('fixes_applied', []))}")
        else:
            logger.warning(f"ä¿®å¤ç»“æœçŠ¶æ€å¼‚å¸¸: {fix_result}")
        
        # æ­¥éª¤7: ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        logger.info("\nğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        generate_test_report(quality_result, overfitting_result, fix_result)
        
        logger.info("âœ… é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿæµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿå¤±è´¥: {e}")
        return False

def generate_test_report(quality_result: dict, overfitting_result: dict, fix_result: dict):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"test_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("HydrAI-SWE é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ•°æ®è´¨é‡æŠ¥å‘Š
            f.write("ğŸ“Š æ•°æ®è´¨é‡æ£€æµ‹ç»“æœ\n")
            f.write("-" * 30 + "\n")
            if quality_result['status'] == 'success':
                f.write(f"è´¨é‡å¾—åˆ†: {quality_result['quality_score']:.3f}\n")
                f.write(f"é—®é¢˜æ€»æ•°: {quality_result['total_issues']}\n")
                f.write(f"ä¸¥é‡é—®é¢˜: {quality_result['critical_issues']}\n")
                f.write(f"é«˜ä¸¥é‡æ€§: {quality_result['high_issues']}\n")
                f.write(f"ä¸­ç­‰ä¸¥é‡æ€§: {quality_result['medium_issues']}\n\n")
            else:
                f.write("æ•°æ®è´¨é‡æ£€æµ‹å¤±è´¥\n\n")
            
            # è¿‡æ‹Ÿåˆæ£€æµ‹æŠ¥å‘Š
            f.write("ğŸ” è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ\n")
            f.write("-" * 30 + "\n")
            if overfitting_result['status'] == 'success':
                f.write(f"è¿‡æ‹Ÿåˆ: {'æ˜¯' if overfitting_result['overfitting'] else 'å¦'}\n")
                if overfitting_result['overfitting']:
                    f.write(f"ä¸¥é‡ç¨‹åº¦: {overfitting_result['severity']:.3f}\n")
                    f.write(f"å»ºè®®: {overfitting_result['recommendation']}\n")
                f.write("\n")
            else:
                f.write("è¿‡æ‹Ÿåˆæ£€æµ‹å¤±è´¥\n\n")
            
            # ä¿®å¤ç»“æœæŠ¥å‘Š
            f.write("ğŸ”§ ä¿®å¤ç»“æœ\n")
            f.write("-" * 30 + "\n")
            if fix_result['status'] == 'success':
                f.write(f"æœ€ç»ˆçŠ¶æ€: {fix_result['final_status']}\n")
                f.write(f"åº”ç”¨ä¿®å¤: {len(fix_result['fixes_applied'])} ä¸ª\n")
                for i, fix in enumerate(fix_result['fixes_applied']):
                    f.write(f"  ä¿®å¤ {i+1}: {fix['type']}\n")
                f.write("\n")
            else:
                f.write("ä¿®å¤å¤±è´¥\n\n")
            
            # æ€»ç»“
            f.write("ğŸ“‹ æµ‹è¯•æ€»ç»“\n")
            f.write("-" * 30 + "\n")
            if quality_result['status'] == 'success' and overfitting_result['status'] == 'success':
                f.write("âœ… é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿæµ‹è¯•æˆåŠŸ\n")
                f.write("âœ… æ•°æ®è´¨é‡æ£€æµ‹åŠŸèƒ½æ­£å¸¸\n")
                f.write("âœ… è¿‡æ‹Ÿåˆæ£€æµ‹åŠŸèƒ½æ­£å¸¸\n")
                if fix_result['status'] == 'success':
                    f.write("âœ… è®­ç»ƒä¿®å¤åŠŸèƒ½æ­£å¸¸\n")
                else:
                    f.write("âŒ è®­ç»ƒä¿®å¤åŠŸèƒ½å¼‚å¸¸\n")
            else:
                f.write("âŒ é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿæµ‹è¯•å¤±è´¥\n")
        
        logger.info(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")

if __name__ == "__main__":
    try:
        logger.info("ğŸš€ å¯åŠ¨é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿæµ‹è¯•...")
        
        success = test_anti_overfitting_system()
        
        if success:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        else:
            logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿ")
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
