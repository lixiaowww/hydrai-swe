#!/usr/bin/env python3
"""
ä½¿ç”¨NOAAçœŸå®æ•°æ®è®­ç»ƒåœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹
æ›¿ä»£åˆæˆæ•°æ®ï¼Œä½¿ç”¨çœŸå®è§‚æµ‹æ•°æ®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import logging
from datetime import datetime
import json
from typing import Dict, List, Tuple
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NOAASoilMoisturePredictor:
    """ä½¿ç”¨NOAAæ•°æ®çš„åœŸå£¤æ¹¿åº¦é¢„æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.scaler = StandardScaler()
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"âœ… NOAAåœŸå£¤æ¹¿åº¦é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def load_and_prepare_data(self) -> Dict:
        """åŠ è½½å’Œå‡†å¤‡NOAAæ•°æ®"""
        try:
            logger.info("ğŸ“¥ åŠ è½½NOAAå¤„ç†åçš„æ•°æ®...")
            
            # åŠ è½½æ¯æ—¥æ•°æ®
            daily_file = "data/processed/noaa_daily/noaa_daily_processed_20250821_192255.csv"
            if os.path.exists(daily_file):
                daily_data = pd.read_csv(daily_file)
                logger.info(f"ğŸ“Š æ¯æ—¥æ•°æ®: {daily_data.shape}")
            else:
                logger.warning("âš ï¸ æ¯æ—¥æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                daily_data = pd.DataFrame()
            
            # åŠ è½½å°æ—¶æ•°æ®
            hourly_file = "data/processed/noaa_hourly/noaa_hourly_processed_20250821_192255.csv"
            if os.path.exists(hourly_file):
                hourly_data = pd.read_csv(hourly_file)
                logger.info(f"ğŸ“Š å°æ—¶æ•°æ®: {hourly_data.shape}")
            else:
                logger.warning("âš ï¸ å°æ—¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                hourly_data = pd.DataFrame()
            
            # åˆå¹¶æ•°æ®
            if not daily_data.empty and not hourly_data.empty:
                # å°†æ¯æ—¥æ•°æ®æ‰©å±•åˆ°å°æ—¶çº§åˆ«
                daily_expanded = daily_data.copy()
                daily_expanded['hour'] = 12  # å‡è®¾æ¯æ—¥æ•°æ®ä»£è¡¨ä¸­åˆ12ç‚¹
                
                # åˆå¹¶æ•°æ®
                combined_data = pd.concat([daily_expanded, hourly_data], ignore_index=True)
                logger.info(f"ğŸ“Š åˆå¹¶åæ•°æ®: {combined_data.shape}")
            elif not daily_data.empty:
                combined_data = daily_data
                logger.info("ğŸ“Š ä½¿ç”¨æ¯æ—¥æ•°æ®")
            elif not hourly_data.empty:
                combined_data = hourly_data
                logger.info("ğŸ“Š ä½¿ç”¨å°æ—¶æ•°æ®")
            else:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„NOAAæ•°æ®")
            
            # ç‰¹å¾å·¥ç¨‹
            features, target = self._engineer_features(combined_data)
            
            # æ•°æ®åˆ†å‰²
            X_train, X_temp, y_train, y_temp = train_test_split(
                features, target, test_size=0.3, random_state=42
            )
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=0.5, random_state=42
            )
            
            # æ ‡å‡†åŒ–
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            X_test_scaled = self.scaler.transform(X_test)
            
            # è½¬æ¢ä¸ºå¼ é‡
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train_scaled), 
                torch.FloatTensor(y_train.values)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val_scaled), 
                torch.FloatTensor(y_val.values)
            )
            test_dataset = TensorDataset(
                torch.FloatTensor(X_test_scaled), 
                torch.FloatTensor(y_test.values)
            )
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))
            test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))
            
            data_loaders = {
                'train': train_loader,
                'val': val_loader,
                'test': test_loader
            }
            
            logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
            logger.info(f"  è®­ç»ƒé›†: {X_train.shape}")
            logger.info(f"  éªŒè¯é›†: {X_val.shape}")
            logger.info(f"  æµ‹è¯•é›†: {X_test.shape}")
            logger.info(f"  ç‰¹å¾æ•°: {X_train.shape[1]}")
            
            return {
                'status': 'success',
                'data_loaders': data_loaders,
                'data_shapes': {
                    'train': X_train.shape,
                    'val': X_val.shape,
                    'test': X_test.shape
                },
                'feature_names': features.columns.tolist()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _engineer_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """ç‰¹å¾å·¥ç¨‹"""
        try:
            # é€‰æ‹©æ•°å€¼ç‰¹å¾
            feature_cols = [
                'temperature', 'temp_squared', 'max_temp', 'min_temp', 'temp_range',
                'precipitation', 'precip_log', 'snow_depth', 'wind_speed', 'pressure',
                'humidity', 'wind_direction', 'wind_direction_sin', 'wind_direction_cos',
                'wind_speed_squared', 'dewpoint',
                'year', 'month', 'day', 'hour', 'day_of_year', 'day_of_week',
                'is_winter', 'is_spring', 'is_summer', 'is_fall',
                'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos'
            ]
            
            # åªä¿ç•™å­˜åœ¨çš„åˆ—
            available_cols = [col for col in feature_cols if col in data.columns]
            features = data[available_cols].copy()
            
            # å¤„ç†ç¼ºå¤±å€¼
            features = features.fillna(method='ffill').fillna(method='bfill')
            
            # ç›®æ ‡å˜é‡
            target = data['estimated_soil_moisture']
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            valid_indices = ~(features.isnull().any(axis=1) | target.isnull())
            features = features[valid_indices]
            target = target[valid_indices]
            
            logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {features.shape[1]} ä¸ªç‰¹å¾")
            return features, target
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    def build_model(self, input_size: int) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        try:
            model = nn.Sequential(
                nn.Linear(input_size, 128),
                nn.ReLU(),
                nn.BatchNorm1d(128),
                nn.Dropout(0.3),
                
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.BatchNorm1d(64),
                nn.Dropout(0.2),
                
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.BatchNorm1d(32),
                nn.Dropout(0.1),
                
                nn.Linear(32, 16),
                nn.ReLU(),
                nn.Linear(16, 1)
            )
            
            self.model = model.to(self.device)
            logger.info(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ: è¾“å…¥ç‰¹å¾æ•° {input_size}")
            return model
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            return None
    
    def train_model(self, data_loaders: Dict, epochs: int = 150) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model()")
            
            logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
            
            # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
            criterion = nn.MSELoss()
            optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15, factor=0.5)
            
            # è®­ç»ƒå†å²
            train_losses = []
            val_losses = []
            best_val_loss = float('inf')
            patience_counter = 0
            
            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                self.model.train()
                train_loss = 0
                for batch_X, batch_y in data_loaders['train']:
                    batch_X = batch_X.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = self.model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    train_loss += loss.item()
                
                train_loss /= len(data_loaders['train'])
                train_losses.append(train_loss)
                
                # éªŒè¯é˜¶æ®µ
                self.model.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch_X, batch_y in data_loaders['val']:
                        batch_X = batch_X.to(self.device)
                        batch_y = batch_y.to(self.device)
                        
                        outputs = self.model(batch_X).squeeze()
                        loss = criterion(outputs, batch_y)
                        val_loss += loss.item()
                
                val_loss /= len(data_loaders['val'])
                val_losses.append(val_loss)
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step(val_loss)
                
                # æ—©åœ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    torch.save(self.model.state_dict(), 'best_noaa_model.pth')
                else:
                    patience_counter += 1
                
                if patience_counter >= 25:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒ")
                    break
                
                if (epoch + 1) % 20 == 0:
                    logger.info(f"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            return {
                'status': 'success',
                'epochs_trained': epoch + 1,
                'final_train_loss': train_loss,
                'final_val_loss': val_loss,
                'best_val_loss': best_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def evaluate_model(self, data_loaders: Dict) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model()")
            
            logger.info("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            self.model.load_state_dict(torch.load('best_noaa_model.pth'))
            self.model.eval()
            
            # æµ‹è¯•é›†è¯„ä¼°
            test_predictions = []
            test_targets = []
            
            with torch.no_grad():
                for batch_X, batch_y in data_loaders['test']:
                    batch_X = batch_X.to(self.device)
                    outputs = self.model(batch_X).squeeze()
                    test_predictions.extend(outputs.cpu().numpy())
                    test_targets.extend(batch_y.numpy())
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            test_predictions = np.array(test_predictions)
            test_targets = np.array(test_targets)
            
            r2 = r2_score(test_targets, test_predictions)
            mae = mean_absolute_error(test_targets, test_predictions)
            rmse = np.sqrt(mean_squared_error(test_targets, test_predictions))
            
            performance = {
                'r2_score': r2,
                'mae': mae,
                'rmse': rmse,
                'status': 'overfitting' if r2 < 0 else 'normal',
                'test_samples': len(test_targets)
            }
            
            logger.info(f"ğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°å®Œæˆ:")
            logger.info(f"  RÂ²: {r2:.4f}")
            logger.info(f"  MAE: {mae:.4f}")
            logger.info(f"  RMSE: {rmse:.4f}")
            logger.info(f"  çŠ¶æ€: {'è¿‡æ‹Ÿåˆ' if r2 < 0 else 'æ­£å¸¸'}")
            
            return performance
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨ä½¿ç”¨NOAAçœŸå®æ•°æ®è®­ç»ƒåœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹...")
        
        # åˆ›å»ºé¢„æµ‹å™¨
        predictor = NOAASoilMoisturePredictor()
        
        # åŠ è½½å’Œå‡†å¤‡æ•°æ®
        data_result = predictor.load_and_prepare_data()
        
        if data_result['status'] != 'success':
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {data_result}")
            return
        
        # æ„å»ºæ¨¡å‹
        input_size = data_result['data_shapes']['train'][1]
        model = predictor.build_model(input_size)
        
        if model is None:
            logger.error("âŒ æ¨¡å‹æ„å»ºå¤±è´¥")
            return
        
        # è®­ç»ƒæ¨¡å‹
        training_result = predictor.train_model(data_result['data_loaders'])
        
        if training_result['status'] != 'success':
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {training_result}")
            return
        
        # è¯„ä¼°æ¨¡å‹
        performance = predictor.evaluate_model(data_result['data_loaders'])
        
        if 'status' not in performance:
            logger.info("ğŸ‰ ä½¿ç”¨NOAAçœŸå®æ•°æ®è®­ç»ƒæ¨¡å‹æˆåŠŸï¼")
            
            # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            report = {
                'timestamp': datetime.now().isoformat(),
                'data_info': {
                    'total_samples': sum(data_result['data_shapes'].values()),
                    'features': input_size,
                    'feature_names': data_result['feature_names']
                },
                'training_summary': training_result,
                'model_performance': performance
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"noaa_data_training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # æ˜¾ç¤ºå…³é”®ç»“æœ
            if performance['r2_score'] > 0:
                logger.info("ğŸ¯ æˆåŠŸï¼RÂ²å·²è½¬ä¸ºæ­£å€¼ï¼ŒNOAAçœŸå®æ•°æ®è§£å†³äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼")
                logger.info(f"ğŸ† æœ€ç»ˆRÂ²: {performance['r2_score']:.4f}")
                logger.info(f"ğŸ“Š ä½¿ç”¨çœŸå®æ•°æ®: {performance['test_samples']} ä¸ªæµ‹è¯•æ ·æœ¬")
            else:
                logger.info("âš ï¸ RÂ²ä»ä¸ºè´Ÿå€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
            
            return report
        else:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {performance}")
            return None
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()
