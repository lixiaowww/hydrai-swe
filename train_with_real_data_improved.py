#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®æ•°æ®æ”¹è¿›çš„é«˜çº§æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºç°æœ‰çœŸå®æ•°æ®ï¼Œæ”¹è¿›ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è®­ç»ƒ
"""

import pandas as pd
import numpy as np
import joblib
import os
import logging
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.cluster import KMeans
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_and_analyze_real_data():
    """åŠ è½½å’Œåˆ†æçœŸå®æ•°æ®"""
    try:
        logger.info("ğŸ“Š åŠ è½½çœŸå®æ´ªæ°´é¢„è­¦æ•°æ®...")
        
        # å°è¯•åŠ è½½å¤šä¸ªæ•°æ®æº
        data_sources = [
            "data/processed/flood_warning/flood_warning_optimized.csv",
            "data/real_flood_data/hydat_streamflow_realistic.csv"
        ]
        
        main_data = None
        for source in data_sources:
            if os.path.exists(source):
                logger.info(f"æ‰¾åˆ°æ•°æ®æº: {source}")
                data = pd.read_csv(source)
                logger.info(f"æ•°æ®å½¢çŠ¶: {data.shape}")
                logger.info(f"åˆ—å: {list(data.columns)}")
                
                if main_data is None:
                    main_data = data
                else:
                    # åˆå¹¶æ•°æ®
                    if 'Date/Time' in main_data.columns and 'Date' in data.columns:
                        main_data['Date/Time'] = pd.to_datetime(main_data['Date/Time'])
                        data['Date'] = pd.to_datetime(data['Date'])
                        main_data = pd.merge(main_data, data, 
                                           left_on='Date/Time', right_on='Date', 
                                           how='left')
                        logger.info(f"åˆå¹¶åæ•°æ®å½¢çŠ¶: {main_data.shape}")
        
        if main_data is None:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æº")
        
        return main_data
        
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        raise

def create_realistic_flood_target(data: pd.DataFrame) -> pd.DataFrame:
    """åˆ›å»ºçœŸå®çš„æ´ªæ°´ç›®æ ‡å˜é‡"""
    try:
        logger.info("ğŸ¯ åˆ›å»ºçœŸå®çš„æ´ªæ°´ç›®æ ‡å˜é‡...")
        
        # æ£€æŸ¥å¾„æµæ•°æ®åˆ—
        flow_columns = [col for col in data.columns if col.startswith('05OC')]
        logger.info(f"æ‰¾åˆ°å¾„æµåˆ—: {flow_columns}")
        
        if not flow_columns:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¾„æµæ•°æ®åˆ—ï¼Œä½¿ç”¨åˆæˆç›®æ ‡å˜é‡")
            np.random.seed(42)
            data['flood_risk'] = np.random.choice([0, 1], size=len(data), p=[0.85, 0.15])
            return data
        
        # ä½¿ç”¨ä¸»è¦å¾„æµåˆ—
        main_flow_col = flow_columns[0]
        flow_data = data[main_flow_col].fillna(0)
        
        # è®¡ç®—çœŸå®çš„æ´ªæ°´é£é™©é˜ˆå€¼
        # ä½¿ç”¨å¤šç§æ–¹æ³•ç¡®å®šé˜ˆå€¼
        
        # æ–¹æ³•1: ç»Ÿè®¡é˜ˆå€¼ï¼ˆ90%åˆ†ä½æ•°ï¼‰
        quantile_threshold = flow_data.quantile(0.9)
        
        # æ–¹æ³•2: å‡å€¼ + 2å€æ ‡å‡†å·®
        mean_threshold = flow_data.mean() + 2 * flow_data.std()
        
        # æ–¹æ³•3: åŸºäºå†å²æå€¼çš„é˜ˆå€¼
        historical_max = flow_data.max()
        historical_threshold = historical_max * 0.7
        
        # é€‰æ‹©æœ€åˆé€‚çš„é˜ˆå€¼
        thresholds = [quantile_threshold, mean_threshold, historical_threshold]
        valid_thresholds = [t for t in thresholds if t > 0 and not np.isnan(t)]
        
        if valid_thresholds:
            # é€‰æ‹©ä¸­ç­‰é˜ˆå€¼ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„é«˜é£é™©æ ·æœ¬
            selected_threshold = np.median(valid_thresholds)
            logger.info(f"é€‰æ‹©çš„æ´ªæ°´é˜ˆå€¼: {selected_threshold:.2f}")
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé˜ˆå€¼ï¼Œä½¿ç”¨åˆ†ä½æ•°
            selected_threshold = flow_data.quantile(0.85)
            logger.info(f"ä½¿ç”¨åˆ†ä½æ•°é˜ˆå€¼: {selected_threshold:.2f}")
        
        # åˆ›å»ºç›®æ ‡å˜é‡
        data['flood_risk'] = (flow_data > selected_threshold).astype(int)
        
        # æ£€æŸ¥ç›®æ ‡å˜é‡åˆ†å¸ƒ
        risk_distribution = data['flood_risk'].value_counts()
        logger.info(f"æ´ªæ°´é£é™©åˆ†å¸ƒ: {risk_distribution.to_dict()}")
        
        # å¦‚æœé«˜é£é™©æ ·æœ¬å¤ªå°‘ï¼Œè°ƒæ•´é˜ˆå€¼
        high_risk_count = risk_distribution.get(1, 0)
        if high_risk_count < 50:
            logger.warning(f"é«˜é£é™©æ ·æœ¬å¤ªå°‘ ({high_risk_count})ï¼Œè°ƒæ•´é˜ˆå€¼...")
            # é™ä½é˜ˆå€¼åˆ°75%åˆ†ä½æ•°
            adjusted_threshold = flow_data.quantile(0.75)
            data['flood_risk'] = (flow_data > adjusted_threshold).astype(int)
            logger.info(f"è°ƒæ•´åé˜ˆå€¼: {adjusted_threshold:.2f}")
            logger.info(f"è°ƒæ•´åé£é™©åˆ†å¸ƒ: {data['flood_risk'].value_counts().to_dict()}")
        
        return data
        
    except Exception as e:
        logger.error(f"åˆ›å»ºæ´ªæ°´ç›®æ ‡å˜é‡å¤±è´¥: {e}")
        raise

def engineer_advanced_features(data: pd.DataFrame) -> pd.DataFrame:
    """å·¥ç¨‹åŒ–é«˜çº§ç‰¹å¾"""
    try:
        logger.info("ğŸ”§ å·¥ç¨‹åŒ–é«˜çº§ç‰¹å¾...")
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        if 'Date/Time' in data.columns:
            data['Date/Time'] = pd.to_datetime(data['Date/Time'])
        elif 'Date' in data.columns:
            data['Date/Time'] = pd.to_datetime(data['Date'])
        
        # æ—¶é—´ç‰¹å¾
        data['Year'] = data['Date/Time'].dt.year
        data['Month'] = data['Date/Time'].dt.month
        data['Day'] = data['Date/Time'].dt.day
        data['DayOfYear'] = data['Date/Time'].dt.dayofyear
        data['WeekOfYear'] = data['Date/Time'].dt.isocalendar().week
        
        # å­£èŠ‚æ€§ç‰¹å¾
        data['Season'] = data['Month'].map({
            12: 'winter', 1: 'winter', 2: 'winter',
            3: 'spring', 4: 'spring', 5: 'spring',
            6: 'summer', 7: 'summer', 8: 'summer',
            9: 'fall', 10: 'fall', 11: 'fall'
        })
        
        # å­£èŠ‚æ€§ç¼–ç 
        season_dummies = pd.get_dummies(data['Season'], prefix='season')
        data = pd.concat([data, season_dummies], axis=1)
        
        # æ—¶é—´å‘¨æœŸæ€§ç‰¹å¾
        data['day_of_year_sin'] = np.sin(2 * np.pi * data['DayOfYear'] / 365)
        data['day_of_year_cos'] = np.cos(2 * np.pi * data['DayOfYear'] / 365)
        data['month_sin'] = np.sin(2 * np.pi * data['Month'] / 12)
        data['month_cos'] = np.cos(2 * np.pi * data['Month'] / 12)
        
        # æ°”è±¡ç‰¹å¾å·¥ç¨‹
        if 'Max Temp (Â°C)' in data.columns:
            data['temp_range'] = data['Max Temp (Â°C)'] - data['Min Temp (Â°C)']
            data['temp_anomaly'] = data['Mean Temp (Â°C)'] - data['Mean Temp (Â°C)'].rolling(30).mean()
            data['temp_trend'] = data['Mean Temp (Â°C)'].rolling(7).mean()
            
            # æ¸©åº¦å˜åŒ–ç‡
            data['temp_change'] = data['Mean Temp (Â°C)'].diff()
            data['temp_acceleration'] = data['temp_change'].diff()
        
        if 'Total Rain (mm)' in data.columns:
            # é™æ°´ç‰¹å¾
            data['rain_cumulative_3d'] = data['Total Rain (mm)'].rolling(3).sum()
            data['rain_cumulative_7d'] = data['Total Rain (mm)'].rolling(7).sum()
            data['rain_intensity'] = data['Total Rain (mm)'].rolling(3).max()
            data['rain_frequency'] = (data['Total Rain (mm)'] > 0).rolling(7).sum()
            
            # é™æ°´å˜åŒ–
            data['rain_change'] = data['Total Rain (mm)'].diff()
            data['rain_trend'] = data['Total Rain (mm)'].rolling(7).mean()
        
        if 'Snow on Grnd (cm)' in data.columns:
            # ç§¯é›ªç‰¹å¾
            data['snow_change'] = data['Snow on Grnd (cm)'].diff()
            data['snow_trend'] = data['Snow on Grnd (cm)'].rolling(7).mean()
            data['snow_accumulation'] = data['Snow on Grnd (cm)'].rolling(30).sum()
            
            # ç§¯é›ªèåŒ–ç‡
            data['snow_melt_rate'] = -data['snow_change']  # è´Ÿå€¼è¡¨ç¤ºèåŒ–
            data['snow_melt_rate'] = data['snow_melt_rate'].clip(lower=0)  # åªä¿ç•™èåŒ–
        
        # å¾„æµç‰¹å¾å·¥ç¨‹
        flow_columns = [col for col in data.columns if col.startswith('05OC')]
        if flow_columns:
            main_flow_col = flow_columns[0]
            
            # å¾„æµå˜åŒ–ç‰¹å¾
            data['flow_change'] = data[main_flow_col].pct_change()
            data['flow_acceleration'] = data['flow_change'].diff()
            data['flow_trend'] = data[main_flow_col].rolling(7).mean()
            data['flow_volatility'] = data[main_flow_col].rolling(7).std()
            
            # å¾„æµå¼‚å¸¸
            data['flow_anomaly'] = data[main_flow_col] / data[main_flow_col].rolling(30).mean()
            data['flow_anomaly'] = data['flow_anomaly'].fillna(1.0)
            
            # å¾„æµå³°å€¼
            data['flow_peak'] = data[main_flow_col].rolling(7).max()
            data['flow_peak_ratio'] = data[main_flow_col] / data['flow_peak']
            
            # å¤šç«™ç‚¹å¾„æµç›¸å…³æ€§
            if len(flow_columns) > 1:
                for i, col1 in enumerate(flow_columns):
                    for j, col2 in enumerate(flow_columns[i+1:], i+1):
                        col_name = f'flow_corr_{i}_{j}'
                        data[col_name] = data[col1].rolling(30).corr(data[col2])
        
        # äº¤äº’ç‰¹å¾
        if 'Mean Temp (Â°C)' in data.columns and 'Total Rain (mm)' in data.columns:
            data['temp_rain_interaction'] = data['Mean Temp (Â°C)'] * data['Total Rain (mm)']
        
        if 'Snow on Grnd (cm)' in data.columns and 'Mean Temp (Â°C)' in data.columns:
            data['snow_temp_interaction'] = data['Snow on Grnd (cm)'] * data['Mean Temp (Â°C)']
        
        # æ»åç‰¹å¾
        if '05OC001' in data.columns:
            for lag in [1, 3, 7]:
                data[f'flow_lag_{lag}'] = data['05OC001'].shift(lag)
        
        if 'Total Rain (mm)' in data.columns:
            for lag in [1, 3, 7]:
                data[f'rain_lag_{lag}'] = data['Total Rain (mm)'].shift(lag)
        
        # æ£€æŸ¥NaNå€¼æƒ…å†µ
        initial_rows = len(data)
        nan_counts = data.isnull().sum()
        logger.info(f"NaNå€¼ç»Ÿè®¡:")
        for col, count in nan_counts[nan_counts > 0].items():
            logger.info(f"  {col}: {count} NaNå€¼")
        
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„NaNå¤„ç†ç­–ç•¥
        # 1. å¯¹äºæ•°å€¼åˆ—ï¼Œç”¨0å¡«å……
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        data[numeric_cols] = data[numeric_cols].fillna(0)
        
        # 2. å¯¹äºåˆ†ç±»åˆ—ï¼Œç”¨å‰å‘å¡«å……
        categorical_cols = data.select_dtypes(include=['object']).columns
        data[categorical_cols] = data[categorical_cols].fillna(method='ffill')
        
        # 3. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNå€¼
        remaining_nans = data.isnull().sum().sum()
        logger.info(f"å¤„ç†åå‰©ä½™NaNå€¼: {remaining_nans}")
        
        if remaining_nans > 0:
            # å¦‚æœè¿˜æœ‰NaNå€¼ï¼Œç”¨0å¡«å……
            data = data.fillna(0)
            logger.info("ä½¿ç”¨0å¡«å……å‰©ä½™NaNå€¼")
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®: {len(data)} è¡Œ")
        
        return data
        
    except Exception as e:
        logger.error(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
        raise

def select_best_features(data: pd.DataFrame, target_col: str = 'flood_risk', k: int = 20):
    """é€‰æ‹©æœ€ä½³ç‰¹å¾"""
    try:
        logger.info(f"ğŸ” é€‰æ‹©æœ€ä½³ {k} ä¸ªç‰¹å¾...")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        feature_cols = [col for col in data.columns if col != target_col and col not in ['Date/Time', 'Date']]
        X = data[feature_cols]
        y = data[target_col]
        
        # åªé€‰æ‹©æ•°å€¼åˆ—
        numeric_feature_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        logger.info(f"æ•°å€¼ç‰¹å¾åˆ—æ•°é‡: {len(numeric_feature_cols)}")
        
        if len(numeric_feature_cols) < k:
            logger.warning(f"æ•°å€¼ç‰¹å¾æ•°é‡ ({len(numeric_feature_cols)}) å°‘äºè¯·æ±‚çš„ {k} ä¸ª")
            k = len(numeric_feature_cols)
        
        X_numeric = X[numeric_feature_cols]
        
        # å¤„ç†æ— ç©·å€¼
        X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan)
        X_numeric = X_numeric.fillna(0)
        
        # ä½¿ç”¨Fæ£€éªŒé€‰æ‹©ç‰¹å¾
        selector = SelectKBest(score_func=f_classif, k=k)
        X_selected = selector.fit_transform(X_numeric, y)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾
        selected_features = X_numeric.columns[selector.get_support()].tolist()
        feature_scores = selector.scores_[selector.get_support()]
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': selected_features,
            'score': feature_scores
        }).sort_values('score', ascending=False)
        
        logger.info("å‰10ä¸ªæœ€ä½³ç‰¹å¾:")
        for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows()):
            logger.info(f"  {i+1:2d}. {row['feature']}: {row['score']:.4f}")
        
        # è¿”å›é€‰ä¸­çš„ç‰¹å¾æ•°æ®
        selected_data = data[selected_features + [target_col, 'Date/Time']]
        
        return selected_data, selected_features
        
    except Exception as e:
        logger.error(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
        raise

def train_improved_model(data: pd.DataFrame, selected_features: list):
    """è®­ç»ƒæ”¹è¿›çš„æ¨¡å‹"""
    try:
        logger.info("ğŸ¯ è®­ç»ƒæ”¹è¿›çš„æ´ªæ°´é¢„è­¦æ¨¡å‹...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = data[selected_features]
        y = data['flood_risk']
        
        # å¤„ç†æ— ç©·å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        logger.info(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
        logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        logger.info(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
        logger.info(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = RobustScaler()  # ä½¿ç”¨RobustScalerå¤„ç†å¼‚å¸¸å€¼
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è®­ç»ƒèšç±»æ¨¡å‹
        logger.info("ğŸ¯ è®­ç»ƒèšç±»æ¨¡å‹...")
        n_clusters = min(5, len(X_train) // 10)  # åŠ¨æ€ç¡®å®šèšç±»æ•°
        cluster_model = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = cluster_model.fit_predict(X_train_scaled)
        
        # è®­ç»ƒä¸»æ¨¡å‹
        logger.info("ğŸ¯ è®­ç»ƒä¸»åˆ†ç±»æ¨¡å‹...")
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)
        
        # æ¨¡å‹è¯„ä¼°
        logger.info("ğŸ“Š æ¨¡å‹è¯„ä¼°...")
        
        # è®­ç»ƒé›†æ€§èƒ½
        y_train_pred = model.predict(X_train_scaled)
        y_train_proba = model.predict_proba(X_train_scaled)[:, 1]
        train_accuracy = (y_train_pred == y_train).mean()
        
        # æµ‹è¯•é›†æ€§èƒ½
        y_test_pred = model.predict(X_test_scaled)
        y_test_proba = model.predict_proba(X_test_scaled)[:, 1]
        test_accuracy = (y_test_pred == y_test).mean()
        
        # äº¤å‰éªŒè¯
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')
        
        # ROC AUC
        train_roc_auc = roc_auc_score(y_train, y_train_proba)
        test_roc_auc = roc_auc_score(y_test, y_test_proba)
        
        logger.info(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}")
        logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
        logger.info(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        logger.info(f"è®­ç»ƒé›†ROC AUC: {train_roc_auc:.4f}")
        logger.info(f"æµ‹è¯•é›†ROC AUC: {test_roc_auc:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        logger.info("åˆ†ç±»æŠ¥å‘Š:")
        logger.info(classification_report(y_test, y_test_pred))
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = dict(zip(selected_features, model.feature_importances_))
        top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("å‰15ä¸ªé‡è¦ç‰¹å¾:")
        for i, (feature, importance) in enumerate(top_features[:15]):
            logger.info(f"  {i+1:2d}. {feature}: {importance:.4f}")
        
        return model, scaler, cluster_model, {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'cv_accuracy_mean': cv_scores.mean(),
            'cv_accuracy_std': cv_scores.std(),
            'train_roc_auc': train_roc_auc,
            'test_roc_auc': test_roc_auc,
            'feature_importance': feature_importance
        }
        
    except Exception as e:
        logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        raise

def save_improved_model(model, scaler, cluster_model, performance_metrics, selected_features):
    """ä¿å­˜æ”¹è¿›çš„æ¨¡å‹"""
    try:
        logger.info("ğŸ’¾ ä¿å­˜æ”¹è¿›çš„æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        os.makedirs("models", exist_ok=True)
        
        # ä¿å­˜ä¸»æ¨¡å‹
        model_path = "models/advanced_flood_warning_model_improved.pkl"
        joblib.dump(model, model_path)
        logger.info(f"ä¸»æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = "models/advanced_flood_warning_scaler_improved.pkl"
        joblib.dump(scaler, scaler_path)
        logger.info(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
        
        # ä¿å­˜èšç±»æ¨¡å‹
        cluster_path = "models/advanced_flood_cluster_model_improved.pkl"
        joblib.dump(cluster_model, cluster_path)
        logger.info(f"èšç±»æ¨¡å‹å·²ä¿å­˜: {cluster_path}")
        
        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        features_path = "models/advanced_flood_features_improved.json"
        import json
        with open(features_path, 'w') as f:
            json.dump(selected_features, f, indent=2)
        logger.info(f"ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜: {features_path}")
        
        # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
        metrics_path = "models/advanced_flood_performance_improved.json"
        with open(metrics_path, 'w') as f:
            json.dump(performance_metrics, f, indent=2)
        logger.info(f"æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")
        
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒæ”¹è¿›çš„æ´ªæ°´é¢„è­¦æ¨¡å‹...")
        
        # 1. åŠ è½½çœŸå®æ•°æ®
        data = load_and_analyze_real_data()
        
        # 2. åˆ›å»ºçœŸå®çš„æ´ªæ°´ç›®æ ‡å˜é‡
        data = create_realistic_flood_target(data)
        
        # 3. å·¥ç¨‹åŒ–é«˜çº§ç‰¹å¾
        data = engineer_advanced_features(data)
        
        # 4. é€‰æ‹©æœ€ä½³ç‰¹å¾
        selected_data, selected_features = select_best_features(data)
        
        # 5. è®­ç»ƒæ”¹è¿›çš„æ¨¡å‹
        model, scaler, cluster_model, performance_metrics = train_improved_model(
            selected_data, selected_features
        )
        
        # 6. ä¿å­˜æ¨¡å‹
        success = save_improved_model(model, scaler, cluster_model, performance_metrics, selected_features)
        
        if success:
            logger.info("âœ… æ”¹è¿›çš„æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {performance_metrics['test_accuracy']:.4f}")
            logger.info(f"æµ‹è¯•é›†ROC AUC: {performance_metrics['test_roc_auc']:.4f}")
            logger.info(f"ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(selected_features)}")
            
            return True
        else:
            logger.error("âŒ æ¨¡å‹ä¿å­˜å¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    try:
        success = main()
        
        if success:
            print("\nğŸ‰ æ”¹è¿›çš„æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
            print("æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜åˆ° models/ ç›®å½•")
        else:
            print("\nâŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
            exit(1)
            
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        exit(1)
