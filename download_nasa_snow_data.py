#!/usr/bin/env python3
"""
NASAé›ªæ•°æ®ä¸‹è½½è„šæœ¬
è·å–å…¨çƒé›ªæ°´å½“é‡æ•°æ®ï¼Œæ‰©å±•æ•°æ®æ¥æº
"""

import os
import sys
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import time
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

class NASASnowDataDownloader:
    """NASAé›ªæ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.base_url = "https://cmr.earthdata.nasa.gov/search"
        self.data_dir = "data/raw/nasa_snow"
        self.processed_dir = "data/processed/nasa_snow"
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
        # NASAæ•°æ®é›†ä¿¡æ¯
        self.datasets = {
            'smap_swe': {
                'name': 'SMAP L4 Global 3-hourly 9 km EASE-Grid Surface and Root Zone Soil Moisture Geophysical Data',
                'short_name': 'SPL4SMGP',
                'version': '7',
                'description': 'SMAPé›ªæ°´å½“é‡æ•°æ®ï¼Œ9kmåˆ†è¾¨ç‡ï¼Œ3å°æ—¶é¢‘ç‡'
            },
            'amsr2_swe': {
                'name': 'AMSR2 Daily L3 Global Snow Water Equivalent EASE-Grids',
                'short_name': 'AE_DySno',
                'version': '1',
                'description': 'AMSR2é›ªæ°´å½“é‡æ•°æ®ï¼Œ25kmåˆ†è¾¨ç‡ï¼Œæ—¥é¢‘ç‡'
            },
            'globsnow_swe': {
                'name': 'GlobSnow v3.0 Northern Hemisphere Snow Water Equivalent',
                'short_name': 'GlobSnow_SWE',
                'version': '3.0',
                'description': 'GlobSnowé›ªæ°´å½“é‡æ•°æ®ï¼Œ25kmåˆ†è¾¨ç‡ï¼Œæ—¥é¢‘ç‡'
            }
        }
        
        # ç›®æ ‡åŒºåŸŸï¼ˆManitobaé™„è¿‘ï¼‰
        self.target_bbox = {
            'west': -102.0,  # è¥¿ç»
            'east': -88.0,   # ä¸œç»
            'north': 60.0,   # åŒ—çº¬
            'south': 49.0    # å—çº¬
        }
    
    def search_datasets(self, dataset_key: str) -> List[Dict[str, Any]]:
        """æœç´¢æ•°æ®é›†"""
        dataset = self.datasets[dataset_key]
        
        print(f"ğŸ” æœç´¢æ•°æ®é›†: {dataset['name']}")
        
        # æ„å»ºæœç´¢å‚æ•°
        params = {
            'collection': f"{dataset['short_name']}.{dataset['version']}",
            'bbox': f"{self.target_bbox['west']},{self.target_bbox['south']},{self.target_bbox['east']},{self.target_bbox['north']}",
            'temporal': '2000-01-01T00:00:00Z/2024-12-31T23:59:59Z',
            'format': 'json'
        }
        
        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            
            results = response.json()
            
            if 'feed' in results and 'entry' in results['feed']:
                granules = results['feed']['entry']
                print(f"âœ… æ‰¾åˆ° {len(granules)} ä¸ªæ•°æ®æ–‡ä»¶")
                return granules
            else:
                print("âš ï¸ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
                return []
                
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def download_granule(self, granule: Dict[str, Any], dataset_key: str) -> Optional[str]:
        """ä¸‹è½½å•ä¸ªæ•°æ®æ–‡ä»¶"""
        try:
            # è·å–ä¸‹è½½é“¾æ¥
            links = granule.get('links', [])
            download_link = None
            
            for link in links:
                if link.get('type') == 'GET DATA':
                    download_link = link.get('href')
                    break
            
            if not download_link:
                print(f"âš ï¸ æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥: {granule.get('id', 'unknown')}")
                return None
            
            # ç”Ÿæˆæ–‡ä»¶å
            granule_id = granule.get('id', 'unknown')
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{dataset_key}_{granule_id}_{timestamp}.nc"
            filepath = os.path.join(self.data_dir, filename)
            
            print(f"ğŸ“¥ ä¸‹è½½: {granule_id}")
            print(f"   é“¾æ¥: {download_link}")
            print(f"   ä¿å­˜åˆ°: {filepath}")
            
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(download_link, stream=True, timeout=60)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
            return filepath
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def download_dataset(self, dataset_key: str, max_files: int = 100) -> List[str]:
        """ä¸‹è½½æ•´ä¸ªæ•°æ®é›†"""
        print(f"ğŸš€ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_key}")
        print("=" * 50)
        
        # æœç´¢æ•°æ®æ–‡ä»¶
        granules = self.search_datasets(dataset_key)
        
        if not granules:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½")
            return []
        
        # é™åˆ¶ä¸‹è½½æ–‡ä»¶æ•°é‡
        granules = granules[:max_files]
        
        downloaded_files = []
        
        for i, granule in enumerate(granules, 1):
            print(f"\nğŸ“Š è¿›åº¦: {i}/{len(granules)}")
            
            filepath = self.download_granule(granule, dataset_key)
            if filepath:
                downloaded_files.append(filepath)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ: {len(downloaded_files)}/{len(granules)} ä¸ªæ–‡ä»¶")
        return downloaded_files
    
    def process_smap_data(self, filepath: str) -> Optional[pd.DataFrame]:
        """å¤„ç†SMAPæ•°æ®"""
        try:
            import netCDF4 as nc
            
            print(f"ğŸ”„ å¤„ç†SMAPæ•°æ®: {filepath}")
            
            # è¯»å–NetCDFæ–‡ä»¶
            with nc.Dataset(filepath, 'r') as ds:
                # è·å–å˜é‡ä¿¡æ¯
                print(f"   å˜é‡: {list(ds.variables.keys())}")
                
                # è¯»å–æ—¶é—´ä¿¡æ¯
                time_var = ds.variables.get('time')
                if time_var:
                    print(f"   æ—¶é—´èŒƒå›´: {time_var[0]} - {time_var[-1]}")
                
                # è¯»å–é›ªæ°´å½“é‡æ•°æ®
                swe_var = ds.variables.get('snow_water_equivalent')
                if swe_var:
                    print(f"   é›ªæ°´å½“é‡å½¢çŠ¶: {swe_var.shape}")
                    
                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è¿›è¡Œå¤„ç†
                    # æš‚æ—¶è¿”å›ç¤ºä¾‹æ•°æ®
                    return self._create_sample_smap_data()
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°é›ªæ°´å½“é‡å˜é‡")
                    return None
                    
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£…netCDF4: pip install netCDF4")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _create_sample_smap_data(self) -> pd.DataFrame:
        """åˆ›å»ºç¤ºä¾‹SMAPæ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥è¯»å–çœŸå®æ•°æ®ï¼‰"""
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        dates = pd.date_range('2000-01-01', '2024-12-31', freq='D')
        
        data = []
        for date in dates:
            # æ¨¡æ‹Ÿå­£èŠ‚æ€§é›ªæ°´å½“é‡å˜åŒ–
            day_of_year = date.timetuple().tm_yday
            seasonal_factor = 50 + 30 * np.sin(2 * np.pi * (day_of_year - 80) / 365)
            random_variation = np.random.normal(0, 10)
            
            swe = max(0, seasonal_factor + random_variation)
            
            data.append({
                'date': date,
                'snow_water_equivalent_mm': swe,
                'data_source': 'SMAP',
                'latitude': 55.0,  # Manitobaä¸­å¿ƒçº¬åº¦
                'longitude': -95.0  # Manitobaä¸­å¿ƒç»åº¦
            })
        
        return pd.DataFrame(data)
    
    def process_amsr2_data(self, filepath: str) -> Optional[pd.DataFrame]:
        """å¤„ç†AMSR2æ•°æ®"""
        try:
            import netCDF4 as nc
            
            print(f"ğŸ”„ å¤„ç†AMSR2æ•°æ®: {filepath}")
            
            # è¯»å–NetCDFæ–‡ä»¶
            with nc.Dataset(filepath, 'r') as ds:
                # è·å–å˜é‡ä¿¡æ¯
                print(f"   å˜é‡: {list(ds.variables.keys())}")
                
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è¿›è¡Œå¤„ç†
                # æš‚æ—¶è¿”å›ç¤ºä¾‹æ•°æ®
                return self._create_sample_amsr2_data()
                
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£…netCDF4: pip install netCDF4")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _create_sample_amsr2_data(self) -> pd.DataFrame:
        """åˆ›å»ºç¤ºä¾‹AMSR2æ•°æ®"""
        dates = pd.date_range('2000-01-01', '2024-12-31', freq='D')
        
        data = []
        for date in dates:
            day_of_year = date.timetuple().tm_yday
            seasonal_factor = 45 + 25 * np.sin(2 * np.pi * (day_of_year - 80) / 365)
            random_variation = np.random.normal(0, 8)
            
            swe = max(0, seasonal_factor + random_variation)
            
            data.append({
                'date': date,
                'snow_water_equivalent_mm': swe,
                'data_source': 'AMSR2',
                'latitude': 55.0,
                'longitude': -95.0
            })
        
        return pd.DataFrame(data)
    
    def merge_all_datasets(self) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ”„ åˆå¹¶æ‰€æœ‰æ•°æ®é›†")
        
        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        
        # è¯»å–ECCCæ•°æ®
        eccc_path = "data/processed/eccc_manitoba_snow_processed.csv"
        if os.path.exists(eccc_path):
            try:
                eccc_data = pd.read_csv(eccc_path, index_col=0, parse_dates=True)
                eccc_data['data_source'] = 'ECCC'
                existing_data.append(eccc_data)
                print(f"âœ… åŠ è½½ECCCæ•°æ®: {len(eccc_data)} æ¡è®°å½•")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ECCCæ•°æ®å¤±è´¥: {e}")
                # å°è¯•è¯»å–ä¿®å¤åçš„æ•°æ®
                eccc_fixed_path = "data/processed/eccc_manitoba_snow_fixed.csv"
                if os.path.exists(eccc_fixed_path):
                    eccc_data = pd.read_csv(eccc_fixed_path, parse_dates=['date'])
                    eccc_data['data_source'] = 'ECCC'
                    existing_data.append(eccc_data)
                    print(f"âœ… åŠ è½½ä¿®å¤åçš„ECCCæ•°æ®: {len(eccc_data)} æ¡è®°å½•")
        
        # è¯»å–HYDATæ•°æ®
        hydat_path = "data/processed/hydat_streamflow_processed.csv"
        if os.path.exists(hydat_path):
            try:
                hydat_data = pd.read_csv(hydat_path, index_col=0, parse_dates=True)
                hydat_data['data_source'] = 'HYDAT'
                existing_data.append(hydat_data)
                print(f"âœ… åŠ è½½HYDATæ•°æ®: {len(hydat_data)} æ¡è®°å½•")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½HYDATæ•°æ®å¤±è´¥: {e}")
                # å°è¯•è¯»å–ä¿®å¤åçš„æ•°æ®
                hydat_fixed_path = "data/processed/hydat_streamflow_fixed.csv"
                if hydat_fixed_path and os.path.exists(hydat_fixed_path):
                    hydat_data = pd.read_csv(hydat_fixed_path, parse_dates=['date'])
                    hydat_data['data_source'] = 'HYDAT'
                    existing_data.append(hydat_data)
                    print(f"âœ… åŠ è½½ä¿®å¤åçš„HYDATæ•°æ®: {len(hydat_data)} æ¡è®°å½•")
        
        # è¯»å–NASAæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        nasa_files = [f for f in os.listdir(self.processed_dir) if f.endswith('.csv')]
        for nasa_file in nasa_files:
            nasa_path = os.path.join(self.processed_dir, nasa_file)
            try:
                nasa_data = pd.read_csv(nasa_path, parse_dates=['date'])
                nasa_data['data_source'] = 'NASA'
                existing_data.append(nasa_data)
                print(f"âœ… åŠ è½½NASAæ•°æ®: {len(nasa_data)} æ¡è®°å½•")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½NASAæ•°æ®å¤±è´¥: {e}")
        
        if not existing_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ•°æ®
        print(f"\nğŸ”„ å¼€å§‹åˆå¹¶ {len(existing_data)} ä¸ªæ•°æ®é›†...")
        
        # æ ‡å‡†åŒ–åˆ—åå’Œæ•°æ®ç»“æ„
        standardized_data = []
        for df in existing_data:
            # ç¡®ä¿æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰å¿…è¦çš„åˆ—
            required_columns = ['date', 'snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm']
            
            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—
            for col in required_columns:
                if col not in df.columns:
                    if col == 'date':
                        # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œå°è¯•ä»ç´¢å¼•åˆ›å»º
                        if df.index.name == 'date' or isinstance(df.index, pd.DatetimeIndex):
                            df = df.reset_index()
                            df['date'] = df.index
                        else:
                            # åˆ›å»ºé»˜è®¤æ—¥æœŸ
                            df['date'] = pd.date_range('2000-01-01', periods=len(df), freq='D')
                    else:
                        # å…¶ä»–åˆ—è®¾ä¸º0
                        df[col] = 0
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            df['date'] = pd.to_datetime(df['date'])
            df['snow_depth_mm'] = pd.to_numeric(df['snow_depth_mm'], errors='coerce').fillna(0)
            df['snow_fall_mm'] = pd.to_numeric(df['snow_fall_mm'], errors='coerce').fillna(0)
            df['snow_water_equivalent_mm'] = pd.to_numeric(df['snow_water_equivalent_mm'], errors='coerce').fillna(0)
            
            # æ·»åŠ æ—¶é—´ç‰¹å¾
            df['day_of_year'] = df['date'].dt.dayofyear
            df['month'] = df['date'].dt.month
            df['year'] = df['date'].dt.year
            
            # é€‰æ‹©æ ‡å‡†åˆ—
            standard_columns = ['date', 'snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                               'day_of_year', 'month', 'year', 'data_source']
            
            df_standardized = df[standard_columns].copy()
            standardized_data.append(df_standardized)
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_data = pd.concat(standardized_data, ignore_index=True)
        
        # å»é‡å’Œæ’åº
        if 'date' in merged_data.columns:
            merged_data = merged_data.drop_duplicates(subset=['date']).sort_values('date')
        
        print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_data)} æ¡è®°å½•")
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"nasa_extended_dataset_{timestamp}.csv"
        output_path = os.path.join(self.processed_dir, output_file)
        
        merged_data.to_csv(output_path, index=False)
        print(f"âœ… æ‰©å±•æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
        
        return merged_data
    
    def save_merged_dataset(self, data: pd.DataFrame, filename: str = None):
        """ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"extended_training_dataset_{timestamp}.csv"
        
        filepath = os.path.join(self.processed_dir, filename)
        
        try:
            data.to_csv(filepath)
            print(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {filepath}")
            print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(filepath) / 1024 / 1024:.2f} MB")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def generate_download_report(self, downloaded_files: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(downloaded_files),
            'successful_downloads': len([f for f in downloaded_files if f]),
            'failed_downloads': len([f for f in downloaded_files if not f]),
            'file_details': [],
            'data_sources': list(self.datasets.keys()),
            'target_region': self.target_bbox
        }
        
        for filepath in downloaded_files:
            if filepath and os.path.exists(filepath):
                file_info = {
                    'filename': os.path.basename(filepath),
                    'size_mb': os.path.getsize(filepath) / 1024 / 1024,
                    'download_time': datetime.fromtimestamp(os.path.getctime(filepath)).isoformat()
                }
                report['file_details'].append(file_info)
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ NASAé›ªæ•°æ®ä¸‹è½½å™¨å¯åŠ¨")
    print("=" * 50)
    
    downloader = NASASnowDataDownloader()
    
    # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
    all_downloaded_files = []
    
    for dataset_key in downloader.datasets.keys():
        print(f"\nğŸ¯ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_key}")
        downloaded_files = downloader.download_dataset(dataset_key, max_files=50)
        all_downloaded_files.extend(downloaded_files)
    
    # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
    report = downloader.generate_download_report(all_downloaded_files)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(downloader.data_dir, f"download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ“Š ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print(f"   æ€»æ–‡ä»¶æ•°: {report['total_files']}")
    print(f"   æˆåŠŸä¸‹è½½: {report['successful_downloads']}")
    print(f"   å¤±è´¥ä¸‹è½½: {report['failed_downloads']}")
    
    # åˆå¹¶æ•°æ®é›†
    print(f"\nğŸ”„ å¼€å§‹åˆå¹¶æ•°æ®é›†...")
    merged_data = downloader.merge_all_datasets()
    
    if not merged_data.empty:
        downloader.save_merged_dataset(merged_data)
        print(f"âœ… æ‰©å±•æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œæ€»è®°å½•æ•°: {len(merged_data)}")
    else:
        print("âŒ æ•°æ®é›†åˆå¹¶å¤±è´¥")

if __name__ == "__main__":
    main()
