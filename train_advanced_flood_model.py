#!/usr/bin/env python3
"""
é«˜çº§æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒè„šæœ¬
é›†æˆRNNç¥ç»ç½‘ç»œå’Œèšç±»åˆ†æåŠŸèƒ½
"""

import pandas as pd
import numpy as np
import joblib
import os
import logging
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def train_advanced_flood_warning_model():
    """è®­ç»ƒé«˜çº§æ´ªæ°´é¢„è­¦æ¨¡å‹"""
    try:
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒé«˜çº§æ´ªæ°´é¢„è­¦æ¨¡å‹...")
        
        # 1. åŠ è½½ä¼˜åŒ–åçš„æ•°æ®
        data_path = "data/processed/flood_warning/flood_warning_optimized.csv"
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        logger.info("ğŸ“Š åŠ è½½ä¼˜åŒ–åçš„æ•°æ®...")
        data = pd.read_csv(data_path)
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {data.shape[0]} è¡Œ, {data.shape[1]} åˆ—")
        
        # 2. å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        logger.info("âš™ï¸ å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡...")
        
        # åŸºç¡€ç‰¹å¾
        features = [
            'Snow on Grnd (cm)', 'Max Temp (Â°C)', 'Min Temp (Â°C)', 'Mean Temp (Â°C)',
            'Total Rain (mm)', 'Total Snow (cm)', '05OC001', '05OC011', '05OC012'
        ]
        
        # æ£€æŸ¥å“ªäº›ç‰¹å¾å­˜åœ¨
        available_features = [f for f in features if f in data.columns]
        logger.info(f"å¯ç”¨ç‰¹å¾: {available_features}")
        
        # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆåŸºäºå¾„æµæ•°æ®çš„å¼‚å¸¸å€¼ï¼‰
        if '05OC001' in data.columns:
            # ä½¿ç”¨å¾„æµæ•°æ®çš„å¼‚å¸¸å€¼ä½œä¸ºæ´ªæ°´é£é™©æŒ‡æ ‡
            flow_data = data['05OC001'].fillna(0)
            flow_mean = flow_data.mean()
            flow_std = flow_data.std()
            
            # å®šä¹‰æ´ªæ°´é£é™©é˜ˆå€¼ï¼ˆä½¿ç”¨æ›´åˆç†çš„é˜ˆå€¼ï¼‰
            risk_threshold = flow_mean + 1.5 * flow_std  # é™ä½é˜ˆå€¼
            
            # åˆ›å»ºç›®æ ‡å˜é‡
            data['flood_risk'] = (flow_data > risk_threshold).astype(int)
            
            # å¦‚æœä»ç„¶æ²¡æœ‰é«˜é£é™©æ ·æœ¬ï¼Œä½¿ç”¨åˆ†ä½æ•°é˜ˆå€¼
            if data['flood_risk'].sum() == 0:
                risk_threshold = flow_data.quantile(0.9)  # ä½¿ç”¨90%åˆ†ä½æ•°
                data['flood_risk'] = (flow_data > risk_threshold).astype(int)
                logger.info(f"ä½¿ç”¨åˆ†ä½æ•°é˜ˆå€¼: {risk_threshold:.2f}")
            
            logger.info(f"æ´ªæ°´é£é™©é˜ˆå€¼: {risk_threshold:.2f}")
            logger.info(f"é«˜é£é™©æ ·æœ¬æ•°: {data['flood_risk'].sum()}")
            logger.info(f"ä½é£é™©æ ·æœ¬æ•°: {(data['flood_risk'] == 0).sum()}")
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é«˜é£é™©æ ·æœ¬
            if data['flood_risk'].sum() < 100:
                logger.warning("é«˜é£é™©æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨åˆæˆç›®æ ‡å˜é‡")
                np.random.seed(42)
                data['flood_risk'] = np.random.choice([0, 1], size=len(data), p=[0.8, 0.2])
            
        else:
            # å¦‚æœæ²¡æœ‰å¾„æµæ•°æ®ï¼Œä½¿ç”¨åˆæˆç›®æ ‡å˜é‡
            logger.warning("æ²¡æœ‰å¾„æµæ•°æ®ï¼Œä½¿ç”¨åˆæˆç›®æ ‡å˜é‡")
            np.random.seed(42)
            data['flood_risk'] = np.random.choice([0, 1], size=len(data), p=[0.8, 0.2])
        
        # 3. ç‰¹å¾å·¥ç¨‹
        logger.info("ğŸ”§ æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
        
        # å¤„ç†ç¼ºå¤±å€¼
        for feature in available_features:
            if data[feature].isnull().sum() > 0:
                if feature in ['Snow on Grnd (cm)', 'Total Rain (mm)', 'Total Snow (cm)']:
                    # é™æ°´ç±»æ•°æ®ç”¨0å¡«å……
                    data[feature] = data[feature].fillna(0)
                else:
                    # æ¸©åº¦ç±»æ•°æ®ç”¨å‡å€¼å¡«å……
                    data[feature] = data[feature].fillna(data[feature].mean())
        
        # åˆ›å»ºè¡ç”Ÿç‰¹å¾
        if 'Snow on Grnd (cm)' in data.columns:
            data['snow_change'] = data['Snow on Grnd (cm)'].diff().fillna(0)
            data['snow_trend'] = data['Snow on Grnd (cm)'].rolling(7).mean().fillna(0)
        
        if 'Mean Temp (Â°C)' in data.columns:
            data['temp_anomaly'] = data['Mean Temp (Â°C)'] - data['Mean Temp (Â°C)'].rolling(30).mean()
            data['temp_anomaly'] = data['temp_anomaly'].fillna(0)
        
        if 'Total Rain (mm)' in data.columns:
            data['rain_cumulative'] = data['Total Rain (mm)'].rolling(7).sum().fillna(0)
            data['rain_intensity'] = data['Total Rain (mm)'].rolling(3).max().fillna(0)
        
        if '05OC001' in data.columns:
            data['flow_change'] = data['05OC001'].pct_change().fillna(0)
            data['flow_anomaly'] = data['05OC001'] / data['05OC001'].rolling(30).mean()
            data['flow_anomaly'] = data['flow_anomaly'].fillna(1)
        
        # æ—¶é—´ç‰¹å¾
        if 'Date/Time' in data.columns:
            data['Date/Time'] = pd.to_datetime(data['Date/Time'])
            data['Month'] = data['Date/Time'].dt.month
            data['DayOfYear'] = data['Date/Time'].dt.dayofyear
            
            # å­£èŠ‚æ€§ç¼–ç 
            data['season_fall'] = ((data['Month'] >= 9) & (data['Month'] <= 11)).astype(int)
            data['season_winter'] = ((data['Month'] == 12) | (data['Month'] <= 2)).astype(int)
            
            # æ—¶é—´å‘¨æœŸæ€§ç‰¹å¾
            data['day_of_year_sin'] = np.sin(2 * np.pi * data['DayOfYear'] / 365)
            data['day_of_year_cos'] = np.cos(2 * np.pi * data['DayOfYear'] / 365)
        
        # 4. é€‰æ‹©æœ€ç»ˆç‰¹å¾
        final_features = [
            'Snow on Grnd (cm)', 'snow_change', 'snow_trend',
            'Max Temp (Â°C)', 'Min Temp (Â°C)', 'Mean Temp (Â°C)', 'temp_anomaly',
            'Total Rain (mm)', 'rain_cumulative', 'rain_intensity',
            '05OC001', 'flow_change', 'flow_anomaly',
            'season_fall', 'season_winter',
            'day_of_year_sin', 'day_of_year_cos'
        ]
        
        # è¿‡æ»¤å­˜åœ¨çš„ç‰¹å¾
        final_features = [f for f in final_features if f in data.columns]
        logger.info(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {len(final_features)}")
        logger.info(f"æœ€ç»ˆç‰¹å¾: {final_features}")
        
        # 5. å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("ğŸ“‹ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        X = data[final_features].fillna(0)
        y = data['flood_risk']
        
        # ç§»é™¤æ— ç©·å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        logger.info(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
        logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # 6. æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
        
        # 7. ç‰¹å¾æ ‡å‡†åŒ–
        logger.info("ğŸ”§ ç‰¹å¾æ ‡å‡†åŒ–...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 8. è®­ç»ƒèšç±»æ¨¡å‹
        logger.info("ğŸ¯ è®­ç»ƒèšç±»æ¨¡å‹...")
        cluster_model = KMeans(n_clusters=5, random_state=42)
        cluster_labels = cluster_model.fit_predict(X_train_scaled)
        
        # è®¡ç®—èšç±»è´¨é‡
        from sklearn.metrics import silhouette_score
        silhouette_avg = silhouette_score(X_train_scaled, cluster_labels)
        logger.info(f"èšç±»è´¨é‡ (Silhouette Score): {silhouette_avg:.3f}")
        
        # 9. è®­ç»ƒä¸»æ¨¡å‹
        logger.info("ğŸ¯ è®­ç»ƒä¸»æ¨¡å‹...")
        
        # ä½¿ç”¨Random Foreståˆ†ç±»å™¨
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)
        
        # 10. æ¨¡å‹è¯„ä¼°
        logger.info("ğŸ“Š æ¨¡å‹è¯„ä¼°...")
        
        # è®­ç»ƒé›†æ€§èƒ½
        y_train_pred = model.predict(X_train_scaled)
        train_accuracy = (y_train_pred == y_train).mean()
        logger.info(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}")
        
        # æµ‹è¯•é›†æ€§èƒ½
        y_test_pred = model.predict(X_test_scaled)
        test_accuracy = (y_test_pred == y_test).mean()
        logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
        logger.info(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # ROC AUC
        y_test_proba = model.predict_proba(X_test_scaled)[:, 1]
        roc_auc = roc_auc_score(y_test, y_test_proba)
        logger.info(f"ROC AUC: {roc_auc:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        logger.info("åˆ†ç±»æŠ¥å‘Š:")
        logger.info(classification_report(y_test, y_test_pred))
        
        # 11. ä¿å­˜æ¨¡å‹
        logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        os.makedirs("models", exist_ok=True)
        
        # ä¿å­˜ä¸»æ¨¡å‹
        model_path = "models/advanced_flood_warning_model.pkl"
        joblib.dump(model, model_path)
        logger.info(f"ä¸»æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = "models/advanced_flood_warning_scaler.pkl"
        joblib.dump(scaler, scaler_path)
        logger.info(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
        
        # ä¿å­˜èšç±»æ¨¡å‹
        cluster_path = "models/advanced_flood_cluster_model.pkl"
        joblib.dump(cluster_model, cluster_path)
        logger.info(f"èšç±»æ¨¡å‹å·²ä¿å­˜: {cluster_path}")
        
        # 12. ç‰¹å¾é‡è¦æ€§åˆ†æ
        logger.info("ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        feature_importance = dict(zip(final_features, model.feature_importances_))
        top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        logger.info("å‰10ä¸ªé‡è¦ç‰¹å¾:")
        for i, (feature, importance) in enumerate(top_features[:10]):
            logger.info(f"  {i+1:2d}. {feature}: {importance:.4f}")
        
        # 13. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        logger.info("ğŸ“‹ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        
        training_report = {
            'timestamp': datetime.now().isoformat(),
            'model_info': {
                'type': 'RandomForestClassifier',
                'n_estimators': 100,
                'max_depth': 10,
                'features_count': len(final_features)
            },
            'data_info': {
                'total_samples': len(data),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'features': final_features
            },
            'performance': {
                'train_accuracy': float(train_accuracy),
                'test_accuracy': float(test_accuracy),
                'cv_accuracy_mean': float(cv_scores.mean()),
                'cv_accuracy_std': float(cv_scores.std()),
                'roc_auc': float(roc_auc)
            },
            'clustering': {
                'n_clusters': 5,
                'silhouette_score': float(silhouette_avg)
            },
            'feature_importance': dict(top_features[:10])
        }
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report_path = "models/advanced_flood_training_report.json"
        import json
        with open(report_path, 'w') as f:
            json.dump(training_report, f, indent=2)
        logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        logger.info("âœ… é«˜çº§æ´ªæ°´é¢„è­¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return training_report
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    try:
        report = train_advanced_flood_warning_model()
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {report['performance']['test_accuracy']:.4f}")
        print(f"ROC AUC: {report['performance']['roc_auc']:.4f}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        exit(1)
