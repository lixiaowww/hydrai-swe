#!/usr/bin/env python3
"""
æ•°æ®ç§‘å­¦åˆ†æAPIç«¯ç‚¹
æä¾›é«˜çº§æ•°æ®åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬æ— ç›‘ç£å­¦ä¹ ã€å¼‚å¸¸æ£€æµ‹ã€èšç±»åˆ†æç­‰
"""

from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import asyncio
import json
import os
from datetime import datetime

# å¯¼å…¥æ•°æ®ç§‘å­¦åˆ†æå™¨
import sys
sys.path.append('/home/sean/hydrai_swe/src')
from models.data_science_analyzer import DataScienceAnalyzer
from models.exploration.insight_discovery import InsightDiscoveryModule

router = APIRouter(prefix="/data-science", tags=["Data Science Analysis"])

# å…¨å±€åˆ†æå™¨å®ä¾‹
analyzer_instance = None

class AnalysisRequest(BaseModel):
    """åˆ†æè¯·æ±‚æ¨¡å‹"""
    data_path: Optional[str] = None
    column: str = "snow_water_equivalent_mm"
    analysis_types: List[str] = ["decomposition", "anomaly", "clustering", "dimensionality", "statistical"]
    save_results: bool = True

class AnalysisResponse(BaseModel):
    """åˆ†æå“åº”æ¨¡å‹"""
    success: bool
    message: str
    analysis_id: Optional[str] = None
    results: Optional[Dict[str, Any]] = None
    timestamp: str

# å­˜å‚¨åˆ†æç»“æœçš„å­—å…¸
analysis_results_storage = {}


def _json_safe(value):
    """Convert pandas/numpy objects to JSON-serializable primitives."""
    import numpy as _np
    import pandas as _pd

    if isinstance(value, _pd.Series):
        idx = value.index
        try:
            idx = [i.isoformat() if hasattr(i, "isoformat") else str(i) for i in idx]
        except Exception:
            idx = [str(i) for i in idx]
        arr = value.astype(float).to_numpy()
        # Replace non-finite entries (NaN/Inf) with None for JSON compliance
        arr = [_np.nan if _np.isfinite(v) else None for v in arr]
        # Replace numpy.nan with None explicitly
        arr = [None if (isinstance(v, float) and (v != v)) else v for v in arr]
        return {"index": idx, "values": arr}
    if isinstance(value, _pd.DataFrame):
        data = {}
        for col in value.columns:
            data[col] = _json_safe(value[col])
        return {"columns": list(value.columns), "data": data}
    if isinstance(value, (list, tuple)):
        return [_json_safe(v) for v in value]
    if isinstance(value, (dict,)):
        return {str(k): _json_safe(v) for k, v in value.items()}
    if isinstance(value, (_np.integer,)):
        return int(value)
    if isinstance(value, (_np.floating,)):
        return float(value)
    if isinstance(value, (_np.ndarray,)):
        arr = value
        arr = _np.where(_np.isfinite(arr), arr, _np.nan).tolist()
        # json cannot carry NaN; map to None
        arr = [None if (isinstance(v, float) and (v != v)) else v for v in arr]
        return arr
    return value

@router.post("/analyze", response_model=AnalysisResponse)
async def run_comprehensive_analysis(request: AnalysisRequest):
    """
    è¿è¡Œæ•°æ®ç§‘å­¦ç»¼åˆåˆ†æ
    
    Args:
        request: åˆ†æè¯·æ±‚å‚æ•°
        
    Returns:
        AnalysisResponse: åˆ†æç»“æœ
    """
    try:
        global analyzer_instance
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer_instance = DataScienceAnalyzer()
        
        # ç¡®å®šæ•°æ®è·¯å¾„
        data_path = request.data_path
        if not data_path:
            # å°è¯•é»˜è®¤æ•°æ®è·¯å¾„
            default_paths = [
                "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                "data/processed/eccc_manitoba_snow_processed.csv",
                "data/raw/eccc_recent/eccc_recent_combined.csv"
            ]
            
            for path in default_paths:
                if os.path.exists(path):
                    data_path = path
                    break
            
            if not data_path:
                raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        
        # åŠ è½½æ•°æ®
        analyzer_instance.load_data(data_path)
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®åŠ è½½å¤±è´¥")
        
        # ç”Ÿæˆåˆ†æID
        analysis_id = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # è¿è¡Œåˆ†æ
        results = {}
        
        if "decomposition" in request.analysis_types:
            results["decomposition"] = _json_safe(analyzer_instance.advanced_time_series_decomposition(request.column))
        
        if "anomaly" in request.analysis_types:
            results["anomaly_detection"] = _json_safe(analyzer_instance.advanced_anomaly_detection(request.column))
        
        if "clustering" in request.analysis_types:
            results["clustering"] = _json_safe(analyzer_instance.clustering_analysis())
        
        if "dimensionality" in request.analysis_types:
            results["dimensionality_reduction"] = _json_safe(analyzer_instance.dimensionality_reduction_analysis())
        
        if "statistical" in request.analysis_types:
            results["statistical_tests"] = _json_safe(analyzer_instance.statistical_hypothesis_testing(request.column))
        
        # å­˜å‚¨ç»“æœ
        analysis_results_storage[analysis_id] = {
            "results": results,
            "timestamp": datetime.now().isoformat(),
            "request": request.dict()
        }
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        if request.save_results:
            save_path = f"analysis_results/{analysis_id}"
            os.makedirs(save_path, exist_ok=True)
            
            with open(f"{save_path}/results.json", "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            # åˆ›å»ºå¯è§†åŒ–
            visualizations = analyzer_instance.create_interactive_visualizations(save_path)
            
            # ä¿å­˜å¯è§†åŒ–ä¿¡æ¯
            with open(f"{save_path}/visualizations.json", "w", encoding="utf-8") as f:
                json.dump(list(visualizations.keys()), f, ensure_ascii=False, indent=2)
        
        return AnalysisResponse(
            success=True,
            message="æ•°æ®ç§‘å­¦åˆ†æå®Œæˆ",
            analysis_id=analysis_id,
            results=results,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@router.get("/analysis/{analysis_id}", response_model=AnalysisResponse)
async def get_analysis_results(analysis_id: str):
    """
    è·å–åˆ†æç»“æœ
    
    Args:
        analysis_id: åˆ†æID
        
    Returns:
        AnalysisResponse: åˆ†æç»“æœ
    """
    if analysis_id not in analysis_results_storage:
        raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
    
    stored_data = analysis_results_storage[analysis_id]
    
    return AnalysisResponse(
        success=True,
        message="åˆ†æç»“æœè·å–æˆåŠŸ",
        analysis_id=analysis_id,
        results=stored_data["results"],
        timestamp=stored_data["timestamp"]
    )

@router.get("/analysis/{analysis_id}/visualizations")
async def get_analysis_visualizations(analysis_id: str):
    """
    è·å–åˆ†æå¯è§†åŒ–æ–‡ä»¶åˆ—è¡¨
    
    Args:
        analysis_id: åˆ†æID
        
    Returns:
        dict: å¯è§†åŒ–æ–‡ä»¶åˆ—è¡¨
    """
    save_path = f"analysis_results/{analysis_id}"
    
    if not os.path.exists(save_path):
        raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
    
    visualizations = []
    for file in os.listdir(save_path):
        if file.endswith('.html'):
            visualizations.append({
                "name": file.replace('.html', ''),
                "file": file,
                "url": f"/analysis_results/{analysis_id}/{file}"
            })
    
    return {
        "analysis_id": analysis_id,
        "visualizations": visualizations
    }

@router.get("/decomposition")
async def get_time_series_decomposition(
    column: str = Query("snow_water_equivalent_mm", description="è¦åˆ†æçš„åˆ—å"),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """
    è·å–æ—¶é—´åºåˆ—åˆ†è§£ç»“æœ
    
    Args:
        column: è¦åˆ†æçš„åˆ—å
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åˆ†è§£ç»“æœ
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
                default_paths = [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]
                for path in default_paths:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")
        
        raw = analyzer_instance.advanced_time_series_decomposition(column)
        results = _json_safe(raw)
        
        return {
            "success": True,
            "column": column,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†è§£åˆ†æå¤±è´¥: {str(e)}")

@router.get("/anomaly-detection")
async def get_anomaly_detection(
    column: str = Query("snow_water_equivalent_mm", description="è¦åˆ†æçš„åˆ—å"),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """
    è·å–å¼‚å¸¸æ£€æµ‹ç»“æœ
    
    Args:
        column: è¦åˆ†æçš„åˆ—å
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: å¼‚å¸¸æ£€æµ‹ç»“æœ
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
                default_paths = [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]
                for path in default_paths:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")
        
        results = _json_safe(analyzer_instance.advanced_anomaly_detection(column))
        
        return {
            "success": True,
            "column": column,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")

@router.get("/clustering")
async def get_clustering_analysis(
    columns: Optional[str] = Query(None, description="è¦åˆ†æçš„åˆ—åï¼Œç”¨é€—å·åˆ†éš”"),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """
    è·å–èšç±»åˆ†æç»“æœ
    
    Args:
        columns: è¦åˆ†æçš„åˆ—åï¼Œç”¨é€—å·åˆ†éš”
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: èšç±»åˆ†æç»“æœ
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
                default_paths = [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]
                for path in default_paths:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")
        
        # å¤„ç†åˆ—åå‚æ•°
        column_list = None
        if columns:
            column_list = [col.strip() for col in columns.split(",")]
        
        raw = analyzer_instance.clustering_analysis(column_list)

        def to_list(v):
            try:
                out = list(v)
            except Exception:
                out = []
            # ensure python primitives
            py = []
            for x in out:
                try:
                    if x is None:
                        py.append(None)
                    elif isinstance(x, (int, float, str, bool)):
                        py.append(x)
                    else:
                        py.append(int(x))
                except Exception:
                    try:
                        py.append(float(x))
                    except Exception:
                        py.append(str(x))
            return py

        compact = {}
        if isinstance(raw, dict):
            km = raw.get('kmeans', {})
            db = raw.get('dbscan', {})
            hi = raw.get('hierarchical', {})
            compact['kmeans'] = {
                'labels': to_list(km.get('labels', [])),
                'n_clusters': int(km.get('n_clusters', 0) or 0),
                'silhouette_score': float(km.get('silhouette_score', 0.0) or 0.0)
            }
            compact['dbscan'] = {
                'labels': to_list(db.get('labels', [])),
                'n_clusters': int(db.get('n_clusters', 0) or 0),
                'n_noise': int(db.get('n_noise', 0) or 0)
            }
            compact['hierarchical'] = {
                'labels': to_list(hi.get('labels', [])),
                'n_clusters': int(hi.get('n_clusters', 0) or 0),
                'silhouette_score': float(hi.get('silhouette_score', 0.0) or 0.0)
            }
            compact['features_used'] = [str(x) for x in to_list(raw.get('features_used', []))]
            compact['interpretation'] = raw.get('interpretation', {})
        else:
            compact = {'kmeans': {'labels': [], 'n_clusters': 0, 'silhouette_score': 0.0},
                       'dbscan': {'labels': [], 'n_clusters': 0, 'n_noise': 0},
                       'hierarchical': {'labels': [], 'n_clusters': 0, 'silhouette_score': 0.0},
                       'features_used': []}

        return {
            'success': True,
            'columns': column_list,
            'results': compact,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"èšç±»åˆ†æå¤±è´¥: {str(e)}")

@router.get("/dimensionality-reduction")
async def get_dimensionality_reduction(
    columns: Optional[str] = Query(None, description="è¦åˆ†æçš„åˆ—åï¼Œç”¨é€—å·åˆ†éš”"),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """
    è·å–é™ç»´åˆ†æç»“æœ
    
    Args:
        columns: è¦åˆ†æçš„åˆ—åï¼Œç”¨é€—å·åˆ†éš”
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: é™ç»´åˆ†æç»“æœ
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
                default_paths = [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]
                for path in default_paths:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")
        
        # å¤„ç†åˆ—åå‚æ•°
        column_list = None
        if columns:
            column_list = [col.strip() for col in columns.split(",")]
        
        results = _json_safe(analyzer_instance.dimensionality_reduction_analysis(column_list))
        
        return {
            "success": True,
            "columns": column_list,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é™ç»´åˆ†æå¤±è´¥: {str(e)}")

@router.get("/statistical-tests")
async def get_statistical_tests(
    column: str = Query("snow_water_equivalent_mm", description="è¦åˆ†æçš„åˆ—å"),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """
    è·å–ç»Ÿè®¡å‡è®¾æ£€éªŒç»“æœ
    
    Args:
        column: è¦åˆ†æçš„åˆ—å
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: ç»Ÿè®¡æ£€éªŒç»“æœ
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
                default_paths = [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]
                for path in default_paths:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")
        
        raw = analyzer_instance.statistical_hypothesis_testing(column)
        print(f"ğŸ” ç»Ÿè®¡æ£€éªŒAPIè°ƒç”¨ç»“æœ: {raw}")
        
        # ç›´æ¥ä½¿ç”¨è¿”å›çš„æ•°æ®ç»“æ„
        compact = raw if isinstance(raw, dict) else {}
        return {
            'success': True,
            'column': column,
            'results': compact,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡æ£€éªŒå¤±è´¥: {str(e)}")

@router.get("/visualizations")
async def create_visualizations(
    analysis_id: Optional[str] = Query(None, description="åˆ†æID"),
    save_path: Optional[str] = Query(None, description="ä¿å­˜è·¯å¾„")
):
    """
    åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–
    
    Args:
        analysis_id: åˆ†æID
        save_path: ä¿å­˜è·¯å¾„
        
    Returns:
        dict: å¯è§†åŒ–ç»“æœ
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="åˆ†æå™¨æœªåˆå§‹åŒ–")
        
        # ç¡®å®šä¿å­˜è·¯å¾„
        if not save_path:
            if analysis_id:
                save_path = f"analysis_results/{analysis_id}"
            else:
                save_path = f"visualizations/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆ›å»ºå¯è§†åŒ–
        visualizations = analyzer_instance.create_interactive_visualizations(save_path)
        
        return {
            "success": True,
            "save_path": save_path,
            "visualizations": list(visualizations.keys()),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {str(e)}")

@router.get("/data-info")
async def get_data_info(data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")):
    """
    è·å–æ•°æ®ä¿¡æ¯
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: æ•°æ®ä¿¡æ¯
    """
    try:
        global analyzer_instance
        
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                # ä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
                default_paths = [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]
                for path in default_paths:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")
        
        data = analyzer_instance.data
        
        return {
            "success": True,
            "data_info": {
                "shape": list(data.shape),
                "columns": data.columns.tolist(),
                "dtypes": {c: str(dtype) for c, dtype in data.dtypes.items()},
                "time_range": {
                    "start": data.index.min().isoformat() if hasattr(data.index.min(), 'isoformat') else str(data.index.min()),
                    "end": data.index.max().isoformat() if hasattr(data.index.max(), 'isoformat') else str(data.index.max())
                },
                "missing_values": {k: int(v) for k, v in data.isnull().sum().to_dict().items()},
                "numeric_columns": data.select_dtypes(include=['number']).columns.tolist()
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®ä¿¡æ¯å¤±è´¥: {str(e)}")

@router.get("/analysis-list")
async def get_analysis_list():
    """
    è·å–æ‰€æœ‰åˆ†æç»“æœåˆ—è¡¨
    
    Returns:
        dict: åˆ†æç»“æœåˆ—è¡¨
    """
    try:
        analysis_list = []
        
        for analysis_id, data in analysis_results_storage.items():
            analysis_list.append({
                "analysis_id": analysis_id,
                "timestamp": data["timestamp"],
                "analysis_types": list(data["results"].keys()),
                "request": data["request"]
            })
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        analysis_list.sort(key=lambda x: x["timestamp"], reverse=True)
        
        return {
            "success": True,
            "analysis_list": analysis_list,
            "total_count": len(analysis_list),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†æåˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/factor-discovery")
async def factor_discovery(
    target: str = Query("snow_water_equivalent_mm", description="ç›®æ ‡åˆ—å"),
    top_k: int = Query(10, ge=1, le=50),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """æ— ç›‘ç£å†·é—¨å½±å“å› ç´ å‘ç°æ¥å£ã€‚"""
    try:
        global analyzer_instance
        if analyzer_instance is None or analyzer_instance.data is None:
            analyzer_instance = DataScienceAnalyzer()
            if data_path:
                analyzer_instance.load_data(data_path)
            else:
                for path in [
                    "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                    "data/processed/eccc_manitoba_snow_processed.csv"
                ]:
                    if os.path.exists(path):
                        analyzer_instance.load_data(path)
                        break
        if analyzer_instance.data is None:
            raise HTTPException(status_code=400, detail="æ•°æ®æœªåŠ è½½")

        raw = analyzer_instance.discover_cold_factors(target, top_k)
        return {"success": True, "results": _json_safe(raw), "timestamp": datetime.now().isoformat()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å› ç´ å‘ç°å¤±è´¥: {str(e)}")

@router.delete("/analysis/{analysis_id}")
async def delete_analysis(analysis_id: str):
    """
    åˆ é™¤åˆ†æç»“æœ
    
    Args:
        analysis_id: åˆ†æID
        
    Returns:
        dict: åˆ é™¤ç»“æœ
    """
    try:
        if analysis_id not in analysis_results_storage:
            raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
        
        # ä»å†…å­˜ä¸­åˆ é™¤
        del analysis_results_storage[analysis_id]
        
        # åˆ é™¤æ–‡ä»¶
        save_path = f"analysis_results/{analysis_id}"
        if os.path.exists(save_path):
            import shutil
            shutil.rmtree(save_path)
        
        return {
            "success": True,
            "message": f"åˆ†æç»“æœ {analysis_id} å·²åˆ é™¤",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤åˆ†æç»“æœå¤±è´¥: {str(e)}")

@router.get("/unsupervised-insights")
async def get_unsupervised_insights(
    target_column: str = Query("estimated_soil_moisture", description="ç›®æ ‡åˆ—å"),
    data_path: Optional[str] = Query(None, description="æ•°æ®æ–‡ä»¶è·¯å¾„")
):
    """
    è·å–æ— ç›‘ç£å­¦ä¹ æ´å¯Ÿ - è°ƒç”¨çœŸå®çš„InsightDiscoveryModule
    
    Args:
        target_column: ç›®æ ‡åˆ†æåˆ—å
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: æ— ç›‘ç£å­¦ä¹ æ´å¯Ÿç»“æœ
    """
    try:
        # åˆ›å»ºæ— ç›‘ç£æ¢ç´¢æ¨¡å—å®ä¾‹
        insight_module = InsightDiscoveryModule()
        
        # ç¡®å®šæ•°æ®è·¯å¾„å¹¶åŠ è½½æ•°æ®
        if not data_path:
            # å°è¯•é»˜è®¤æ•°æ®è·¯å¾„
            default_paths = [
                "src/neuralhydrology/data/red_river_basin/timeseries.csv",
                "data/processed/eccc_manitoba_snow_processed.csv",
                "data/raw/eccc_recent/eccc_recent_combined.csv"
            ]
            
            for path in default_paths:
                if os.path.exists(path):
                    data_path = path
                    break
        
        if not data_path or not os.path.exists(data_path):
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        
        # åŠ è½½æ•°æ®
        import pandas as pd
        data = pd.read_csv(data_path)
        
        # å¤„ç†æ—¥æœŸç´¢å¼•
        if 'date' in data.columns:
            data['date'] = pd.to_datetime(data['date'])
            data.set_index('date', inplace=True)
        elif 'Date' in data.columns:
            data['Date'] = pd.to_datetime(data['Date'])
            data.set_index('Date', inplace=True)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(data)} æ¡è®°å½•")
        print(f"ğŸ“Š æ•°æ®åˆ—: {list(data.columns)}")
        
        # è¿è¡Œæ— ç›‘ç£æ¨¡å¼å‘ç°
        insights = insight_module.discover_patterns(data, target_column)
        
        # è¿è¡Œè§£è¯»æ´å¯Ÿç»“æœ
        interpretation = insight_module.interpret_insights(insights)
        
        # ç»„åˆå®Œæ•´ç»“æœ
        complete_results = {
            "insights": _json_safe(insights),
            "interpretation": _json_safe(interpretation)
        }
        
        return {
            "success": True,
            "target_column": target_column,
            "data_path": data_path,
            "data_shape": list(data.shape),
            "results": complete_results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        import traceback
        error_detail = f"æ— ç›‘ç£å­¦ä¹ æ´å¯Ÿå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)

@router.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    Returns:
        dict: å¥åº·çŠ¶æ€
    """
    return {
        "status": "healthy",
        "service": "Data Science Analysis API",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }
