"""
ç®€åŒ–ç‰ˆæ•°æ®ç§‘å­¦åˆ†æå™¨ - ä¿®å¤ç»ˆç«¯æŠ¥é”™
åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼Œç§»é™¤å¤æ‚çš„ç¼©è¿›é—®é¢˜
"""
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import logging

class DataScienceAnalyzer:
    def __init__(self):
        self.data = None
        self.scaler = StandardScaler()
        self.analysis_results = {}
    
    def advanced_anomaly_detection(self, column='snow_water_equivalent_mm'):
        """ç®€åŒ–çš„å¼‚å¸¸æ£€æµ‹"""
        print(f"\nğŸš¨ æ‰§è¡Œé«˜çº§å¼‚å¸¸æ£€æµ‹: {column}")
        print("=" * 60)
        
        if self.data is None or column not in self.data.columns:
            return {}
        
        series = self.data[column].dropna()
        if len(series) == 0:
            return {}
        
        # Z-scoreå¼‚å¸¸æ£€æµ‹
        z_scores = np.abs(stats.zscore(series))
        z_anomalies = (z_scores > 2.0).astype(float)
        
        # Isolation Forest
        X = series.values.reshape(-1, 1)
        iso_forest = IsolationForest(contamination=0.02, random_state=42)
        iso_predictions = iso_forest.fit_predict(X)
        iso_anomalies = (iso_predictions == -1).astype(float)
        
        # é›†æˆç»“æœ
        ensemble_anomalies = ((z_anomalies + iso_anomalies) >= 1).astype(float)
        
        results = {
            'statistical': {
                'z_score_anomalies': z_anomalies.tolist()
            },
            'machine_learning': {
                'isolation_forest_anomalies': iso_anomalies.tolist()
            },
            'ensemble': {
                'ensemble_anomalies': ensemble_anomalies.tolist()
            },
            'interpretation': f"æ£€æµ‹åˆ° {int(ensemble_anomalies.sum())} ä¸ªå¼‚å¸¸ç‚¹ï¼Œå æ€»æ•°æ®çš„ {ensemble_anomalies.sum()/len(ensemble_anomalies)*100:.1f}%"
        }
        
        print("âœ… Advanced anomaly detection completed")
        return results
    
    def clustering_analysis(self):
        """ç®€åŒ–çš„èšç±»åˆ†æ"""
        print("\nğŸ” æ‰§è¡Œèšç±»åˆ†æ")
        print("=" * 60)
        
        if self.data is None:
            return {}
        
        # é€‰æ‹©æ•°å€¼åˆ—
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return {}
        
        data_subset = self.data[numeric_cols].dropna()
        if len(data_subset) == 0:
            return {}
        
        # æ ‡å‡†åŒ–æ•°æ®
        X_scaled = self.scaler.fit_transform(data_subset)
        
        # K-meansèšç±»
        print("ğŸ¯ K-meansèšç±»...")
        kmeans = KMeans(n_clusters=3, random_state=42)
        kmeans_labels = kmeans.fit_predict(X_scaled)
        
        # DBSCANèšç±»
        print("ğŸŒ DBSCANèšç±»...")
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        dbscan_labels = dbscan.fit_predict(X_scaled)
        
        results = {
            'kmeans': {
                'labels': kmeans_labels.tolist(),
                'n_clusters': len(set(kmeans_labels))
            },
            'dbscan': {
                'labels': dbscan_labels.tolist(),
                'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
            },
            'interpretation': f"K-meansè¯†åˆ«äº† {len(set(kmeans_labels))} ä¸ªèšç±»ï¼ŒDBSCANè¯†åˆ«äº† {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)} ä¸ªèšç±»"
        }
        
        print("âœ… Clustering analysis completed")
        return results
    
    def statistical_hypothesis_testing(self, column='snow_water_equivalent_mm'):
        """ç®€åŒ–çš„ç»Ÿè®¡å‡è®¾æ£€éªŒ"""
        print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡å‡è®¾æ£€éªŒ: {column}")
        print("=" * 60)
        
        if self.data is None or column not in self.data.columns:
            return {}
        
        series = self.data[column].dropna()
        if len(series) == 0:
            return {}
        
        # æ­£æ€æ€§æ£€éªŒ
        print("ğŸ“Š æ­£æ€æ€§æ£€éªŒ...")
        shapiro_stat, shapiro_p = stats.shapiro(series[:5000] if len(series) > 5000 else series)
        
        # å¹³ç¨³æ€§æ£€éªŒï¼ˆç®€åŒ–ç‰ˆï¼‰
        print("ğŸ“Š å¹³ç¨³æ€§æ£€éªŒ...")
        # ä½¿ç”¨ADFæ£€éªŒçš„ç®€åŒ–ç‰ˆæœ¬
        try:
            from statsmodels.tsa.stattools import adfuller
            adf_stat, adf_p, _, _, _, _ = adfuller(series)
            stationarity_test = {'adf_statistic': adf_stat, 'p_value': adf_p}
        except:
            stationarity_test = {'adf_statistic': 0, 'p_value': 1.0}
        
        results = {
            'normality': {
                'shapiro_statistic': float(shapiro_stat),
                'shapiro_p_value': float(shapiro_p)
            },
            'stationarity': stationarity_test,
            'interpretation': f"æ•°æ®{'ç¬¦åˆ' if shapiro_p > 0.05 else 'ä¸ç¬¦åˆ'}æ­£æ€åˆ†å¸ƒ (p={shapiro_p:.4f})ï¼Œ{'æ˜¯' if stationarity_test['p_value'] < 0.05 else 'ä¸æ˜¯'}å¹³ç¨³åºåˆ—"
        }
        
        print("âœ… Statistical hypothesis testing completed")
        return results
    
    def advanced_time_series_decomposition(self, column='snow_water_equivalent_mm'):
        """ç®€åŒ–çš„æ—¶é—´åºåˆ—åˆ†è§£"""
        print(f"\nğŸ” æ‰§è¡Œé«˜çº§æ—¶é—´åºåˆ—åˆ†è§£: {column}")
        print("=" * 60)
        
        if self.data is None or column not in self.data.columns:
            return {}
        
        series = self.data[column].dropna()
        if len(series) == 0:
            return {}
        
        # ç®€å•çš„ç§»åŠ¨å¹³å‡åˆ†è§£
        print("ğŸ“Š æ‰§è¡Œç®€åŒ–åˆ†è§£...")
        window = min(365, len(series) // 4)
        
        trend = series.rolling(window=window, center=True).mean()
        seasonal = series - trend
        seasonal = seasonal.rolling(window=7, center=True).mean()  # å‘¨æœŸæ€§
        residual = series - trend - seasonal
        
        # å¡«å……ç¼ºå¤±å€¼
        trend = trend.fillna(method='bfill').fillna(method='ffill')
        seasonal = seasonal.fillna(0)
        residual = residual.fillna(0)
        
        results = {
            'stl_decomposition': {
                'trend': {
                    'index': [t.isoformat() for t in series.index],
                    'values': trend.tolist()
                },
                'seasonal': {
                    'index': [t.isoformat() for t in series.index],
                    'values': seasonal.tolist()
                },
                'resid': {
                    'index': [t.isoformat() for t in series.index],
                    'values': residual.tolist()
                }
            },
            'interpretation': f"æ—¶é—´åºåˆ—åˆ†è§£å®Œæˆï¼šè¶‹åŠ¿èŒƒå›´ {trend.min():.1f} åˆ° {trend.max():.1f}ï¼Œå­£èŠ‚æ€§èŒƒå›´ {seasonal.min():.1f} åˆ° {seasonal.max():.1f}"
        }
        
        print("âœ… Advanced time series decomposition completed")
        return results
    
    def discover_cold_factors(self, target_column='snow_water_equivalent_mm', top_k=10):
        """ç®€åŒ–çš„å› å­å‘ç°"""
        print(f"\nğŸ“Š æ‰§è¡Œå› å­å‘ç°: {target_column}")
        
        if self.data is None or target_column not in self.data.columns:
            return {}
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        factors = []
        
        for col in numeric_cols:
            if col == target_column:
                continue
            
            try:
                correlation = stats.spearmanr(self.data[col].dropna(), 
                                           self.data[target_column].dropna())[0]
                if not np.isnan(correlation):
                    factors.append({
                        'factor': col,
                        'correlation': abs(correlation),
                        'score': abs(correlation)
                    })
            except:
                continue
        
        # æ’åºå¹¶å–å‰kä¸ª
        factors.sort(key=lambda x: x['score'], reverse=True)
        factors = factors[:top_k]
        
        results = {
            'high_predictive': factors,
            'interpretation': f"å‘ç° {len(factors)} ä¸ªé‡è¦å› å­ï¼Œæœ€å¼ºç›¸å…³å› å­æ˜¯ {factors[0]['factor'] if factors else 'None'}"
        }
        
        print("âœ… Factor discovery completed")
        return results


