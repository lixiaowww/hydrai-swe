#!/usr/bin/env python3
"""
æ— ç›‘ç£æ¢ç´¢æ¨¡å— - å‘ç°é—®é¢˜èƒŒåçš„æ¨¡å¼
å®šä½ï¼šæ¢ç´¢ + è§£é‡Šï¼Œè¡¥å……é¢„æµ‹çš„å¯ä¿¡åº¦å’Œç†è§£
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import logging
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    logger.warning("âš ï¸ ç»˜å›¾åº“æœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–åŠŸèƒ½")
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class InsightDiscoveryModule:
    """æ— ç›‘ç£æ¢ç´¢æ¨¡å— - å‘ç°é—®é¢˜èƒŒåçš„æ¨¡å¼"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¢ç´¢æ¨¡å—"""
        self.scaler = StandardScaler()
        self.pca = None
        self.clusters = None
        self.anomalies = None
        self.insights = {}
        
        logger.info("ğŸ” æ— ç›‘ç£æ¢ç´¢æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def discover_patterns(self, data: pd.DataFrame, target_col: str = 'estimated_soil_moisture') -> Dict:
        """å‘ç°æ•°æ®èƒŒåçš„æ¨¡å¼"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ— ç›‘ç£æ¨¡å¼å‘ç°...")
            
            # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†
            logger.info("ğŸ”§ æ­¥éª¤1: æ•°æ®é¢„å¤„ç†...")
            X_processed = self._preprocess_data(data)
            
            # æ­¥éª¤2: å¼‚å¸¸æ£€æµ‹
            logger.info("ğŸ” æ­¥éª¤2: å¼‚å¸¸æ£€æµ‹...")
            anomaly_insights = self._detect_anomalies(X_processed, data)
            
            # æ­¥éª¤3: èšç±»åˆ†æ
            logger.info("ğŸ” æ­¥éª¤3: èšç±»åˆ†æ...")
            cluster_insights = self._cluster_analysis(X_processed, data)
            
            # æ­¥éª¤4: é™ç»´å¯è§†åŒ–
            logger.info("ğŸ” æ­¥éª¤4: é™ç»´å¯è§†åŒ–...")
            dimension_insights = self._dimension_reduction(X_processed, data)
            
            # æ­¥éª¤5: æ—¶é—´æ¨¡å¼åˆ†æ
            logger.info("ğŸ” æ­¥éª¤5: æ—¶é—´æ¨¡å¼åˆ†æ...")
            temporal_insights = self._temporal_patterns(data, target_col)
            
            # æ­¥éª¤6: é£é™©æœºåˆ¶è¯†åˆ«
            logger.info("ğŸ” æ­¥éª¤6: é£é™©æœºåˆ¶è¯†åˆ«...")
            risk_insights = self._identify_risk_mechanisms(data, target_col)
            
            # æ­¥éª¤7: é‡è¦å½±å“å› ç´ å‘ç°
            logger.info("ğŸ” æ­¥éª¤7: é‡è¦å½±å“å› ç´ å‘ç°...")
            factor_insights = self._discover_important_factors(data, target_col)
            
            # æ­¥éª¤8: ç›¸å…³æ€§ç½‘ç»œåˆ†æ
            logger.info("ğŸ” æ­¥éª¤8: ç›¸å…³æ€§ç½‘ç»œåˆ†æ...")
            correlation_insights = self._analyze_correlation_network(data, target_col)
            
            # æ•´åˆæ‰€æœ‰æ´å¯Ÿ
            self.insights = {
                'timestamp': datetime.now().isoformat(),
                'anomalies': anomaly_insights,
                'clusters': cluster_insights,
                'dimensions': dimension_insights,
                'temporal': temporal_insights,
                'risk_mechanisms': risk_insights,
                'important_factors': factor_insights,
                'correlation_network': correlation_insights
            }
            
            # æ­¥éª¤9: ç”Ÿæˆæ‘˜è¦ (åœ¨æ‰€æœ‰æ´å¯Ÿæ„å»ºå®Œæˆå)
            logger.info("ğŸ” æ­¥éª¤9: ç”Ÿæˆæ‘˜è¦...")
            summary_insights = self._generate_summary()
            self.insights['summary'] = summary_insights
            
            logger.info("ğŸ‰ æ— ç›‘ç£æ¨¡å¼å‘ç°å®Œæˆï¼")
            return self.insights
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å¼å‘ç°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _preprocess_data(self, data: pd.DataFrame) -> np.ndarray:
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç† - ä¸“é—¨å¤„ç†é«˜ç¼ºå¤±ç‡æ•°æ®"""
        try:
            # é€‰æ‹©æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            
            # ç§»é™¤ç›®æ ‡å˜é‡
            if 'estimated_soil_moisture' in numeric_cols:
                numeric_cols.remove('estimated_soil_moisture')
            
            # åˆ†æç¼ºå¤±å€¼æ¨¡å¼
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            logger.info(f"ğŸ“Š ç¼ºå¤±å€¼åˆ†æ: æ€»ç‰¹å¾æ•° {len(numeric_cols)}")
            
            # åªä¿ç•™ç¼ºå¤±ç‡ < 50% çš„ç‰¹å¾
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            high_missing_features = missing_rates[missing_rates >= 0.5].index.tolist()
            
            logger.info(f"âœ… æœ‰æ•ˆç‰¹å¾: {len(valid_features)} ä¸ª (ç¼ºå¤±ç‡ < 50%)")
            logger.info(f"âš ï¸ é«˜ç¼ºå¤±ç‰¹å¾: {len(high_missing_features)} ä¸ª (ç¼ºå¤±ç‡ >= 50%)")
            
            if len(valid_features) == 0:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œå°è¯•æ”¾å®½æ ‡å‡†åˆ°80%ç¼ºå¤±ç‡")
                valid_features = missing_rates[missing_rates < 0.8].index.tolist()
                if len(valid_features) == 0:
                    logger.error("âŒ æ‰€æœ‰ç‰¹å¾ç¼ºå¤±ç‡éƒ½è¿‡é«˜ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
                    return np.array([])
            
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„ç¼ºå¤±å€¼å¡«å……
            X = data[valid_features].copy()
            
            # å¯¹äºè¿ç»­å˜é‡ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    median_val = X[col].median()
                    if pd.isna(median_val):
                        # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼Œä½¿ç”¨0å¡«å……
                        X[col] = X[col].fillna(0)
                    else:
                        X[col] = X[col].fillna(median_val)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNå€¼
            remaining_nans = X.isnull().sum().sum()
            if remaining_nans > 0:
                logger.warning(f"âš ï¸ ä»æœ‰ {remaining_nans} ä¸ªNaNå€¼ï¼Œä½¿ç”¨0å¡«å……")
                X = X.fillna(0)
            
            # æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            logger.info(f"âœ… æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å®Œæˆ: {X.shape[1]} ä¸ªæœ‰æ•ˆç‰¹å¾")
            logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X_scaled.shape}")
            
            return X_scaled
            
        except Exception as e:
            logger.error(f"âŒ æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return np.array([])
    
    def _detect_anomalies(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """å¼‚å¸¸æ£€æµ‹"""
        try:
            # ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(X)
            
            # ç»Ÿè®¡å¼‚å¸¸
            anomaly_count = np.sum(anomaly_labels == -1)
            total_count = len(anomaly_labels)
            anomaly_rate = anomaly_count / total_count
            
            # åˆ†æå¼‚å¸¸ç‰¹å¾
            anomaly_data = data[anomaly_labels == -1]
            normal_data = data[anomaly_labels == 1]
            
            anomaly_insights = {
                'anomaly_count': int(anomaly_count),
                'anomaly_rate': float(anomaly_rate),
                'total_count': int(total_count),
                'anomaly_features': self._analyze_anomaly_features(anomaly_data, normal_data),
                'anomaly_timestamps': anomaly_data['timestamp'].tolist() if 'timestamp' in anomaly_data.columns else []
            }
            
            logger.info(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {anomaly_count} ä¸ªå¼‚å¸¸ ({anomaly_rate:.1%})")
            return anomaly_insights
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_anomaly_features(self, anomaly_data: pd.DataFrame, normal_data: pd.DataFrame) -> Dict:
        """åˆ†æå¼‚å¸¸ç‰¹å¾"""
        try:
            numeric_cols = anomaly_data.select_dtypes(include=[np.number]).columns.tolist()
            
            feature_analysis = {}
            for col in numeric_cols:
                if col in normal_data.columns:
                    anomaly_mean = anomaly_data[col].mean()
                    normal_mean = normal_data[col].mean()
                    difference = anomaly_mean - normal_mean
                    
                    feature_analysis[col] = {
                        'anomaly_mean': float(anomaly_mean),
                        'normal_mean': float(normal_mean),
                        'difference': float(difference),
                        'deviation': float(abs(difference) / normal_mean) if normal_mean != 0 else 0
                    }
            
            return feature_analysis
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _cluster_analysis(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """æ”¹è¿›çš„èšç±»åˆ†æ - å¢åŠ ç¼ºå¤±å€¼æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
            if np.isnan(X).any():
                logger.error("âŒ è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†'
                }
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if X.size == 0:
                logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤'
                }
            
            # ç¡®å®šæœ€ä½³èšç±»æ•°
            silhouette_scores = []
            K_range = range(2, min(11, len(X) // 10 + 1))
            
            if len(K_range) == 0:
                logger.warning("âš ï¸ æ•°æ®é‡è¿‡å°‘ï¼Œä½¿ç”¨é»˜è®¤èšç±»æ•°2")
                K_range = [2]
            
            for k in K_range:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(X)
                    score = silhouette_score(X, cluster_labels)
                    silhouette_scores.append(score)
                except Exception as e:
                    logger.warning(f"âš ï¸ èšç±»æ•° {k} å¤±è´¥: {e}")
                    silhouette_scores.append(-1)
            
            if not silhouette_scores or max(silhouette_scores) == -1:
                logger.warning("âš ï¸ æ‰€æœ‰èšç±»æ•°éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤èšç±»æ•°2")
                best_k = 2
                best_score = 0
            else:
                best_k = K_range[np.argmax(silhouette_scores)]
                best_score = max(silhouette_scores)
            
            # æ‰§è¡Œæœ€ä½³èšç±»
            try:
                kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(X)
                
                # åˆ†æèšç±»ç‰¹å¾
                data_with_clusters = data.copy()
                data_with_clusters['cluster'] = cluster_labels
                
                cluster_insights = {
                    'optimal_clusters': int(best_k),
                    'silhouette_score': float(best_score),
                    'cluster_sizes': data_with_clusters['cluster'].value_counts().to_dict(),
                    'cluster_characteristics': self._analyze_cluster_characteristics(data_with_clusters)
                }
                
                self.clusters = cluster_labels
                
                logger.info(f"âœ… èšç±»åˆ†æå®Œæˆ: æœ€ä½³èšç±»æ•° {best_k}, è½®å»“ç³»æ•° {best_score:.3f}")
                return cluster_insights
                
            except Exception as e:
                logger.error(f"âŒ æœ€ç»ˆèšç±»å¤±è´¥: {e}")
                return {'status': 'error', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"âŒ èšç±»åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_cluster_characteristics(self, data: pd.DataFrame) -> Dict:
        """åˆ†æèšç±»ç‰¹å¾"""
        try:
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if 'cluster' in numeric_cols:
                numeric_cols.remove('cluster')
            
            cluster_chars = {}
            for cluster_id in data['cluster'].unique():
                cluster_data = data[data['cluster'] == cluster_id]
                
                cluster_profile = {}
                for col in numeric_cols:
                    cluster_profile[col] = {
                        'mean': float(cluster_data[col].mean()),
                        'std': float(cluster_data[col].std()),
                        'min': float(cluster_data[col].min()),
                        'max': float(cluster_data[col].max())
                    }
                
                cluster_chars[f'cluster_{cluster_id}'] = {
                    'size': int(len(cluster_data)),
                    'profile': cluster_profile
                }
            
            return cluster_chars
            
        except Exception as e:
            logger.error(f"âŒ èšç±»ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _dimension_reduction(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """æ”¹è¿›çš„é™ç»´åˆ†æ - å¢åŠ ç¼ºå¤±å€¼æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
            if np.isnan(X).any():
                logger.error("âŒ è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†'
                }
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if X.size == 0:
                logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤'
                }
            
            # æ£€æŸ¥ç‰¹å¾æ•°é‡
            if X.shape[1] < 2:
                logger.warning("âš ï¸ ç‰¹å¾æ•°é‡å°‘äº2ï¼Œæ— æ³•è¿›è¡ŒPCAé™ç»´")
                return {
                    'status': 'warning',
                    'message': 'ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡PCAé™ç»´'
                }
            
            # PCAé™ç»´
            n_components = min(3, X.shape[1])
            self.pca = PCA(n_components=n_components)
            
            try:
                X_pca = self.pca.fit_transform(X)
                
                # åˆ†æä¸»æˆåˆ†
                explained_variance = self.pca.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                
                # ç‰¹å¾é‡è¦æ€§
                feature_importance = {}
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                
                for i, component in enumerate(self.pca.components_):
                    for j, importance in enumerate(component):
                        if j < len(numeric_cols):
                            col_name = numeric_cols[j]
                            if col_name not in feature_importance:
                                feature_importance[col_name] = []
                            feature_importance[col_name].append(float(importance))
                
                dimension_insights = {
                    'n_components': int(n_components),
                    'explained_variance': explained_variance.tolist(),
                    'cumulative_variance': cumulative_variance.tolist(),
                    'feature_importance': feature_importance,
                    'pca_data': X_pca.tolist()
                }
                
                logger.info(f"âœ… é™ç»´åˆ†æå®Œæˆ: ä¿ç•™ {n_components} ä¸ªä¸»æˆåˆ†")
                logger.info(f"ğŸ“Š è§£é‡Šæ–¹å·®: {explained_variance}")
                logger.info(f"ğŸ“Š ç´¯ç§¯æ–¹å·®: {cumulative_variance}")
                
                return dimension_insights
                
            except Exception as e:
                logger.error(f"âŒ PCAè®¡ç®—å¤±è´¥: {e}")
                return {'status': 'error', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"âŒ é™ç»´åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _temporal_patterns(self, data: pd.DataFrame, target_col: str) -> Dict:
        """æ”¹è¿›çš„æ—¶é—´æ¨¡å¼åˆ†æ - å¢åŠ æ•°æ®åˆ—æ£€æŸ¥"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç›¸å…³åˆ—
            time_columns = []
            if 'Date/Time' in data.columns:
                time_columns.append('Date/Time')
            if 'Year' in data.columns:
                time_columns.append('Year')
            if 'Month' in data.columns:
                time_columns.append('Month')
            if 'Day' in data.columns:
                time_columns.append('Day')
            
            if not time_columns:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œè·³è¿‡æ—¶é—´æ¨¡å¼åˆ†æ")
                return {
                    'status': 'warning',
                    'message': 'æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œè·³è¿‡æ—¶é—´æ¨¡å¼åˆ†æ'
                }
            
            # æ£€æŸ¥ç›®æ ‡åˆ—
            if target_col not in data.columns:
                logger.warning(f"âš ï¸ ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—")
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    target_col = numeric_cols[0]
                else:
                    logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´æ¨¡å¼åˆ†æ")
                    return {
                        'status': 'error',
                        'error': 'æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´æ¨¡å¼åˆ†æ'
                    }
            
            # æ—¶é—´æ¨¡å¼åˆ†æ
            temporal_insights = {
                'time_columns_found': time_columns,
                'target_column': target_col,
                'patterns': {}
            }
            
            # å¹´åº¦æ¨¡å¼
            if 'Year' in data.columns:
                yearly_stats = data.groupby('Year')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['yearly'] = {
                    'yearly_stats': yearly_stats.to_dict('records'),
                    'yearly_trend': 'stable'  # ç®€åŒ–å¤„ç†
                }
            
            # æœˆåº¦æ¨¡å¼
            if 'Month' in data.columns:
                monthly_stats = data.groupby('Month')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['monthly'] = {
                    'monthly_stats': monthly_stats.to_dict('records'),
                    'seasonal_pattern': 'detected'  # ç®€åŒ–å¤„ç†
                }
            
            # æ—¥æ¨¡å¼
            if 'Day' in data.columns:
                daily_stats = data.groupby('Day')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['daily'] = {
                    'daily_stats': daily_stats.to_dict('records')
                }
            
            logger.info(f"âœ… æ—¶é—´æ¨¡å¼åˆ†æå®Œæˆ: åˆ†æäº† {len(time_columns)} ä¸ªæ—¶é—´åˆ—")
            return temporal_insights
            
        except Exception as e:
            logger.error(f"âŒ æ—¶é—´æ¨¡å¼åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_seasonal_trends(self, data: pd.DataFrame, target_col: str) -> Dict:
        """åˆ†æå­£èŠ‚æ€§è¶‹åŠ¿"""
        try:
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            data_sorted = data.sort_values('timestamp')
            data_sorted[f'{target_col}_ma7'] = data_sorted[target_col].rolling(window=7).mean()
            data_sorted[f'{target_col}_ma30'] = data_sorted[target_col].rolling(window=30).mean()
            
            # å­£èŠ‚æ€§ç»Ÿè®¡
            seasonal_stats = data_sorted.groupby('month')[target_col].agg(['mean', 'std', 'min', 'max']).to_dict()
            
            return {
                'seasonal_stats': seasonal_stats,
                'trend_data': {
                    'ma7': data_sorted[f'{target_col}_ma7'].dropna().tolist(),
                    'ma30': data_sorted[f'{target_col}_ma30'].dropna().tolist()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ å­£èŠ‚æ€§è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _identify_risk_mechanisms(self, data: pd.DataFrame, target_col: str) -> Dict:
        """è¯†åˆ«é£é™©æœºåˆ¶"""
        try:
            risk_mechanisms = {}
            
            # 1. æç«¯å€¼é£é™©
            if target_col in data.columns:
                target_data = data[target_col].dropna()
                q1, q3 = target_data.quantile([0.25, 0.75])
                iqr = q3 - q1
                extreme_threshold = 1.5 * iqr
                
                extreme_low = q1 - extreme_threshold
                extreme_high = q3 + extreme_threshold
                
                extreme_events = data[
                    (data[target_col] < extreme_low) | 
                    (data[target_col] > extreme_high)
                ]
                
                risk_mechanisms['extreme_values'] = {
                    'threshold_low': float(extreme_low),
                    'threshold_high': float(extreme_high),
                    'extreme_count': int(len(extreme_events)),
                    'risk_level': 'high' if len(extreme_events) > len(data) * 0.1 else 'medium'
                }
            
            # 2. æ•°æ®è´¨é‡é£é™©
            missing_rates = data.isnull().sum() / len(data)
            high_missing_features = missing_rates[missing_rates > 0.1].index.tolist()
            
            risk_mechanisms['data_quality'] = {
                'high_missing_features': high_missing_features,
                'overall_missing_rate': float(data.isnull().sum().sum() / (len(data) * len(data.columns))),
                'risk_level': 'high' if len(high_missing_features) > 0 else 'low'
            }
            
            # 3. æ—¶é—´è¿ç»­æ€§é£é™©
            if 'timestamp' in data.columns:
                data_sorted = data.sort_values('timestamp')
                time_gaps = data_sorted['timestamp'].diff().dt.total_seconds() / 3600  # å°æ—¶
                large_gaps = time_gaps[time_gaps > 24]  # è¶…è¿‡24å°æ—¶çš„é—´éš”
                
                risk_mechanisms['temporal_continuity'] = {
                    'large_gaps_count': int(len(large_gaps)),
                    'max_gap_hours': float(large_gaps.max()) if len(large_gaps) > 0 else 0,
                    'risk_level': 'high' if len(large_gaps) > 0 else 'low'
                }
            
            logger.info("âœ… é£é™©æœºåˆ¶è¯†åˆ«å®Œæˆ")
            return risk_mechanisms
            
        except Exception as e:
            logger.error(f"âŒ é£é™©æœºåˆ¶è¯†åˆ«å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _generate_summary(self) -> Dict:
        """æ”¹è¿›çš„æ‘˜è¦ç”Ÿæˆ - å¤„ç†æ–°çš„åˆ†æç»“æœæ ¼å¼"""
        try:
            summary = {
                'total_insights': 0,
                'key_findings': [],
                'risk_assessment': 'unknown',
                'recommendations': []
            }
            
            # ç»Ÿè®¡æ´å¯Ÿæ•°é‡
            insight_count = 0
            
            # å¼‚å¸¸æ£€æµ‹æ´å¯Ÿ
            if 'anomalies' in self.insights and 'anomaly_count' in self.insights['anomalies']:
                insight_count += 1
                anomaly_rate = self.insights['anomalies'].get('anomaly_rate', 0)
                if anomaly_rate > 0.1:
                    summary['key_findings'].append(f"å‘ç°å¼‚å¸¸æ•°æ®æ¯”ä¾‹è¾ƒé«˜: {anomaly_rate:.1%}")
                else:
                    summary['key_findings'].append(f"å¼‚å¸¸æ£€æµ‹æ­£å¸¸: {anomaly_rate:.1%}")
            
            # èšç±»åˆ†ææ´å¯Ÿ
            if 'clusters' in self.insights and 'optimal_clusters' in self.insights['clusters']:
                insight_count += 1
                optimal_clusters = self.insights['clusters'].get('optimal_clusters', 0)
                silhouette_score = self.insights['clusters'].get('silhouette_score', 0)
                summary['key_findings'].append(f"èšç±»åˆ†æå®Œæˆ: æœ€ä½³èšç±»æ•° {optimal_clusters}, è½®å»“ç³»æ•° {silhouette_score:.3f}")
            elif 'clusters' in self.insights and self.insights['clusters'].get('status') == 'warning':
                summary['key_findings'].append(f"èšç±»åˆ†æ: {self.insights['clusters'].get('message', 'è­¦å‘Š')}")
            
            # é™ç»´åˆ†ææ´å¯Ÿ
            if 'dimensions' in self.insights and 'n_components' in self.insights['dimensions']:
                insight_count += 1
                n_components = self.insights['dimensions'].get('n_components', 0)
                cumulative_variance = self.insights['dimensions'].get('cumulative_variance', [])
                if cumulative_variance:
                    total_variance = cumulative_variance[-1] if cumulative_variance else 0
                    summary['key_findings'].append(f"é™ç»´åˆ†æå®Œæˆ: {n_components} ä¸ªä¸»æˆåˆ†è§£é‡Š {total_variance:.1%} çš„æ–¹å·®")
            elif 'dimensions' in self.insights and self.insights['dimensions'].get('status') == 'warning':
                summary['key_findings'].append(f"é™ç»´åˆ†æ: {self.insights['dimensions'].get('message', 'è­¦å‘Š')}")
            
            # æ—¶é—´æ¨¡å¼æ´å¯Ÿ
            if 'temporal' in self.insights and 'time_columns_found' in self.insights['temporal']:
                insight_count += 1
                time_columns = self.insights['temporal'].get('time_columns_found', [])
                summary['key_findings'].append(f"æ—¶é—´æ¨¡å¼åˆ†æå®Œæˆ: åˆ†æäº† {len(time_columns)} ä¸ªæ—¶é—´ç»´åº¦")
            elif 'temporal' in self.insights and self.insights['temporal'].get('status') == 'warning':
                summary['key_findings'].append(f"æ—¶é—´æ¨¡å¼åˆ†æ: {self.insights['temporal'].get('message', 'è­¦å‘Š')}")
            
            # é£é™©æœºåˆ¶æ´å¯Ÿ
            if 'risk_mechanisms' in self.insights:
                data_quality = self.insights['risk_mechanisms'].get('data_quality', {})
                missing_rate = data_quality.get('overall_missing_rate', 0)
                risk_level = data_quality.get('risk_level', 'unknown')
                
                summary['key_findings'].append(f"æ•°æ®ç¼ºå¤±ç‡: {missing_rate:.1%}")
                summary['risk_assessment'] = risk_level
                
                if missing_rate > 0.5:
                    summary['key_findings'].append("æ•°æ®è´¨é‡é£é™©è¾ƒé«˜ï¼Œå»ºè®®æ”¹å–„æ•°æ®æ”¶é›†")
                    summary['recommendations'].append("å»ºè®®ç«‹å³æ£€æŸ¥æ•°æ®è´¨é‡å’Œå¼‚å¸¸å€¼")
                    summary['recommendations'].append("è€ƒè™‘å¢åŠ æ•°æ®éªŒè¯æœºåˆ¶")
                elif missing_rate > 0.2:
                    summary['key_findings'].append("æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦å…³æ³¨")
                    summary['recommendations'].append("å»ºè®®å®šæœŸç›‘æ§æ•°æ®è´¨é‡")
                    summary['recommendations'].append("ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹")
                else:
                    summary['key_findings'].append("æ•°æ®è´¨é‡è‰¯å¥½")
                    summary['recommendations'].append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç»§ç»­ç°æœ‰æµç¨‹")
                    summary['recommendations'].append("å»ºè®®å®šæœŸè¿›è¡Œæ¨¡å¼åˆ†æ")
            
            # é‡è¦å½±å“å› ç´ æ´å¯Ÿ
            if 'important_factors' in self.insights and 'new_discoveries' in self.insights['important_factors']:
                insight_count += 1
                new_discoveries = self.insights['important_factors']['new_discoveries']
                for discovery in new_discoveries[:2]:  # æ˜¾ç¤ºå‰2ä¸ªé‡è¦å‘ç°
                    summary['key_findings'].append(f"é‡è¦å‘ç°: {discovery}")
                
                # æ·»åŠ åŸºäºå‘ç°çš„å»ºè®®
                if any("äº¤äº’æ•ˆåº”" in discovery for discovery in new_discoveries):
                    summary['recommendations'].append("å‘ç°æ˜¾è‘—äº¤äº’æ•ˆåº”ï¼Œå»ºè®®åœ¨é¢„æµ‹æ¨¡å‹ä¸­è€ƒè™‘ç‰¹å¾äº¤äº’é¡¹")
                if any("å­£èŠ‚æ€§" in discovery for discovery in new_discoveries):
                    summary['recommendations'].append("å‘ç°å¼ºå­£èŠ‚æ€§ç‰¹å¾ï¼Œå»ºè®®å»ºç«‹å­£èŠ‚æ€§é¢„æµ‹æ¨¡å‹")
            
            # ç›¸å…³æ€§ç½‘ç»œæ´å¯Ÿ
            if 'correlation_network' in self.insights and 'central_features' in self.insights['correlation_network']:
                insight_count += 1
                central_features = self.insights['correlation_network']['central_features']
                if central_features:
                    top_central = central_features[0]
                    summary['key_findings'].append(f"ç½‘ç»œä¸­å¿ƒç‰¹å¾: {top_central['feature']} (ä¸­å¿ƒæ€§å¾—åˆ†: {top_central['centrality_score']:.3f})")
                    summary['recommendations'].append(f"é‡ç‚¹å…³æ³¨ {top_central['feature']} ä½œä¸ºå…³é”®å½±å“å› ç´ ")
            
            summary['total_insights'] = insight_count
            
            # å¦‚æœæ²¡æœ‰å…³é”®å‘ç°ï¼Œæ·»åŠ é»˜è®¤ä¿¡æ¯
            if not summary['key_findings']:
                summary['key_findings'].append("æ•°æ®æ¢ç´¢å®Œæˆï¼Œä½†å‘ç°æœ‰é™")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return {
                'status': 'error', 
                'error': str(e),
                'total_insights': 0,
                'key_findings': ['æ‘˜è¦ç”Ÿæˆå¤±è´¥'],
                'risk_assessment': 'unknown',
                'recommendations': ['è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€']
            }
    
    def _discover_important_factors(self, data: pd.DataFrame, target_col: str) -> Dict:
        """å‘ç°é‡è¦å½±å“å› ç´  - æ ¸å¿ƒåŠŸèƒ½ï¼šè§£é‡Šæ•°æ®å…³ç³»"""
        try:
            logger.info("ğŸ” å¼€å§‹é‡è¦å½±å“å› ç´ å‘ç°...")
            
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if target_col in numeric_cols:
                numeric_cols.remove(target_col)
            
            # ç§»é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„ç‰¹å¾
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            
            if len(valid_features) == 0:
                return {'status': 'warning', 'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆç‰¹å¾è¿›è¡Œå½±å“å› ç´ åˆ†æ'}
            
            # å‡†å¤‡æ•°æ®
            X = data[valid_features].fillna(data[valid_features].median())
            y = data[target_col].fillna(data[target_col].median()) if target_col in data.columns else None
            
            factor_insights = {
                'total_features_analyzed': len(valid_features),
                'feature_importance': {},
                'correlation_analysis': {},
                'interaction_effects': {},
                'seasonal_factors': {},
                'new_discoveries': []
            }
            
            # 1. ç‰¹å¾é‡è¦æ€§åˆ†æ (åŸºäºæ–¹å·®å’Œç›¸å…³æ€§)
            feature_importance = {}
            for col in valid_features:
                if col in X.columns:
                    # è®¡ç®—æ–¹å·® (é«˜æ–¹å·® = é«˜å½±å“æ½œåŠ›)
                    variance = X[col].var()
                    
                    # è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§
                    if y is not None:
                        correlation = X[col].corr(y)
                    else:
                        correlation = 0
                    
                    # è®¡ç®—å˜å¼‚ç³»æ•° (ç¨³å®šæ€§æŒ‡æ ‡)
                    cv = X[col].std() / X[col].mean() if X[col].mean() != 0 else 0
                    
                    feature_importance[col] = {
                        'variance': float(variance),
                        'correlation_with_target': float(correlation) if not pd.isna(correlation) else 0,
                        'coefficient_of_variation': float(cv),
                        'importance_score': float(abs(correlation) * variance) if not pd.isna(correlation) else 0
                    }
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_features = sorted(feature_importance.items(), 
                                   key=lambda x: x[1]['importance_score'], reverse=True)
            
            factor_insights['feature_importance'] = dict(sorted_features)
            
            # 2. ç›¸å…³æ€§ç½‘ç»œåˆ†æ
            correlation_matrix = X.corr()
            strong_correlations = []
            
            for i, col1 in enumerate(valid_features):
                for j, col2 in enumerate(valid_features[i+1:], i+1):
                    corr_value = correlation_matrix.loc[col1, col2]
                    if abs(corr_value) > 0.7:  # å¼ºç›¸å…³
                        strong_correlations.append({
                            'feature1': col1,
                            'feature2': col2,
                            'correlation': float(corr_value),
                            'strength': 'strong' if abs(corr_value) > 0.8 else 'moderate'
                        })
            
            factor_insights['correlation_analysis'] = {
                'strong_correlations': strong_correlations,
                'correlation_matrix': correlation_matrix.to_dict()
            }
            
            # 3. äº¤äº’æ•ˆåº”å‘ç°
            interaction_effects = []
            top_features = [f[0] for f in sorted_features[:5]]  # å‰5ä¸ªé‡è¦ç‰¹å¾
            
            for i, feat1 in enumerate(top_features):
                for feat2 in top_features[i+1:]:
                    if feat1 in X.columns and feat2 in X.columns:
                        # è®¡ç®—äº¤äº’é¡¹
                        interaction = X[feat1] * X[feat2]
                        if y is not None:
                            interaction_corr = interaction.corr(y)
                            if abs(interaction_corr) > 0.3:  # æ˜¾è‘—äº¤äº’æ•ˆåº”
                                interaction_effects.append({
                                    'feature1': feat1,
                                    'feature2': feat2,
                                    'interaction_correlation': float(interaction_corr),
                                    'interpretation': f"{feat1} å’Œ {feat2} çš„äº¤äº’æ•ˆåº”æ˜¾è‘—"
                                })
            
            factor_insights['interaction_effects'] = interaction_effects
            
            # 4. å­£èŠ‚æ€§å› ç´ åˆ†æ
            seasonal_factors = {}
            if 'Month' in data.columns:
                monthly_stats = data.groupby('Month')[valid_features].mean()
                seasonal_variation = monthly_stats.std() / monthly_stats.mean()
                
                seasonal_factors = {
                    'monthly_variation': seasonal_variation.to_dict(),
                    'most_seasonal_features': seasonal_variation.nlargest(3).to_dict()
                }
            
            factor_insights['seasonal_factors'] = seasonal_factors
            
            # 5. æ–°å‘ç°æ€»ç»“
            new_discoveries = []
            
            # å‘ç°æœ€é‡è¦çš„å½±å“å› ç´ 
            if sorted_features:
                top_factor = sorted_features[0]
                new_discoveries.append(f"æœ€é‡è¦çš„å½±å“å› ç´ : {top_factor[0]} (é‡è¦æ€§å¾—åˆ†: {top_factor[1]['importance_score']:.3f})")
            
            # å‘ç°å¼ºç›¸å…³ç‰¹å¾å¯¹
            if strong_correlations:
                strongest_corr = max(strong_correlations, key=lambda x: abs(x['correlation']))
                new_discoveries.append(f"æœ€å¼ºç›¸å…³ç‰¹å¾å¯¹: {strongest_corr['feature1']} â†” {strongest_corr['feature2']} (ç›¸å…³ç³»æ•°: {strongest_corr['correlation']:.3f})")
            
            # å‘ç°æ˜¾è‘—äº¤äº’æ•ˆåº”
            if interaction_effects:
                strongest_interaction = max(interaction_effects, key=lambda x: abs(x['interaction_correlation']))
                new_discoveries.append(f"æ˜¾è‘—äº¤äº’æ•ˆåº”: {strongest_interaction['feature1']} Ã— {strongest_interaction['feature2']} (äº¤äº’ç›¸å…³ç³»æ•°: {strongest_interaction['interaction_correlation']:.3f})")
            
            # å‘ç°å­£èŠ‚æ€§ç‰¹å¾
            if seasonal_factors and 'most_seasonal_features' in seasonal_factors:
                most_seasonal = max(seasonal_factors['most_seasonal_features'].items(), key=lambda x: x[1])
                new_discoveries.append(f"æœ€å¼ºå­£èŠ‚æ€§ç‰¹å¾: {most_seasonal[0]} (å­£èŠ‚æ€§å˜å¼‚ç³»æ•°: {most_seasonal[1]:.3f})")
            
            factor_insights['new_discoveries'] = new_discoveries
            
            logger.info(f"âœ… é‡è¦å½±å“å› ç´ å‘ç°å®Œæˆ: åˆ†æäº† {len(valid_features)} ä¸ªç‰¹å¾")
            logger.info(f"ğŸ” å‘ç° {len(new_discoveries)} ä¸ªé‡è¦æ´å¯Ÿ")
            
            return factor_insights
            
        except Exception as e:
            logger.error(f"âŒ é‡è¦å½±å“å› ç´ å‘ç°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_correlation_network(self, data: pd.DataFrame, target_col: str) -> Dict:
        """ç›¸å…³æ€§ç½‘ç»œåˆ†æ - å‘ç°ç‰¹å¾é—´çš„å¤æ‚å…³ç³»"""
        try:
            logger.info("ğŸ” å¼€å§‹ç›¸å…³æ€§ç½‘ç»œåˆ†æ...")
            
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if target_col in numeric_cols:
                numeric_cols.remove(target_col)
            
            # ç§»é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„ç‰¹å¾
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            
            if len(valid_features) < 3:
                return {'status': 'warning', 'message': 'ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç½‘ç»œåˆ†æ'}
            
            # å‡†å¤‡æ•°æ® - ç¡®ä¿æ²¡æœ‰NaNå€¼
            X = data[valid_features].fillna(data[valid_features].median())
            
            # å†æ¬¡æ£€æŸ¥å¹¶å¤„ç†ä»»ä½•å‰©ä½™çš„NaNå€¼
            X = X.fillna(0)
            
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            correlation_matrix = X.corr()
            
            network_insights = {
                'network_statistics': {},
                'central_features': [],
                'feature_clusters': [],
                'influence_paths': [],
                'network_visualization': {}
            }
            
            # 1. ç½‘ç»œç»Ÿè®¡
            total_connections = 0
            strong_connections = 0
            moderate_connections = 0
            
            for i, col1 in enumerate(valid_features):
                for j, col2 in enumerate(valid_features[i+1:], i+1):
                    corr_value = abs(correlation_matrix.loc[col1, col2])
                    if corr_value > 0.3:  # æœ‰æ„ä¹‰çš„è¿æ¥
                        total_connections += 1
                        if corr_value > 0.7:
                            strong_connections += 1
                        elif corr_value > 0.5:
                            moderate_connections += 1
            
            network_insights['network_statistics'] = {
                'total_features': len(valid_features),
                'total_connections': total_connections,
                'strong_connections': strong_connections,
                'moderate_connections': moderate_connections,
                'network_density': total_connections / (len(valid_features) * (len(valid_features) - 1) / 2)
            }
            
            # 2. ä¸­å¿ƒæ€§ç‰¹å¾ (ä¸å…¶ä»–ç‰¹å¾ç›¸å…³æ€§æœ€å¤šçš„ç‰¹å¾)
            centrality_scores = {}
            for col in valid_features:
                connections = 0
                total_corr = 0
                for other_col in valid_features:
                    if col != other_col:
                        corr_value = abs(correlation_matrix.loc[col, other_col])
                        if corr_value > 0.3:
                            connections += 1
                            total_corr += corr_value
                
                centrality_scores[col] = {
                    'connection_count': connections,
                    'average_correlation': total_corr / connections if connections > 0 else 0,
                    'centrality_score': connections * (total_corr / connections if connections > 0 else 0)
                }
            
            # æŒ‰ä¸­å¿ƒæ€§æ’åº
            central_features = sorted(centrality_scores.items(), 
                                    key=lambda x: x[1]['centrality_score'], reverse=True)[:5]
            
            network_insights['central_features'] = [
                {
                    'feature': feat,
                    'centrality_score': score['centrality_score'],
                    'connection_count': score['connection_count'],
                    'average_correlation': score['average_correlation']
                }
                for feat, score in central_features
            ]
            
            # 3. ç‰¹å¾èšç±» (åŸºäºç›¸å…³æ€§)
            from sklearn.cluster import AgglomerativeClustering
            
            # ä½¿ç”¨1-|correlation|ä½œä¸ºè·ç¦»
            distance_matrix = 1 - abs(correlation_matrix)
            
            # èšç±»
            clustering = AgglomerativeClustering(n_clusters=min(3, len(valid_features)//2), 
                                               metric='precomputed', linkage='average')
            cluster_labels = clustering.fit_predict(distance_matrix)
            
            # ç»„ç»‡èšç±»ç»“æœ
            feature_clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in feature_clusters:
                    feature_clusters[label] = []
                feature_clusters[label].append(valid_features[i])
            
            network_insights['feature_clusters'] = [
                {
                    'cluster_id': cluster_id,
                    'features': features,
                    'cluster_size': len(features),
                    'intra_cluster_correlation': self._calculate_intra_cluster_correlation(features, correlation_matrix)
                }
                for cluster_id, features in feature_clusters.items()
            ]
            
            # 4. å½±å“è·¯å¾„åˆ†æ
            influence_paths = []
            for central_feat, _ in central_features[:3]:  # å‰3ä¸ªä¸­å¿ƒç‰¹å¾
                paths = self._find_influence_paths(central_feat, valid_features, correlation_matrix)
                influence_paths.extend(paths)
            
            network_insights['influence_paths'] = influence_paths
            
            # 5. ç½‘ç»œå¯è§†åŒ–æ•°æ®
            network_insights['network_visualization'] = {
                'nodes': [
                    {
                        'id': feat,
                        'centrality': centrality_scores[feat]['centrality_score'],
                        'cluster': cluster_labels[valid_features.index(feat)]
                    }
                    for feat in valid_features
                ],
                'edges': [
                    {
                        'source': valid_features[i],
                        'target': valid_features[j],
                        'weight': abs(correlation_matrix.loc[valid_features[i], valid_features[j]]),
                        'correlation': correlation_matrix.loc[valid_features[i], valid_features[j]]
                    }
                    for i in range(len(valid_features))
                    for j in range(i+1, len(valid_features))
                    if abs(correlation_matrix.loc[valid_features[i], valid_features[j]]) > 0.3
                ]
            }
            
            logger.info(f"âœ… ç›¸å…³æ€§ç½‘ç»œåˆ†æå®Œæˆ: {len(valid_features)} ä¸ªç‰¹å¾, {total_connections} ä¸ªè¿æ¥")
            
            return network_insights
            
        except Exception as e:
            logger.error(f"âŒ ç›¸å…³æ€§ç½‘ç»œåˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _calculate_intra_cluster_correlation(self, features: List[str], correlation_matrix: pd.DataFrame) -> float:
        """è®¡ç®—èšç±»å†…å¹³å‡ç›¸å…³æ€§"""
        if len(features) < 2:
            return 0.0
        
        total_corr = 0
        count = 0
        for i, feat1 in enumerate(features):
            for feat2 in features[i+1:]:
                if feat1 in correlation_matrix.columns and feat2 in correlation_matrix.columns:
                    total_corr += abs(correlation_matrix.loc[feat1, feat2])
                    count += 1
        
        return total_corr / count if count > 0 else 0.0
    
    def _find_influence_paths(self, central_feature: str, all_features: List[str], 
                            correlation_matrix: pd.DataFrame, max_depth: int = 2) -> List[Dict]:
        """å‘ç°å½±å“è·¯å¾„"""
        paths = []
        
        # æ‰¾åˆ°ä¸ä¸­å¿ƒç‰¹å¾å¼ºç›¸å…³çš„ç‰¹å¾
        strong_connections = []
        for feat in all_features:
            if feat != central_feature:
                corr = abs(correlation_matrix.loc[central_feature, feat])
                if corr > 0.5:
                    strong_connections.append((feat, corr))
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        strong_connections.sort(key=lambda x: x[1], reverse=True)
        
        # æ„å»ºå½±å“è·¯å¾„
        for connected_feat, corr in strong_connections[:3]:  # å‰3ä¸ªå¼ºè¿æ¥
            paths.append({
                'central_feature': central_feature,
                'connected_feature': connected_feat,
                'correlation_strength': float(corr),
                'path_type': 'direct_influence',
                'interpretation': f"{central_feature} ç›´æ¥å½±å“ {connected_feat} (ç›¸å…³ç³»æ•°: {corr:.3f})"
            })
        
        return paths
    
    def save_insights(self, output_dir: str = "insights") -> str:
        """ä¿å­˜æ´å¯Ÿç»“æœ"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            insights_file = os.path.join(output_dir, f"insights_discovery_{timestamp}.json")
            
            with open(insights_file, 'w', encoding='utf-8') as f:
                json.dump(self.insights, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… æ´å¯Ÿç»“æœå·²ä¿å­˜: {insights_file}")
            return insights_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ´å¯Ÿç»“æœå¤±è´¥: {e}")
            return ""

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æ— ç›‘ç£æ¢ç´¢æ¨¡å—...")
        
        # åŠ è½½Environment Canadaæ•°æ®
        data_path = "data/real/environment_canada/environment_canada_merged.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return
        
        # è¯»å–æ•°æ®
        data = pd.read_csv(data_path)
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {data.shape}")
        
        # ä¼°ç®—åœŸå£¤æ¹¿åº¦
        if 'Total Precip (mm)' in data.columns and 'Temp (Â°C)' in data.columns:
            # ç®€å•çš„åœŸå£¤æ¹¿åº¦ä¼°ç®—
            data['estimated_soil_moisture'] = (
                0.3 +  # åŸºç¡€æ¹¿åº¦
                0.1 * np.log1p(data['Total Precip (mm)'].fillna(0)) +  # é™æ°´å½±å“
                0.05 * (1 - (data['Temp (Â°C)'].fillna(0) + 20) / 60)  # æ¸©åº¦å½±å“
            )
            data['estimated_soil_moisture'] = np.clip(data['estimated_soil_moisture'], 0.1, 0.9)
        
        # åˆ›å»ºæ¢ç´¢æ¨¡å—
        explorer = InsightDiscoveryModule()
        
        # å‘ç°æ¨¡å¼
        insights = explorer.discover_patterns(data)
        
        if 'status' not in insights:
            # ä¿å­˜æ´å¯Ÿç»“æœ
            insights_file = explorer.save_insights()
            
            logger.info("ğŸ‰ æ— ç›‘ç£æ¢ç´¢å®Œæˆï¼")
            logger.info(f"ğŸ“Š å‘ç° {insights['summary']['total_insights']} ç±»æ´å¯Ÿ")
            logger.info(f"âš ï¸ é£é™©è¯„ä¼°: {insights['summary']['risk_assessment']}")
            
            # æ˜¾ç¤ºå…³é”®å‘ç°
            for finding in insights['summary']['key_findings']:
                logger.info(f"ğŸ” {finding}")
            
            # æ˜¾ç¤ºå»ºè®®
            for rec in insights['summary']['recommendations']:
                logger.info(f"ğŸ’¡ {rec}")
            
            return insights
        else:
            logger.error(f"âŒ æ¢ç´¢å¤±è´¥: {insights}")
            return None
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()
