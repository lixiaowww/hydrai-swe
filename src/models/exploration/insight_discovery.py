#!/usr/bin/env python3
"""
æ— ç›‘ç£æ¢ç´¢æ¨¡å— - å‘ç°é—®é¢˜èƒŒåçš„æ¨¡å¼
å®šä½ï¼šæ¢ç´¢ + è§£é‡Šï¼Œè¡¥å……é¢„æµ‹çš„å¯ä¿¡åº¦å’Œç†è§£
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import logging
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    # åœ¨loggerå®šä¹‰ä¹‹å‰ä¸èƒ½ä½¿ç”¨logger
    PLOTTING_AVAILABLE = False
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class InsightDiscoveryModule:
    """æ— ç›‘ç£æ¢ç´¢æ¨¡å— - å‘ç°é—®é¢˜èƒŒåçš„æ¨¡å¼"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¢ç´¢æ¨¡å—"""
        self.scaler = StandardScaler()
        self.pca = None
        self.clusters = None
        self.anomalies = None
        self.insights = {}
        
        logger.info("ğŸ” æ— ç›‘ç£æ¢ç´¢æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def discover_patterns(self, data: pd.DataFrame, target_col: str = 'estimated_soil_moisture') -> Dict:
        """å‘ç°æ•°æ®èƒŒåçš„æ¨¡å¼"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ— ç›‘ç£æ¨¡å¼å‘ç°...")
            
            # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†
            logger.info("ğŸ”§ æ­¥éª¤1: æ•°æ®é¢„å¤„ç†...")
            X_processed = self._preprocess_data(data)
            
            # æ­¥éª¤2: å¼‚å¸¸æ£€æµ‹
            logger.info("ğŸ” æ­¥éª¤2: å¼‚å¸¸æ£€æµ‹...")
            anomaly_insights = self._detect_anomalies(X_processed, data)
            
            # æ­¥éª¤3: èšç±»åˆ†æ
            logger.info("ğŸ” æ­¥éª¤3: èšç±»åˆ†æ...")
            cluster_insights = self._cluster_analysis(X_processed, data)
            
            # æ­¥éª¤4: é™ç»´å¯è§†åŒ–
            logger.info("ğŸ” æ­¥éª¤4: é™ç»´å¯è§†åŒ–...")
            dimension_insights = self._dimension_reduction(X_processed, data)
            
            # æ­¥éª¤5: æ—¶é—´æ¨¡å¼åˆ†æ
            logger.info("ğŸ” æ­¥éª¤5: æ—¶é—´æ¨¡å¼åˆ†æ...")
            temporal_insights = self._temporal_patterns(data, target_col)
            
            # æ­¥éª¤6: é£é™©æœºåˆ¶è¯†åˆ«
            logger.info("ğŸ” æ­¥éª¤6: é£é™©æœºåˆ¶è¯†åˆ«...")
            risk_insights = self._identify_risk_mechanisms(data, target_col)
            
            # æ­¥éª¤7: é‡è¦å½±å“å› ç´ å‘ç°
            logger.info("ğŸ” æ­¥éª¤7: é‡è¦å½±å“å› ç´ å‘ç°...")
            factor_insights = self._discover_important_factors(data, target_col)
            
            # æ­¥éª¤8: ç›¸å…³æ€§ç½‘ç»œåˆ†æ
            logger.info("ğŸ” æ­¥éª¤8: ç›¸å…³æ€§ç½‘ç»œåˆ†æ...")
            correlation_insights = self._analyze_correlation_network(data, target_col)
            
            # æ­¥éª¤9: SWEå†·é—¨å› ç´ å‘ç°
            logger.info("ğŸ” æ­¥éª¤9: SWEå†·é—¨å› ç´ å‘ç°...")
            swe_cold_factors = self._discover_swe_cold_factors(data, target_col)
            
            # æ•´åˆæ‰€æœ‰æ´å¯Ÿ
            self.insights = {
                'timestamp': datetime.now().isoformat(),
                'anomalies': anomaly_insights,
                'clusters': cluster_insights,
                'dimensions': dimension_insights,
                'temporal': temporal_insights,
                'risk_mechanisms': risk_insights,
                'important_factors': factor_insights,
                'correlation_network': correlation_insights,
                'swe_cold_factors': swe_cold_factors
            }
            
            # æ­¥éª¤10: ç”Ÿæˆæ‘˜è¦ (åœ¨æ‰€æœ‰æ´å¯Ÿæ„å»ºå®Œæˆå)
            logger.info("ğŸ” æ­¥éª¤10: ç”Ÿæˆæ‘˜è¦...")
            summary_insights = self._generate_summary()
            self.insights['summary'] = summary_insights
            
            logger.info("ğŸ‰ æ— ç›‘ç£æ¨¡å¼å‘ç°å®Œæˆï¼")
            return self.insights
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å¼å‘ç°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _preprocess_data(self, data: pd.DataFrame) -> np.ndarray:
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç† - ä¸“é—¨å¤„ç†é«˜ç¼ºå¤±ç‡æ•°æ®"""
        try:
            # é€‰æ‹©æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            
            # ç§»é™¤ç›®æ ‡å˜é‡
            if 'estimated_soil_moisture' in numeric_cols:
                numeric_cols.remove('estimated_soil_moisture')
            
            # åˆ†æç¼ºå¤±å€¼æ¨¡å¼
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            logger.info(f"ğŸ“Š ç¼ºå¤±å€¼åˆ†æ: æ€»ç‰¹å¾æ•° {len(numeric_cols)}")
            
            # åªä¿ç•™ç¼ºå¤±ç‡ < 50% çš„ç‰¹å¾
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            high_missing_features = missing_rates[missing_rates >= 0.5].index.tolist()
            
            logger.info(f"âœ… æœ‰æ•ˆç‰¹å¾: {len(valid_features)} ä¸ª (ç¼ºå¤±ç‡ < 50%)")
            logger.info(f"âš ï¸ é«˜ç¼ºå¤±ç‰¹å¾: {len(high_missing_features)} ä¸ª (ç¼ºå¤±ç‡ >= 50%)")
            
            if len(valid_features) == 0:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œå°è¯•æ”¾å®½æ ‡å‡†åˆ°80%ç¼ºå¤±ç‡")
                valid_features = missing_rates[missing_rates < 0.8].index.tolist()
                if len(valid_features) == 0:
                    logger.error("âŒ æ‰€æœ‰ç‰¹å¾ç¼ºå¤±ç‡éƒ½è¿‡é«˜ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
                    return np.array([])
            
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„ç¼ºå¤±å€¼å¡«å……
            X = data[valid_features].copy()
            
            # å¯¹äºè¿ç»­å˜é‡ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    median_val = X[col].median()
                    if pd.isna(median_val):
                        # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼Œä½¿ç”¨0å¡«å……
                        X[col] = X[col].fillna(0)
                    else:
                        X[col] = X[col].fillna(median_val)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNå€¼
            remaining_nans = X.isnull().sum().sum()
            if remaining_nans > 0:
                logger.warning(f"âš ï¸ ä»æœ‰ {remaining_nans} ä¸ªNaNå€¼ï¼Œä½¿ç”¨0å¡«å……")
                X = X.fillna(0)
            
            # æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            logger.info(f"âœ… æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å®Œæˆ: {X.shape[1]} ä¸ªæœ‰æ•ˆç‰¹å¾")
            logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X_scaled.shape}")
            
            return X_scaled
            
        except Exception as e:
            logger.error(f"âŒ æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return np.array([])
    
    def _detect_anomalies(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """å¼‚å¸¸æ£€æµ‹"""
        try:
            # ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(X)
            
            # ç»Ÿè®¡å¼‚å¸¸
            anomaly_count = np.sum(anomaly_labels == -1)
            total_count = len(anomaly_labels)
            anomaly_rate = anomaly_count / total_count
            
            # åˆ†æå¼‚å¸¸ç‰¹å¾
            anomaly_data = data[anomaly_labels == -1]
            normal_data = data[anomaly_labels == 1]
            
            anomaly_insights = {
                'anomaly_count': int(anomaly_count),
                'anomaly_rate': float(anomaly_rate),
                'total_count': int(total_count),
                'anomaly_features': self._analyze_anomaly_features(anomaly_data, normal_data),
                'anomaly_timestamps': anomaly_data['timestamp'].tolist() if 'timestamp' in anomaly_data.columns else []
            }
            
            logger.info(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {anomaly_count} ä¸ªå¼‚å¸¸ ({anomaly_rate:.1%})")
            return anomaly_insights
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_anomaly_features(self, anomaly_data: pd.DataFrame, normal_data: pd.DataFrame) -> Dict:
        """åˆ†æå¼‚å¸¸ç‰¹å¾"""
        try:
            numeric_cols = anomaly_data.select_dtypes(include=[np.number]).columns.tolist()
            
            feature_analysis = {}
            for col in numeric_cols:
                if col in normal_data.columns:
                    anomaly_mean = anomaly_data[col].mean()
                    normal_mean = normal_data[col].mean()
                    difference = anomaly_mean - normal_mean
                    
                    feature_analysis[col] = {
                        'anomaly_mean': float(anomaly_mean),
                        'normal_mean': float(normal_mean),
                        'difference': float(difference),
                        'deviation': float(abs(difference) / normal_mean) if normal_mean != 0 else 0
                    }
            
            return feature_analysis
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _cluster_analysis(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """æ”¹è¿›çš„èšç±»åˆ†æ - å¢åŠ ç¼ºå¤±å€¼æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
            if np.isnan(X).any():
                logger.error("âŒ è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†'
                }
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if X.size == 0:
                logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤'
                }
            
            # ç¡®å®šæœ€ä½³èšç±»æ•°
            silhouette_scores = []
            K_range = range(2, min(11, len(X) // 10 + 1))
            
            if len(K_range) == 0:
                logger.warning("âš ï¸ æ•°æ®é‡è¿‡å°‘ï¼Œä½¿ç”¨é»˜è®¤èšç±»æ•°2")
                K_range = [2]
            
            for k in K_range:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(X)
                    score = silhouette_score(X, cluster_labels)
                    silhouette_scores.append(score)
                except Exception as e:
                    logger.warning(f"âš ï¸ èšç±»æ•° {k} å¤±è´¥: {e}")
                    silhouette_scores.append(-1)
            
            if not silhouette_scores or max(silhouette_scores) == -1:
                logger.warning("âš ï¸ æ‰€æœ‰èšç±»æ•°éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤èšç±»æ•°2")
                best_k = 2
                best_score = 0
            else:
                best_k = K_range[np.argmax(silhouette_scores)]
                best_score = max(silhouette_scores)
            
            # æ‰§è¡Œæœ€ä½³èšç±»
            try:
                kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(X)
                
                # åˆ†æèšç±»ç‰¹å¾
                data_with_clusters = data.copy()
                data_with_clusters['cluster'] = cluster_labels
                
                cluster_insights = {
                    'optimal_clusters': int(best_k),
                    'silhouette_score': float(best_score),
                    'cluster_sizes': data_with_clusters['cluster'].value_counts().to_dict(),
                    'cluster_characteristics': self._analyze_cluster_characteristics(data_with_clusters)
                }
                
                self.clusters = cluster_labels
                
                logger.info(f"âœ… èšç±»åˆ†æå®Œæˆ: æœ€ä½³èšç±»æ•° {best_k}, è½®å»“ç³»æ•° {best_score:.3f}")
                return cluster_insights
                
            except Exception as e:
                logger.error(f"âŒ æœ€ç»ˆèšç±»å¤±è´¥: {e}")
                return {'status': 'error', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"âŒ èšç±»åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_cluster_characteristics(self, data: pd.DataFrame) -> Dict:
        """åˆ†æèšç±»ç‰¹å¾"""
        try:
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if 'cluster' in numeric_cols:
                numeric_cols.remove('cluster')
            
            cluster_chars = {}
            for cluster_id in data['cluster'].unique():
                cluster_data = data[data['cluster'] == cluster_id]
                
                cluster_profile = {}
                for col in numeric_cols:
                    cluster_profile[col] = {
                        'mean': float(cluster_data[col].mean()),
                        'std': float(cluster_data[col].std()),
                        'min': float(cluster_data[col].min()),
                        'max': float(cluster_data[col].max())
                    }
                
                cluster_chars[f'cluster_{cluster_id}'] = {
                    'size': int(len(cluster_data)),
                    'profile': cluster_profile
                }
            
            return cluster_chars
            
        except Exception as e:
            logger.error(f"âŒ èšç±»ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _dimension_reduction(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """æ”¹è¿›çš„é™ç»´åˆ†æ - å¢åŠ ç¼ºå¤±å€¼æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
            if np.isnan(X).any():
                logger.error("âŒ è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†'
                }
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if X.size == 0:
                logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤'
                }
            
            # æ£€æŸ¥ç‰¹å¾æ•°é‡
            if X.shape[1] < 2:
                logger.warning("âš ï¸ ç‰¹å¾æ•°é‡å°‘äº2ï¼Œæ— æ³•è¿›è¡ŒPCAé™ç»´")
                return {
                    'status': 'warning',
                    'message': 'ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡PCAé™ç»´'
                }
            
            # PCAé™ç»´
            n_components = min(3, X.shape[1])
            self.pca = PCA(n_components=n_components)
            
            try:
                X_pca = self.pca.fit_transform(X)
                
                # åˆ†æä¸»æˆåˆ†
                explained_variance = self.pca.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                
                # ç‰¹å¾é‡è¦æ€§
                feature_importance = {}
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                
                for i, component in enumerate(self.pca.components_):
                    for j, importance in enumerate(component):
                        if j < len(numeric_cols):
                            col_name = numeric_cols[j]
                            if col_name not in feature_importance:
                                feature_importance[col_name] = []
                            feature_importance[col_name].append(float(importance))
                
                dimension_insights = {
                    'n_components': int(n_components),
                    'explained_variance': explained_variance.tolist(),
                    'cumulative_variance': cumulative_variance.tolist(),
                    'feature_importance': feature_importance,
                    'pca_data': X_pca.tolist()
                }
                
                logger.info(f"âœ… é™ç»´åˆ†æå®Œæˆ: ä¿ç•™ {n_components} ä¸ªä¸»æˆåˆ†")
                logger.info(f"ğŸ“Š è§£é‡Šæ–¹å·®: {explained_variance}")
                logger.info(f"ğŸ“Š ç´¯ç§¯æ–¹å·®: {cumulative_variance}")
                
                return dimension_insights
                
            except Exception as e:
                logger.error(f"âŒ PCAè®¡ç®—å¤±è´¥: {e}")
                return {'status': 'error', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"âŒ é™ç»´åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _temporal_patterns(self, data: pd.DataFrame, target_col: str) -> Dict:
        """æ”¹è¿›çš„æ—¶é—´æ¨¡å¼åˆ†æ - å¢åŠ æ•°æ®åˆ—æ£€æŸ¥"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç›¸å…³åˆ—
            time_columns = []
            if 'Date/Time' in data.columns:
                time_columns.append('Date/Time')
            if 'Year' in data.columns:
                time_columns.append('Year')
            if 'Month' in data.columns:
                time_columns.append('Month')
            if 'Day' in data.columns:
                time_columns.append('Day')
            
            if not time_columns:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œè·³è¿‡æ—¶é—´æ¨¡å¼åˆ†æ")
                return {
                    'status': 'warning',
                    'message': 'æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œè·³è¿‡æ—¶é—´æ¨¡å¼åˆ†æ'
                }
            
            # æ£€æŸ¥ç›®æ ‡åˆ—
            if target_col not in data.columns:
                logger.warning(f"âš ï¸ ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—")
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    target_col = numeric_cols[0]
                else:
                    logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´æ¨¡å¼åˆ†æ")
                    return {
                        'status': 'error',
                        'error': 'æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´æ¨¡å¼åˆ†æ'
                    }
            
            # æ—¶é—´æ¨¡å¼åˆ†æ
            temporal_insights = {
                'time_columns_found': time_columns,
                'target_column': target_col,
                'patterns': {}
            }
            
            # å¹´åº¦æ¨¡å¼
            if 'Year' in data.columns:
                yearly_stats = data.groupby('Year')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['yearly'] = {
                    'yearly_stats': yearly_stats.to_dict('records'),
                    'yearly_trend': 'stable'  # ç®€åŒ–å¤„ç†
                }
            
            # æœˆåº¦æ¨¡å¼
            if 'Month' in data.columns:
                monthly_stats = data.groupby('Month')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['monthly'] = {
                    'monthly_stats': monthly_stats.to_dict('records'),
                    'seasonal_pattern': 'detected'  # ç®€åŒ–å¤„ç†
                }
            
            # æ—¥æ¨¡å¼
            if 'Day' in data.columns:
                daily_stats = data.groupby('Day')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['daily'] = {
                    'daily_stats': daily_stats.to_dict('records')
                }
            
            logger.info(f"âœ… æ—¶é—´æ¨¡å¼åˆ†æå®Œæˆ: åˆ†æäº† {len(time_columns)} ä¸ªæ—¶é—´åˆ—")
            return temporal_insights
            
        except Exception as e:
            logger.error(f"âŒ æ—¶é—´æ¨¡å¼åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_seasonal_trends(self, data: pd.DataFrame, target_col: str) -> Dict:
        """åˆ†æå­£èŠ‚æ€§è¶‹åŠ¿"""
        try:
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            data_sorted = data.sort_values('timestamp')
            data_sorted[f'{target_col}_ma7'] = data_sorted[target_col].rolling(window=7).mean()
            data_sorted[f'{target_col}_ma30'] = data_sorted[target_col].rolling(window=30).mean()
            
            # å­£èŠ‚æ€§ç»Ÿè®¡
            seasonal_stats = data_sorted.groupby('month')[target_col].agg(['mean', 'std', 'min', 'max']).to_dict()
            
            return {
                'seasonal_stats': seasonal_stats,
                'trend_data': {
                    'ma7': data_sorted[f'{target_col}_ma7'].dropna().tolist(),
                    'ma30': data_sorted[f'{target_col}_ma30'].dropna().tolist()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ å­£èŠ‚æ€§è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _identify_risk_mechanisms(self, data: pd.DataFrame, target_col: str) -> Dict:
        """è¯†åˆ«é£é™©æœºåˆ¶"""
        try:
            risk_mechanisms = {}
            
            # 1. æç«¯å€¼é£é™©
            if target_col in data.columns:
                target_data = data[target_col].dropna()
                q1, q3 = target_data.quantile([0.25, 0.75])
                iqr = q3 - q1
                extreme_threshold = 1.5 * iqr
                
                extreme_low = q1 - extreme_threshold
                extreme_high = q3 + extreme_threshold
                
                extreme_events = data[
                    (data[target_col] < extreme_low) | 
                    (data[target_col] > extreme_high)
                ]
                
                risk_mechanisms['extreme_values'] = {
                    'threshold_low': float(extreme_low),
                    'threshold_high': float(extreme_high),
                    'extreme_count': int(len(extreme_events)),
                    'risk_level': 'high' if len(extreme_events) > len(data) * 0.1 else 'medium'
                }
            
            # 2. æ•°æ®è´¨é‡é£é™©
            missing_rates = data.isnull().sum() / len(data)
            high_missing_features = missing_rates[missing_rates > 0.1].index.tolist()
            
            risk_mechanisms['data_quality'] = {
                'high_missing_features': high_missing_features,
                'overall_missing_rate': float(data.isnull().sum().sum() / (len(data) * len(data.columns))),
                'risk_level': 'high' if len(high_missing_features) > 0 else 'low'
            }
            
            # 3. æ—¶é—´è¿ç»­æ€§é£é™©
            if 'timestamp' in data.columns:
                data_sorted = data.sort_values('timestamp')
                time_gaps = data_sorted['timestamp'].diff().dt.total_seconds() / 3600  # å°æ—¶
                large_gaps = time_gaps[time_gaps > 24]  # è¶…è¿‡24å°æ—¶çš„é—´éš”
                
                risk_mechanisms['temporal_continuity'] = {
                    'large_gaps_count': int(len(large_gaps)),
                    'max_gap_hours': float(large_gaps.max()) if len(large_gaps) > 0 else 0,
                    'risk_level': 'high' if len(large_gaps) > 0 else 'low'
                }
            
            logger.info("âœ… é£é™©æœºåˆ¶è¯†åˆ«å®Œæˆ")
            return risk_mechanisms
            
        except Exception as e:
            logger.error(f"âŒ é£é™©æœºåˆ¶è¯†åˆ«å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _generate_summary(self) -> Dict:
        """æ”¹è¿›çš„æ‘˜è¦ç”Ÿæˆ - å¤„ç†æ–°çš„åˆ†æç»“æœæ ¼å¼"""
        try:
            summary = {
                'total_insights': 0,
                'key_findings': [],
                'risk_assessment': 'unknown',
                'recommendations': []
            }
            
            # ç»Ÿè®¡æ´å¯Ÿæ•°é‡
            insight_count = 0
            
            # å¼‚å¸¸æ£€æµ‹æ´å¯Ÿ
            if 'anomalies' in self.insights and 'anomaly_count' in self.insights['anomalies']:
                insight_count += 1
                anomaly_rate = self.insights['anomalies'].get('anomaly_rate', 0)
                if anomaly_rate > 0.1:
                    summary['key_findings'].append(f"å‘ç°å¼‚å¸¸æ•°æ®æ¯”ä¾‹è¾ƒé«˜: {anomaly_rate:.1%}")
                else:
                    summary['key_findings'].append(f"å¼‚å¸¸æ£€æµ‹æ­£å¸¸: {anomaly_rate:.1%}")
            
            # èšç±»åˆ†ææ´å¯Ÿ
            if 'clusters' in self.insights and 'optimal_clusters' in self.insights['clusters']:
                insight_count += 1
                optimal_clusters = self.insights['clusters'].get('optimal_clusters', 0)
                silhouette_score = self.insights['clusters'].get('silhouette_score', 0)
                summary['key_findings'].append(f"èšç±»åˆ†æå®Œæˆ: æœ€ä½³èšç±»æ•° {optimal_clusters}, è½®å»“ç³»æ•° {silhouette_score:.3f}")
            elif 'clusters' in self.insights and self.insights['clusters'].get('status') == 'warning':
                summary['key_findings'].append(f"èšç±»åˆ†æ: {self.insights['clusters'].get('message', 'è­¦å‘Š')}")
            
            # é™ç»´åˆ†ææ´å¯Ÿ
            if 'dimensions' in self.insights and 'n_components' in self.insights['dimensions']:
                insight_count += 1
                n_components = self.insights['dimensions'].get('n_components', 0)
                cumulative_variance = self.insights['dimensions'].get('cumulative_variance', [])
                if cumulative_variance:
                    total_variance = cumulative_variance[-1] if cumulative_variance else 0
                    summary['key_findings'].append(f"é™ç»´åˆ†æå®Œæˆ: {n_components} ä¸ªä¸»æˆåˆ†è§£é‡Š {total_variance:.1%} çš„æ–¹å·®")
            elif 'dimensions' in self.insights and self.insights['dimensions'].get('status') == 'warning':
                summary['key_findings'].append(f"é™ç»´åˆ†æ: {self.insights['dimensions'].get('message', 'è­¦å‘Š')}")
            
            # æ—¶é—´æ¨¡å¼æ´å¯Ÿ
            if 'temporal' in self.insights and 'time_columns_found' in self.insights['temporal']:
                insight_count += 1
                time_columns = self.insights['temporal'].get('time_columns_found', [])
                summary['key_findings'].append(f"æ—¶é—´æ¨¡å¼åˆ†æå®Œæˆ: åˆ†æäº† {len(time_columns)} ä¸ªæ—¶é—´ç»´åº¦")
            elif 'temporal' in self.insights and self.insights['temporal'].get('status') == 'warning':
                summary['key_findings'].append(f"æ—¶é—´æ¨¡å¼åˆ†æ: {self.insights['temporal'].get('message', 'è­¦å‘Š')}")
            
            # é£é™©æœºåˆ¶æ´å¯Ÿ
            if 'risk_mechanisms' in self.insights:
                data_quality = self.insights['risk_mechanisms'].get('data_quality', {})
                missing_rate = data_quality.get('overall_missing_rate', 0)
                risk_level = data_quality.get('risk_level', 'unknown')
                
                summary['key_findings'].append(f"æ•°æ®ç¼ºå¤±ç‡: {missing_rate:.1%}")
                summary['risk_assessment'] = risk_level
                
                if missing_rate > 0.5:
                    summary['key_findings'].append("æ•°æ®è´¨é‡é£é™©è¾ƒé«˜ï¼Œå»ºè®®æ”¹å–„æ•°æ®æ”¶é›†")
                    summary['recommendations'].append("å»ºè®®ç«‹å³æ£€æŸ¥æ•°æ®è´¨é‡å’Œå¼‚å¸¸å€¼")
                    summary['recommendations'].append("è€ƒè™‘å¢åŠ æ•°æ®éªŒè¯æœºåˆ¶")
                elif missing_rate > 0.2:
                    summary['key_findings'].append("æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦å…³æ³¨")
                    summary['recommendations'].append("å»ºè®®å®šæœŸç›‘æ§æ•°æ®è´¨é‡")
                    summary['recommendations'].append("ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹")
                else:
                    summary['key_findings'].append("æ•°æ®è´¨é‡è‰¯å¥½")
                    summary['recommendations'].append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç»§ç»­ç°æœ‰æµç¨‹")
                    summary['recommendations'].append("å»ºè®®å®šæœŸè¿›è¡Œæ¨¡å¼åˆ†æ")
            
            # é‡è¦å½±å“å› ç´ æ´å¯Ÿ
            if 'important_factors' in self.insights and 'new_discoveries' in self.insights['important_factors']:
                insight_count += 1
                new_discoveries = self.insights['important_factors']['new_discoveries']
                for discovery in new_discoveries[:2]:  # æ˜¾ç¤ºå‰2ä¸ªé‡è¦å‘ç°
                    summary['key_findings'].append(f"é‡è¦å‘ç°: {discovery}")
                
                # æ·»åŠ åŸºäºå‘ç°çš„å»ºè®®
                if any("äº¤äº’æ•ˆåº”" in discovery for discovery in new_discoveries):
                    summary['recommendations'].append("å‘ç°æ˜¾è‘—äº¤äº’æ•ˆåº”ï¼Œå»ºè®®åœ¨é¢„æµ‹æ¨¡å‹ä¸­è€ƒè™‘ç‰¹å¾äº¤äº’é¡¹")
                if any("å­£èŠ‚æ€§" in discovery for discovery in new_discoveries):
                    summary['recommendations'].append("å‘ç°å¼ºå­£èŠ‚æ€§ç‰¹å¾ï¼Œå»ºè®®å»ºç«‹å­£èŠ‚æ€§é¢„æµ‹æ¨¡å‹")
            
            # ç›¸å…³æ€§ç½‘ç»œæ´å¯Ÿ
            if 'correlation_network' in self.insights and 'central_features' in self.insights['correlation_network']:
                insight_count += 1
                central_features = self.insights['correlation_network']['central_features']
                if central_features:
                    top_central = central_features[0]
                    summary['key_findings'].append(f"ç½‘ç»œä¸­å¿ƒç‰¹å¾: {top_central['feature']} (ä¸­å¿ƒæ€§å¾—åˆ†: {top_central['centrality_score']:.3f})")
                    summary['recommendations'].append(f"é‡ç‚¹å…³æ³¨ {top_central['feature']} ä½œä¸ºå…³é”®å½±å“å› ç´ ")
            
            # SWEå†·é—¨å› ç´ æ´å¯Ÿ
            if 'swe_cold_factors' in self.insights and 'potential_discoveries' in self.insights['swe_cold_factors']:
                insight_count += 1
                potential_discoveries = self.insights['swe_cold_factors']['potential_discoveries']
                
                if potential_discoveries:
                    # æ˜¾ç¤ºå‰2ä¸ªæœ€é‡è¦çš„å‘ç°
                    for discovery in potential_discoveries[:2]:
                        if discovery.get('type') == 'hidden_effect':
                            summary['key_findings'].append(f"éšè—æ•ˆåº”å‘ç°: {discovery['feature']} (æ®‹å·®ç›¸å…³æ€§: {discovery['residual_correlation']:.3f})")
                        elif discovery.get('type') == 'nonlinear_interaction':
                            summary['key_findings'].append(f"éçº¿æ€§äº¤äº’: {discovery['cold_feature']} Ã— {discovery['main_feature']} (å¼ºåº¦: {discovery['interaction_strength']:.3f})")
                        else:
                            summary['key_findings'].append(f"å†·é—¨å› ç´ : {discovery.get('description', 'æœªçŸ¥')} (é‡è¦æ€§: {discovery.get('potential_importance', 0):.3f})")
                    
                    # æ·»åŠ åŸºäºå†·é—¨å› ç´ çš„å»ºè®®
                    summary['recommendations'].append("å‘ç°æ½œåœ¨å†·é—¨å› ç´ ï¼Œå»ºè®®åœ¨SWEé¢„æµ‹æ¨¡å‹ä¸­è€ƒè™‘åœŸå£¤æ¹¿åº¦ã€ç©ºé—´å˜å¼‚æ€§ç­‰è¢«å¿½è§†çš„å› ç´ ")
                    summary['recommendations'].append("å»ºè®®è¿›è¡Œæ ‡å‡†åŒ–åˆ†æï¼Œå»é™¤å·²çŸ¥ä¸»æ•ˆåº”ä»¥å‘ç°éšè—çš„å½±å“å› ç´ ")
                
                # æ·»åŠ ç ”ç©¶æ´å¯Ÿ
                if 'research_insights' in self.insights['swe_cold_factors']:
                    research_insights = self.insights['swe_cold_factors']['research_insights']
                    for insight in research_insights[:2]:  # æ˜¾ç¤ºå‰2ä¸ªç ”ç©¶æ´å¯Ÿ
                        summary['recommendations'].append(f"ç ”ç©¶å»ºè®®: {insight}")
            
            summary['total_insights'] = insight_count
            
            # å¦‚æœæ²¡æœ‰å…³é”®å‘ç°ï¼Œæ·»åŠ é»˜è®¤ä¿¡æ¯
            if not summary['key_findings']:
                summary['key_findings'].append("æ•°æ®æ¢ç´¢å®Œæˆï¼Œä½†å‘ç°æœ‰é™")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return {
                'status': 'error', 
                'error': str(e),
                'total_insights': 0,
                'key_findings': ['æ‘˜è¦ç”Ÿæˆå¤±è´¥'],
                'risk_assessment': 'unknown',
                'recommendations': ['è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€']
            }
    
    def _discover_important_factors(self, data: pd.DataFrame, target_col: str) -> Dict:
        """å‘ç°é‡è¦å½±å“å› ç´  - æ ¸å¿ƒåŠŸèƒ½ï¼šè§£é‡Šæ•°æ®å…³ç³»"""
        try:
            logger.info("ğŸ” å¼€å§‹é‡è¦å½±å“å› ç´ å‘ç°...")
            
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if target_col in numeric_cols:
                numeric_cols.remove(target_col)
            
            # ç§»é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„ç‰¹å¾
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            
            if len(valid_features) == 0:
                return {'status': 'warning', 'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆç‰¹å¾è¿›è¡Œå½±å“å› ç´ åˆ†æ'}
            
            # å‡†å¤‡æ•°æ®
            X = data[valid_features].fillna(data[valid_features].median())
            y = data[target_col].fillna(data[target_col].median()) if target_col in data.columns else None
            
            factor_insights = {
                'total_features_analyzed': len(valid_features),
                'feature_importance': {},
                'correlation_analysis': {},
                'interaction_effects': {},
                'seasonal_factors': {},
                'new_discoveries': []
            }
            
            # 1. ç‰¹å¾é‡è¦æ€§åˆ†æ (åŸºäºæ–¹å·®å’Œç›¸å…³æ€§)
            feature_importance = {}
            for col in valid_features:
                if col in X.columns:
                    # è®¡ç®—æ–¹å·® (é«˜æ–¹å·® = é«˜å½±å“æ½œåŠ›)
                    variance = X[col].var()
                    
                    # è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§
                    if y is not None:
                        correlation = X[col].corr(y)
                    else:
                        correlation = 0
                    
                    # è®¡ç®—å˜å¼‚ç³»æ•° (ç¨³å®šæ€§æŒ‡æ ‡)
                    cv = X[col].std() / X[col].mean() if X[col].mean() != 0 else 0
                    
                    feature_importance[col] = {
                        'variance': float(variance),
                        'correlation_with_target': float(correlation) if not pd.isna(correlation) else 0,
                        'coefficient_of_variation': float(cv),
                        'importance_score': float(abs(correlation) * variance) if not pd.isna(correlation) else 0
                    }
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_features = sorted(feature_importance.items(), 
                                   key=lambda x: x[1]['importance_score'], reverse=True)
            
            factor_insights['feature_importance'] = dict(sorted_features)
            
            # 2. ç›¸å…³æ€§ç½‘ç»œåˆ†æ
            correlation_matrix = X.corr()
            strong_correlations = []
            
            for i, col1 in enumerate(valid_features):
                for j, col2 in enumerate(valid_features[i+1:], i+1):
                    corr_value = correlation_matrix.loc[col1, col2]
                    if abs(corr_value) > 0.7:  # å¼ºç›¸å…³
                        strong_correlations.append({
                            'feature1': col1,
                            'feature2': col2,
                            'correlation': float(corr_value),
                            'strength': 'strong' if abs(corr_value) > 0.8 else 'moderate'
                        })
            
            factor_insights['correlation_analysis'] = {
                'strong_correlations': strong_correlations,
                'correlation_matrix': correlation_matrix.to_dict()
            }
            
            # 3. äº¤äº’æ•ˆåº”å‘ç°
            interaction_effects = []
            top_features = [f[0] for f in sorted_features[:5]]  # å‰5ä¸ªé‡è¦ç‰¹å¾
            
            for i, feat1 in enumerate(top_features):
                for feat2 in top_features[i+1:]:
                    if feat1 in X.columns and feat2 in X.columns:
                        # è®¡ç®—äº¤äº’é¡¹
                        interaction = X[feat1] * X[feat2]
                        if y is not None:
                            interaction_corr = interaction.corr(y)
                            if abs(interaction_corr) > 0.3:  # æ˜¾è‘—äº¤äº’æ•ˆåº”
                                interaction_effects.append({
                                    'feature1': feat1,
                                    'feature2': feat2,
                                    'interaction_correlation': float(interaction_corr),
                                    'interpretation': f"{feat1} å’Œ {feat2} çš„äº¤äº’æ•ˆåº”æ˜¾è‘—"
                                })
            
            factor_insights['interaction_effects'] = interaction_effects
            
            # 4. å­£èŠ‚æ€§å› ç´ åˆ†æ
            seasonal_factors = {}
            if 'Month' in data.columns:
                monthly_stats = data.groupby('Month')[valid_features].mean()
                seasonal_variation = monthly_stats.std() / monthly_stats.mean()
                
                seasonal_factors = {
                    'monthly_variation': seasonal_variation.to_dict(),
                    'most_seasonal_features': seasonal_variation.nlargest(3).to_dict()
                }
            
            factor_insights['seasonal_factors'] = seasonal_factors
            
            # 5. æ–°å‘ç°æ€»ç»“
            new_discoveries = []
            
            # å‘ç°æœ€é‡è¦çš„å½±å“å› ç´ 
            if sorted_features:
                top_factor = sorted_features[0]
                new_discoveries.append(f"æœ€é‡è¦çš„å½±å“å› ç´ : {top_factor[0]} (é‡è¦æ€§å¾—åˆ†: {top_factor[1]['importance_score']:.3f})")
            
            # å‘ç°å¼ºç›¸å…³ç‰¹å¾å¯¹
            if strong_correlations:
                strongest_corr = max(strong_correlations, key=lambda x: abs(x['correlation']))
                new_discoveries.append(f"æœ€å¼ºç›¸å…³ç‰¹å¾å¯¹: {strongest_corr['feature1']} â†” {strongest_corr['feature2']} (ç›¸å…³ç³»æ•°: {strongest_corr['correlation']:.3f})")
            
            # å‘ç°æ˜¾è‘—äº¤äº’æ•ˆåº”
            if interaction_effects:
                strongest_interaction = max(interaction_effects, key=lambda x: abs(x['interaction_correlation']))
                new_discoveries.append(f"æ˜¾è‘—äº¤äº’æ•ˆåº”: {strongest_interaction['feature1']} Ã— {strongest_interaction['feature2']} (äº¤äº’ç›¸å…³ç³»æ•°: {strongest_interaction['interaction_correlation']:.3f})")
            
            # å‘ç°å­£èŠ‚æ€§ç‰¹å¾
            if seasonal_factors and 'most_seasonal_features' in seasonal_factors:
                most_seasonal = max(seasonal_factors['most_seasonal_features'].items(), key=lambda x: x[1])
                new_discoveries.append(f"æœ€å¼ºå­£èŠ‚æ€§ç‰¹å¾: {most_seasonal[0]} (å­£èŠ‚æ€§å˜å¼‚ç³»æ•°: {most_seasonal[1]:.3f})")
            
            factor_insights['new_discoveries'] = new_discoveries
            
            logger.info(f"âœ… é‡è¦å½±å“å› ç´ å‘ç°å®Œæˆ: åˆ†æäº† {len(valid_features)} ä¸ªç‰¹å¾")
            logger.info(f"ğŸ” å‘ç° {len(new_discoveries)} ä¸ªé‡è¦æ´å¯Ÿ")
            
            return factor_insights
            
        except Exception as e:
            logger.error(f"âŒ é‡è¦å½±å“å› ç´ å‘ç°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_correlation_network(self, data: pd.DataFrame, target_col: str) -> Dict:
        """ç›¸å…³æ€§ç½‘ç»œåˆ†æ - å‘ç°ç‰¹å¾é—´çš„å¤æ‚å…³ç³»"""
        try:
            logger.info("ğŸ” å¼€å§‹ç›¸å…³æ€§ç½‘ç»œåˆ†æ...")
            
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if target_col in numeric_cols:
                numeric_cols.remove(target_col)
            
            # ç§»é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„ç‰¹å¾
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            
            if len(valid_features) < 3:
                return {'status': 'warning', 'message': 'ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç½‘ç»œåˆ†æ'}
            
            # å‡†å¤‡æ•°æ® - ç¡®ä¿æ²¡æœ‰NaNå€¼
            X = data[valid_features].fillna(data[valid_features].median())
            
            # å†æ¬¡æ£€æŸ¥å¹¶å¤„ç†ä»»ä½•å‰©ä½™çš„NaNå€¼
            X = X.fillna(0)
            
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            correlation_matrix = X.corr()
            
            network_insights = {
                'network_statistics': {},
                'central_features': [],
                'feature_clusters': [],
                'influence_paths': [],
                'network_visualization': {}
            }
            
            # 1. ç½‘ç»œç»Ÿè®¡
            total_connections = 0
            strong_connections = 0
            moderate_connections = 0
            
            for i, col1 in enumerate(valid_features):
                for j, col2 in enumerate(valid_features[i+1:], i+1):
                    corr_value = abs(correlation_matrix.loc[col1, col2])
                    if corr_value > 0.3:  # æœ‰æ„ä¹‰çš„è¿æ¥
                        total_connections += 1
                        if corr_value > 0.7:
                            strong_connections += 1
                        elif corr_value > 0.5:
                            moderate_connections += 1
            
            network_insights['network_statistics'] = {
                'total_features': len(valid_features),
                'total_connections': total_connections,
                'strong_connections': strong_connections,
                'moderate_connections': moderate_connections,
                'network_density': total_connections / (len(valid_features) * (len(valid_features) - 1) / 2)
            }
            
            # 2. ä¸­å¿ƒæ€§ç‰¹å¾ (ä¸å…¶ä»–ç‰¹å¾ç›¸å…³æ€§æœ€å¤šçš„ç‰¹å¾)
            centrality_scores = {}
            for col in valid_features:
                connections = 0
                total_corr = 0
                for other_col in valid_features:
                    if col != other_col:
                        corr_value = abs(correlation_matrix.loc[col, other_col])
                        if corr_value > 0.3:
                            connections += 1
                            total_corr += corr_value
                
                centrality_scores[col] = {
                    'connection_count': connections,
                    'average_correlation': total_corr / connections if connections > 0 else 0,
                    'centrality_score': connections * (total_corr / connections if connections > 0 else 0)
                }
            
            # æŒ‰ä¸­å¿ƒæ€§æ’åº
            central_features = sorted(centrality_scores.items(), 
                                    key=lambda x: x[1]['centrality_score'], reverse=True)[:5]
            
            network_insights['central_features'] = [
                {
                    'feature': feat,
                    'centrality_score': score['centrality_score'],
                    'connection_count': score['connection_count'],
                    'average_correlation': score['average_correlation']
                }
                for feat, score in central_features
            ]
            
            # 3. ç‰¹å¾èšç±» (åŸºäºç›¸å…³æ€§)
            from sklearn.cluster import AgglomerativeClustering
            
            # ä½¿ç”¨1-|correlation|ä½œä¸ºè·ç¦»
            distance_matrix = 1 - abs(correlation_matrix)
            
            # èšç±»
            clustering = AgglomerativeClustering(n_clusters=min(3, len(valid_features)//2), 
                                               metric='precomputed', linkage='average')
            cluster_labels = clustering.fit_predict(distance_matrix)
            
            # ç»„ç»‡èšç±»ç»“æœ
            feature_clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in feature_clusters:
                    feature_clusters[label] = []
                feature_clusters[label].append(valid_features[i])
            
            network_insights['feature_clusters'] = [
                {
                    'cluster_id': cluster_id,
                    'features': features,
                    'cluster_size': len(features),
                    'intra_cluster_correlation': self._calculate_intra_cluster_correlation(features, correlation_matrix)
                }
                for cluster_id, features in feature_clusters.items()
            ]
            
            # 4. å½±å“è·¯å¾„åˆ†æ
            influence_paths = []
            for central_feat, _ in central_features[:3]:  # å‰3ä¸ªä¸­å¿ƒç‰¹å¾
                paths = self._find_influence_paths(central_feat, valid_features, correlation_matrix)
                influence_paths.extend(paths)
            
            network_insights['influence_paths'] = influence_paths
            
            # 5. ç½‘ç»œå¯è§†åŒ–æ•°æ®
            network_insights['network_visualization'] = {
                'nodes': [
                    {
                        'id': feat,
                        'centrality': centrality_scores[feat]['centrality_score'],
                        'cluster': cluster_labels[valid_features.index(feat)]
                    }
                    for feat in valid_features
                ],
                'edges': [
                    {
                        'source': valid_features[i],
                        'target': valid_features[j],
                        'weight': abs(correlation_matrix.loc[valid_features[i], valid_features[j]]),
                        'correlation': correlation_matrix.loc[valid_features[i], valid_features[j]]
                    }
                    for i in range(len(valid_features))
                    for j in range(i+1, len(valid_features))
                    if abs(correlation_matrix.loc[valid_features[i], valid_features[j]]) > 0.3
                ]
            }
            
            logger.info(f"âœ… ç›¸å…³æ€§ç½‘ç»œåˆ†æå®Œæˆ: {len(valid_features)} ä¸ªç‰¹å¾, {total_connections} ä¸ªè¿æ¥")
            
            return network_insights
            
        except Exception as e:
            logger.error(f"âŒ ç›¸å…³æ€§ç½‘ç»œåˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _calculate_intra_cluster_correlation(self, features: List[str], correlation_matrix: pd.DataFrame) -> float:
        """è®¡ç®—èšç±»å†…å¹³å‡ç›¸å…³æ€§"""
        if len(features) < 2:
            return 0.0
        
        total_corr = 0
        count = 0
        for i, feat1 in enumerate(features):
            for feat2 in features[i+1:]:
                if feat1 in correlation_matrix.columns and feat2 in correlation_matrix.columns:
                    total_corr += abs(correlation_matrix.loc[feat1, feat2])
                    count += 1
        
        return total_corr / count if count > 0 else 0.0
    
    def _find_influence_paths(self, central_feature: str, all_features: List[str], 
                            correlation_matrix: pd.DataFrame, max_depth: int = 2) -> List[Dict]:
        """å‘ç°å½±å“è·¯å¾„"""
        paths = []
        
        # æ‰¾åˆ°ä¸ä¸­å¿ƒç‰¹å¾å¼ºç›¸å…³çš„ç‰¹å¾
        strong_connections = []
        for feat in all_features:
            if feat != central_feature:
                corr = abs(correlation_matrix.loc[central_feature, feat])
                if corr > 0.5:
                    strong_connections.append((feat, corr))
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        strong_connections.sort(key=lambda x: x[1], reverse=True)
        
        # æ„å»ºå½±å“è·¯å¾„
        for connected_feat, corr in strong_connections[:3]:  # å‰3ä¸ªå¼ºè¿æ¥
            paths.append({
                'central_feature': central_feature,
                'connected_feature': connected_feat,
                'correlation_strength': float(corr),
                'path_type': 'direct_influence',
                'interpretation': f"{central_feature} ç›´æ¥å½±å“ {connected_feat} (ç›¸å…³ç³»æ•°: {corr:.3f})"
            })
        
        return paths
    
    def _discover_swe_cold_factors(self, data: pd.DataFrame, target_col: str) -> Dict:
        """å‘ç°SWEå†·é—¨å› ç´  - åŸºäºç ”ç©¶æ–‡çŒ®çš„æ½œåœ¨é‡è¦å½±å“å› ç´ """
        try:
            logger.info("ğŸ” å¼€å§‹SWEå†·é—¨å› ç´ å‘ç°...")
            
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if target_col in numeric_cols:
                numeric_cols.remove(target_col)
            
            # ç§»é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„ç‰¹å¾
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            
            if len(valid_features) == 0:
                return {'status': 'warning', 'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆç‰¹å¾è¿›è¡ŒSWEå†·é—¨å› ç´ åˆ†æ'}
            
            # å‡†å¤‡æ•°æ®
            X = data[valid_features].fillna(data[valid_features].median())
            y = data[target_col].fillna(data[target_col].median()) if target_col in data.columns else None
            
            swe_cold_factors = {
                'target_analysis': target_col,
                'candidate_cold_factors': {},
                'standardized_analysis': {},
                'nonlinear_interactions': {},
                'potential_discoveries': [],
                'research_insights': []
            }
            
            # 1. å®šä¹‰SWEå†·é—¨å› ç´ å€™é€‰ç‰¹å¾
            cold_factor_candidates = {
                'soil_moisture_related': {
                    'description': 'åœŸå£¤æ¹¿åº¦ç›¸å…³å› ç´ ',
                    'research_basis': 'åœŸå£¤æ¹¿åº¦å¯¹é›ªç›–åŠ¨æ€å…·æœ‰é‡è¦å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åœ°å½¢åŒºåŸŸ',
                    'candidate_features': [col for col in valid_features if any(keyword in col.lower() 
                                    for keyword in ['moisture', 'humidity', 'precip', 'rain'])]
                },
                'spatial_variability': {
                    'description': 'ç©ºé—´å˜å¼‚æ€§å› ç´ ',
                    'research_basis': 'é›ªå¯†åº¦å’ŒSWEçš„ç©ºé—´å˜å¼‚æ€§å¯¹ä¼°è®¡å‡†ç¡®æ€§æœ‰æ˜¾è‘—å½±å“',
                    'candidate_features': [col for col in valid_features if any(keyword in col.lower() 
                                    for keyword in ['lat', 'lon', 'longitude', 'latitude', 'elevation', 'altitude'])]
                },
                'forest_cover_impact': {
                    'description': 'æ£®æ—è¦†ç›–å½±å“',
                    'research_basis': 'æ£®æ—è¦†ç›–å½±å“é¥æ„Ÿæ•°æ®å¯¹é›ªç›–çš„æµ‹é‡ç²¾åº¦',
                    'candidate_features': [col for col in valid_features if any(keyword in col.lower() 
                                    for keyword in ['forest', 'tree', 'vegetation', 'cover'])]
                },
                'snow_type_changes': {
                    'description': 'é›ªç›–ç±»å‹å˜åŒ–',
                    'research_basis': 'é›ªç›–ç±»å‹å˜åŒ–å¯èƒ½å¯¼è‡´SWEå˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ°”å€™å˜åŒ–å½±å“ä¸‹',
                    'candidate_features': [col for col in valid_features if any(keyword in col.lower() 
                                    for keyword in ['snow', 'ice', 'density', 'type'])]
                },
                'temporal_changes': {
                    'description': 'æ—¶é—´å˜åŒ–å› ç´ ',
                    'research_basis': 'SWEçš„æ—¶é—´å˜åŒ–å½±å“æ°´èµ„æºå¯ç”¨æ€§ï¼ŒåŒ—åŠçƒSWEåœ¨1951-2022å¹´æœŸé—´ä¸‹é™',
                    'candidate_features': [col for col in valid_features if any(keyword in col.lower() 
                                    for keyword in ['time', 'date', 'year', 'month', 'day', 'season'])]
                }
            }
            
            # 2. åˆ†ææ¯ä¸ªå†·é—¨å› ç´ ç±»åˆ«
            for category, info in cold_factor_candidates.items():
                candidate_features = info['candidate_features']
                
                if candidate_features:
                    category_analysis = {
                        'description': info['description'],
                        'research_basis': info['research_basis'],
                        'candidate_features': candidate_features,
                        'feature_analysis': {},
                        'potential_importance': 0.0
                    }
                    
                    # åˆ†ææ¯ä¸ªå€™é€‰ç‰¹å¾
                    for feature in candidate_features:
                        if feature in X.columns:
                            # è®¡ç®—ç‰¹å¾ç»Ÿè®¡
                            feature_stats = {
                                'variance': float(X[feature].var()),
                                'mean': float(X[feature].mean()),
                                'std': float(X[feature].std()),
                                'skewness': float(X[feature].skew()),
                                'kurtosis': float(X[feature].kurtosis())
                            }
                            
                            # è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§ï¼ˆå¦‚æœæœ‰ç›®æ ‡å˜é‡ï¼‰
                            if y is not None:
                                correlation = X[feature].corr(y)
                                feature_stats['correlation_with_target'] = float(correlation) if not pd.isna(correlation) else 0.0
                            else:
                                feature_stats['correlation_with_target'] = 0.0
                            
                            # è®¡ç®—å˜å¼‚ç³»æ•°
                            cv = X[feature].std() / X[feature].mean() if X[feature].mean() != 0 else 0
                            feature_stats['coefficient_of_variation'] = float(cv)
                            
                            # è®¡ç®—æ½œåœ¨é‡è¦æ€§å¾—åˆ†
                            importance_score = abs(feature_stats['correlation_with_target']) * feature_stats['variance'] * (1 + abs(cv))
                            feature_stats['potential_importance'] = float(importance_score)
                            
                            category_analysis['feature_analysis'][feature] = feature_stats
                            category_analysis['potential_importance'] += importance_score
                    
                    swe_cold_factors['candidate_cold_factors'][category] = category_analysis
            
            # 3. æ ‡å‡†åŒ–åˆ†æ - å»é™¤å·²çŸ¥ä¸»æ•ˆåº”
            logger.info("ğŸ” è¿›è¡Œæ ‡å‡†åŒ–åˆ†æï¼Œå»é™¤å·²çŸ¥ä¸»æ•ˆåº”...")
            
            # è¯†åˆ«ä¸»è¦å½±å“å› ç´ ï¼ˆæ¸©åº¦ã€é™æ°´ç­‰ï¼‰
            main_effects = []
            for col in valid_features:
                if any(keyword in col.lower() for keyword in ['temp', 'precip', 'rain', 'snow']):
                    main_effects.append(col)
            
            if main_effects and y is not None:
                # è®¡ç®—ä¸»æ•ˆåº”çš„çº¿æ€§ç»„åˆ
                main_effect_values = X[main_effects].mean(axis=1)
                
                # ä»ç›®æ ‡å˜é‡ä¸­å»é™¤ä¸»æ•ˆåº”
                y_residual = y - main_effect_values.corr(y) * main_effect_values if len(main_effects) > 0 else y
                
                # é‡æ–°è®¡ç®—å†·é—¨å› ç´ ä¸æ®‹å·®çš„ç›¸å…³æ€§
                standardized_analysis = {}
                for category, info in swe_cold_factors['candidate_cold_factors'].items():
                    if 'feature_analysis' in info:
                        standardized_features = {}
                        for feature, stats in info['feature_analysis'].items():
                            if feature in X.columns:
                                # è®¡ç®—ä¸æ®‹å·®çš„ç›¸å…³æ€§
                                residual_correlation = X[feature].corr(y_residual)
                                standardized_features[feature] = {
                                    'original_correlation': stats['correlation_with_target'],
                                    'residual_correlation': float(residual_correlation) if not pd.isna(residual_correlation) else 0.0,
                                    'correlation_change': float(residual_correlation - stats['correlation_with_target']) if not pd.isna(residual_correlation) else 0.0,
                                    'potential_hidden_effect': abs(residual_correlation) > abs(stats['correlation_with_target']) if not pd.isna(residual_correlation) else False
                                }
                        standardized_analysis[category] = standardized_features
                
                swe_cold_factors['standardized_analysis'] = standardized_analysis
            
            # 4. éçº¿æ€§äº¤äº’æ•ˆåº”å‘ç°
            logger.info("ğŸ” å‘ç°éçº¿æ€§äº¤äº’æ•ˆåº”...")
            
            nonlinear_interactions = {}
            top_cold_features = []
            
            # æ”¶é›†æ‰€æœ‰å†·é—¨ç‰¹å¾çš„æ½œåœ¨é‡è¦æ€§
            for category, info in swe_cold_factors['candidate_cold_factors'].items():
                if 'feature_analysis' in info:
                    for feature, stats in info['feature_analysis'].items():
                        if stats['potential_importance'] > 0:
                            top_cold_features.append((feature, stats['potential_importance']))
            
            # æŒ‰é‡è¦æ€§æ’åº
            top_cold_features.sort(key=lambda x: x[1], reverse=True)
            top_cold_features = [f[0] for f in top_cold_features[:5]]  # å‰5ä¸ªå†·é—¨ç‰¹å¾
            
            # åˆ†æå†·é—¨ç‰¹å¾ä¸ä¸»è¦ç‰¹å¾çš„äº¤äº’æ•ˆåº”
            for cold_feature in top_cold_features:
                if cold_feature in X.columns:
                    interactions = {}
                    for main_feature in main_effects[:3]:  # å‰3ä¸ªä¸»è¦ç‰¹å¾
                        if main_feature in X.columns:
                            # è®¡ç®—äº¤äº’é¡¹
                            interaction_term = X[cold_feature] * X[main_feature]
                            
                            # è®¡ç®—äº¤äº’é¡¹ä¸ç›®æ ‡çš„ç›¸å…³æ€§
                            if y is not None:
                                interaction_correlation = interaction_term.corr(y)
                                interactions[main_feature] = {
                                    'interaction_correlation': float(interaction_correlation) if not pd.isna(interaction_correlation) else 0.0,
                                    'interaction_strength': abs(interaction_correlation) if not pd.isna(interaction_correlation) else 0.0,
                                    'interpretation': f"{cold_feature} Ã— {main_feature} çš„äº¤äº’æ•ˆåº”"
                                }
                    
                    if interactions:
                        nonlinear_interactions[cold_feature] = interactions
            
            swe_cold_factors['nonlinear_interactions'] = nonlinear_interactions
            
            # 5. æ½œåœ¨å‘ç°æ€»ç»“
            potential_discoveries = []
            
            # å‘ç°é«˜æ½œåœ¨é‡è¦æ€§çš„å†·é—¨å› ç´ 
            for category, info in swe_cold_factors['candidate_cold_factors'].items():
                if info['potential_importance'] > 0:
                    potential_discoveries.append({
                        'category': category,
                        'description': info['description'],
                        'potential_importance': info['potential_importance'],
                        'research_basis': info['research_basis']
                    })
            
            # å‘ç°æ ‡å‡†åŒ–åçš„éšè—æ•ˆåº”
            if 'standardized_analysis' in swe_cold_factors:
                for category, features in swe_cold_factors['standardized_analysis'].items():
                    for feature, analysis in features.items():
                        if analysis['potential_hidden_effect']:
                            potential_discoveries.append({
                                'type': 'hidden_effect',
                                'feature': feature,
                                'category': category,
                                'original_correlation': analysis['original_correlation'],
                                'residual_correlation': analysis['residual_correlation'],
                                'interpretation': f"{feature} åœ¨å»é™¤ä¸»æ•ˆåº”åæ˜¾ç¤ºå‡ºéšè—çš„å½±å“"
                            })
            
            # å‘ç°æ˜¾è‘—çš„éçº¿æ€§äº¤äº’
            for cold_feature, interactions in nonlinear_interactions.items():
                for main_feature, interaction in interactions.items():
                    if interaction['interaction_strength'] > 0.3:  # æ˜¾è‘—äº¤äº’
                        potential_discoveries.append({
                            'type': 'nonlinear_interaction',
                            'cold_feature': cold_feature,
                            'main_feature': main_feature,
                            'interaction_strength': interaction['interaction_strength'],
                            'interpretation': interaction['interpretation']
                        })
            
            swe_cold_factors['potential_discoveries'] = potential_discoveries
            
            # 6. ç ”ç©¶æ´å¯Ÿæ€»ç»“
            research_insights = [
                "åŸºäºæ–‡çŒ®ç ”ç©¶ï¼ŒåœŸå£¤æ¹¿åº¦ã€é›ªå¯†åº¦ç©ºé—´å˜å¼‚æ€§ã€æ£®æ—è¦†ç›–å½±å“ç­‰å†·é—¨å› ç´ å¯èƒ½å¯¹SWEä¼°è®¡æœ‰é‡è¦å½±å“",
                "é€šè¿‡æ ‡å‡†åŒ–åˆ†æå»é™¤å·²çŸ¥ä¸»æ•ˆåº”ï¼Œå¯ä»¥æ›´æ•æ„Ÿåœ°å‘ç°æ¬¡è¦æˆ–æ½œåœ¨å› ç´ ",
                "å†·é—¨å› ç´ å¯èƒ½ä¸å…¶ä»–ç‰¹å¾å­˜åœ¨éçº¿æ€§ç»„åˆæ•ˆåº”ï¼Œéœ€è¦å®Œæ•´ç‰¹å¾é›†æ‰èƒ½æ•æ‰",
                "å»ºè®®é‡‡ç”¨å¹¿æ³›å€™é€‰ç‰¹å¾â†’æ ‡å‡†åŒ–â†’é™ç»´â†’æ½œåœ¨è´¡çŒ®åˆ†æçš„ç­–ç•¥"
            ]
            
            swe_cold_factors['research_insights'] = research_insights
            
            logger.info(f"âœ… SWEå†·é—¨å› ç´ å‘ç°å®Œæˆ: åˆ†æäº† {len(cold_factor_candidates)} ä¸ªå› ç´ ç±»åˆ«")
            logger.info(f"ğŸ” å‘ç° {len(potential_discoveries)} ä¸ªæ½œåœ¨é‡è¦å‘ç°")
            
            return swe_cold_factors
            
        except Exception as e:
            logger.error(f"âŒ SWEå†·é—¨å› ç´ å‘ç°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def interpret_insights(self, insights: Dict = None) -> Dict:
        """è§£è¯»æ´å¯Ÿç»“æœ - å°†æŠ€æœ¯åˆ†æè½¬åŒ–ä¸ºç”¨æˆ·å¯ç†è§£çš„æ´å¯Ÿ"""
        try:
            logger.info("ğŸ” å¼€å§‹è§£è¯»æ´å¯Ÿç»“æœ...")
            
            if insights is None:
                insights = self.insights
            
            if not insights:
                return {'status': 'error', 'error': 'æ²¡æœ‰å¯è§£è¯»çš„æ´å¯Ÿç»“æœ'}
            
            interpretation = {
                'timestamp': datetime.now().isoformat(),
                'executive_summary': {},
                'business_insights': {},
                'technical_explanations': {},
                'actionable_recommendations': {},
                'risk_assessment': {},
                'data_quality_insights': {}
            }
            
            # 1. æ‰§è¡Œæ‘˜è¦è§£è¯»
            if 'summary' in insights:
                summary = insights['summary']
                interpretation['executive_summary'] = {
                    'total_discoveries': summary.get('total_insights', 0),
                    'key_message': self._generate_key_message(summary),
                    'business_impact': self._assess_business_impact(summary),
                    'urgency_level': self._assess_urgency(summary)
                }
            
            # 2. å¼‚å¸¸æ£€æµ‹è§£è¯»
            if 'anomalies' in insights and 'anomaly_count' in insights['anomalies']:
                interpretation['business_insights']['anomaly_analysis'] = self._interpret_anomalies(insights['anomalies'])
            
            # 3. èšç±»åˆ†æè§£è¯»
            if 'clusters' in insights and 'optimal_clusters' in insights['clusters']:
                interpretation['business_insights']['clustering_analysis'] = self._interpret_clusters(insights['clusters'])
            
            # 4. é™ç»´åˆ†æè§£è¯»
            if 'dimensions' in insights and 'n_components' in insights['dimensions']:
                interpretation['business_insights']['dimension_analysis'] = self._interpret_dimensions(insights['dimensions'])
            
            # 5. æ—¶é—´æ¨¡å¼è§£è¯»
            if 'temporal' in insights and 'time_columns_found' in insights['temporal']:
                interpretation['business_insights']['temporal_analysis'] = self._interpret_temporal_patterns(insights['temporal'])
            
            # 6. é£é™©æœºåˆ¶è§£è¯»
            if 'risk_mechanisms' in insights:
                interpretation['risk_assessment'] = self._interpret_risk_mechanisms(insights['risk_mechanisms'])
            
            # 7. é‡è¦å½±å“å› ç´ è§£è¯»
            if 'important_factors' in insights and 'new_discoveries' in insights['important_factors']:
                interpretation['business_insights']['factor_analysis'] = self._interpret_important_factors(insights['important_factors'])
            
            # 8. ç›¸å…³æ€§ç½‘ç»œè§£è¯»
            if 'correlation_network' in insights and 'central_features' in insights['correlation_network']:
                interpretation['business_insights']['network_analysis'] = self._interpret_correlation_network(insights['correlation_network'])
            
            # 9. SWEå†·é—¨å› ç´ è§£è¯»
            if 'swe_cold_factors' in insights and 'potential_discoveries' in insights['swe_cold_factors']:
                interpretation['business_insights']['cold_factors_analysis'] = self._interpret_swe_cold_factors(insights['swe_cold_factors'])
            
            # 10. æ•°æ®è´¨é‡æ´å¯Ÿ
            interpretation['data_quality_insights'] = self._interpret_data_quality(insights)
            
            # 11. å¯æ“ä½œå»ºè®®
            interpretation['actionable_recommendations'] = self._generate_actionable_recommendations(interpretation)
            
            logger.info("âœ… æ´å¯Ÿç»“æœè§£è¯»å®Œæˆ")
            return interpretation
            
        except Exception as e:
            logger.error(f"âŒ æ´å¯Ÿç»“æœè§£è¯»å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _generate_key_message(self, summary: Dict) -> str:
        """ç”Ÿæˆå…³é”®ä¿¡æ¯"""
        try:
            risk_level = summary.get('risk_assessment', 'unknown')
            total_insights = summary.get('total_insights', 0)
            
            if risk_level == 'high':
                return f"âš ï¸ æ•°æ®è´¨é‡é£é™©è¾ƒé«˜ï¼Œå‘ç° {total_insights} ä¸ªé‡è¦æ´å¯Ÿï¼Œå»ºè®®ç«‹å³å…³æ³¨"
            elif risk_level == 'medium':
                return f"ğŸ“Š æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œå‘ç° {total_insights} ä¸ªé‡è¦æ´å¯Ÿï¼Œéœ€è¦æŒç»­ç›‘æ§"
            else:
                return f"âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå‘ç° {total_insights} ä¸ªé‡è¦æ´å¯Ÿï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸"
        except Exception as e:
            return "æ•°æ®æ¢ç´¢å®Œæˆï¼Œå‘ç°æœ‰é™"
    
    def _assess_business_impact(self, summary: Dict) -> str:
        """è¯„ä¼°ä¸šåŠ¡å½±å“"""
        try:
            risk_level = summary.get('risk_assessment', 'unknown')
            
            if risk_level == 'high':
                return "é«˜å½±å“ - æ•°æ®è´¨é‡é—®é¢˜å¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§ï¼Œå»ºè®®ç«‹å³å¤„ç†"
            elif risk_level == 'medium':
                return "ä¸­ç­‰å½±å“ - æ•°æ®è´¨é‡éœ€è¦å…³æ³¨ï¼Œå¯èƒ½å½±å“é•¿æœŸé¢„æµ‹æ•ˆæœ"
            else:
                return "ä½å½±å“ - æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç³»ç»Ÿè¿è¡Œç¨³å®š"
        except Exception as e:
            return "å½±å“ç¨‹åº¦å¾…è¯„ä¼°"
    
    def _assess_urgency(self, summary: Dict) -> str:
        """è¯„ä¼°ç´§æ€¥ç¨‹åº¦"""
        try:
            risk_level = summary.get('risk_assessment', 'unknown')
            
            if risk_level == 'high':
                return "é«˜ç´§æ€¥ - å»ºè®®24å°æ—¶å†…å¤„ç†"
            elif risk_level == 'medium':
                return "ä¸­ç­‰ç´§æ€¥ - å»ºè®®1å‘¨å†…å¤„ç†"
            else:
                return "ä½ç´§æ€¥ - å¯å®šæœŸç›‘æ§"
        except Exception as e:
            return "ç´§æ€¥ç¨‹åº¦å¾…è¯„ä¼°"
    
    def _interpret_anomalies(self, anomalies: Dict) -> Dict:
        """è§£è¯»å¼‚å¸¸æ£€æµ‹ç»“æœ"""
        try:
            anomaly_rate = anomalies.get('anomaly_rate', 0)
            anomaly_count = anomalies.get('anomaly_count', 0)
            
            interpretation = {
                'anomaly_rate_interpretation': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # å¼‚å¸¸ç‡è§£è¯»
            if anomaly_rate > 0.15:
                interpretation['anomaly_rate_interpretation'] = f"å¼‚å¸¸æ•°æ®æ¯”ä¾‹è¿‡é«˜ ({anomaly_rate:.1%})ï¼Œè¡¨æ˜æ•°æ®è´¨é‡å­˜åœ¨ä¸¥é‡é—®é¢˜"
                interpretation['business_implications'] = "é«˜å¼‚å¸¸ç‡å¯èƒ½å½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœå’Œé¢„æµ‹å‡†ç¡®æ€§"
                interpretation['recommended_actions'] = [
                    "ç«‹å³æ£€æŸ¥æ•°æ®æºå’Œæ•°æ®æ”¶é›†æµç¨‹",
                    "éªŒè¯ä¼ æ„Ÿå™¨å’Œè®¾å¤‡çŠ¶æ€",
                    "æš‚åœä½¿ç”¨æœ‰é—®é¢˜çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒ"
                ]
            elif anomaly_rate > 0.05:
                interpretation['anomaly_rate_interpretation'] = f"å¼‚å¸¸æ•°æ®æ¯”ä¾‹è¾ƒé«˜ ({anomaly_rate:.1%})ï¼Œéœ€è¦å…³æ³¨æ•°æ®è´¨é‡"
                interpretation['business_implications'] = "ä¸­ç­‰å¼‚å¸¸ç‡å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ï¼Œéœ€è¦ç›‘æ§"
                interpretation['recommended_actions'] = [
                    "å®šæœŸæ£€æŸ¥æ•°æ®è´¨é‡",
                    "ç›‘æ§å¼‚å¸¸æ•°æ®è¶‹åŠ¿",
                    "ä¼˜åŒ–æ•°æ®é¢„å¤„ç†æµç¨‹"
                ]
            else:
                interpretation['anomaly_rate_interpretation'] = f"å¼‚å¸¸æ•°æ®æ¯”ä¾‹æ­£å¸¸ ({anomaly_rate:.1%})ï¼Œæ•°æ®è´¨é‡è‰¯å¥½"
                interpretation['business_implications'] = "ä½å¼‚å¸¸ç‡è¡¨æ˜æ•°æ®è´¨é‡ç¨³å®šï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨"
                interpretation['recommended_actions'] = [
                    "ç»§ç»­ç›‘æ§æ•°æ®è´¨é‡",
                    "å®šæœŸè¿›è¡Œå¼‚å¸¸æ£€æµ‹",
                    "ä¿æŒç°æœ‰æ•°æ®æ”¶é›†æµç¨‹"
                ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'å¼‚å¸¸æ£€æµ‹è§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_clusters(self, clusters: Dict) -> Dict:
        """è§£è¯»èšç±»åˆ†æç»“æœ"""
        try:
            optimal_clusters = clusters.get('optimal_clusters', 0)
            silhouette_score = clusters.get('silhouette_score', 0)
            cluster_sizes = clusters.get('cluster_sizes', {})
            
            interpretation = {
                'cluster_interpretation': '',
                'data_pattern_insights': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # èšç±»æ•°è§£è¯»
            if optimal_clusters == 2:
                interpretation['cluster_interpretation'] = "æ•°æ®å‘ˆç°æ˜æ˜¾çš„äºŒå…ƒåˆ†å¸ƒæ¨¡å¼"
                interpretation['data_pattern_insights'] = "å¯èƒ½å­˜åœ¨ä¸¤ç§ä¸åŒçš„æ•°æ®çŠ¶æ€æˆ–æ¡ä»¶"
            elif optimal_clusters == 3:
                interpretation['cluster_interpretation'] = "æ•°æ®å‘ˆç°ä¸‰å…ƒåˆ†å¸ƒæ¨¡å¼"
                interpretation['data_pattern_insights'] = "å¯èƒ½å­˜åœ¨ä¸‰ç§ä¸åŒçš„æ•°æ®çŠ¶æ€æˆ–æ¡ä»¶"
            else:
                interpretation['cluster_interpretation'] = f"æ•°æ®å‘ˆç° {optimal_clusters} å…ƒåˆ†å¸ƒæ¨¡å¼"
                interpretation['data_pattern_insights'] = f"æ•°æ®å…·æœ‰ {optimal_clusters} ç§ä¸åŒçš„çŠ¶æ€æˆ–æ¡ä»¶"
            
            # è½®å»“ç³»æ•°è§£è¯»
            if silhouette_score > 0.7:
                interpretation['business_implications'] = "èšç±»è´¨é‡å¾ˆé«˜ï¼Œæ•°æ®æ¨¡å¼æ¸…æ™°ï¼Œæ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°åŒºåˆ†ä¸åŒç±»åˆ«"
            elif silhouette_score > 0.5:
                interpretation['business_implications'] = "èšç±»è´¨é‡è‰¯å¥½ï¼Œæ•°æ®æ¨¡å¼ç›¸å¯¹æ¸…æ™°"
            elif silhouette_score > 0.3:
                interpretation['business_implications'] = "èšç±»è´¨é‡ä¸€èˆ¬ï¼Œæ•°æ®æ¨¡å¼æœ‰ä¸€å®šé‡å "
            else:
                interpretation['business_implications'] = "èšç±»è´¨é‡è¾ƒä½ï¼Œæ•°æ®æ¨¡å¼é‡å ä¸¥é‡ï¼Œå¯èƒ½éœ€è¦æ›´å¤šç‰¹å¾æˆ–æ›´å¥½çš„é¢„å¤„ç†"
            
            # èšç±»å¤§å°åˆ†æ
            if cluster_sizes:
                cluster_balance = max(cluster_sizes.values()) / min(cluster_sizes.values())
                if cluster_balance > 5:
                    interpretation['recommended_actions'].append("èšç±»å¤§å°ä¸å¹³è¡¡ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åˆ†å¸ƒæ˜¯å¦åˆç†")
                else:
                    interpretation['recommended_actions'].append("èšç±»å¤§å°ç›¸å¯¹å¹³è¡¡ï¼Œæ•°æ®åˆ†å¸ƒåˆç†")
            
            interpretation['recommended_actions'].extend([
                "åŸºäºèšç±»ç»“æœä¼˜åŒ–ç‰¹å¾å·¥ç¨‹",
                "è€ƒè™‘ä¸ºä¸åŒèšç±»å»ºç«‹ä¸“é—¨çš„é¢„æµ‹æ¨¡å‹",
                "ç›‘æ§èšç±»ç¨³å®šæ€§"
            ])
            
            return interpretation
            
        except Exception as e:
            return {'error': f'èšç±»åˆ†æè§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_dimensions(self, dimensions: Dict) -> Dict:
        """è§£è¯»é™ç»´åˆ†æç»“æœ"""
        try:
            n_components = dimensions.get('n_components', 0)
            cumulative_variance = dimensions.get('cumulative_variance', [])
            feature_importance = dimensions.get('feature_importance', {})
            
            interpretation = {
                'dimension_interpretation': '',
                'feature_importance_insights': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # ä¸»æˆåˆ†æ•°è§£è¯»
            if n_components == 2:
                interpretation['dimension_interpretation'] = "æ•°æ®å¯ä»¥ç”¨2ä¸ªä¸»æˆåˆ†å¾ˆå¥½åœ°è¡¨ç¤º"
            elif n_components == 3:
                interpretation['dimension_interpretation'] = "æ•°æ®å¯ä»¥ç”¨3ä¸ªä¸»æˆåˆ†å¾ˆå¥½åœ°è¡¨ç¤º"
            else:
                interpretation['dimension_interpretation'] = f"æ•°æ®éœ€è¦ {n_components} ä¸ªä¸»æˆåˆ†æ¥è¡¨ç¤º"
            
            # æ–¹å·®è§£é‡Šç‡è§£è¯»
            if cumulative_variance:
                total_variance = cumulative_variance[-1] if cumulative_variance else 0
                if total_variance > 0.9:
                    interpretation['business_implications'] = f"é™ç»´æ•ˆæœå¾ˆå¥½ï¼Œ{n_components}ä¸ªä¸»æˆåˆ†è§£é‡Šäº†{total_variance:.1%}çš„æ–¹å·®ï¼Œä¿¡æ¯æŸå¤±å¾ˆå°"
                elif total_variance > 0.8:
                    interpretation['business_implications'] = f"é™ç»´æ•ˆæœè‰¯å¥½ï¼Œ{n_components}ä¸ªä¸»æˆåˆ†è§£é‡Šäº†{total_variance:.1%}çš„æ–¹å·®ï¼Œä¿¡æ¯æŸå¤±è¾ƒå°"
                elif total_variance > 0.7:
                    interpretation['business_implications'] = f"é™ç»´æ•ˆæœä¸€èˆ¬ï¼Œ{n_components}ä¸ªä¸»æˆåˆ†è§£é‡Šäº†{total_variance:.1%}çš„æ–¹å·®ï¼Œæœ‰ä¸€å®šä¿¡æ¯æŸå¤±"
                else:
                    interpretation['business_implications'] = f"é™ç»´æ•ˆæœè¾ƒå·®ï¼Œ{n_components}ä¸ªä¸»æˆåˆ†åªè§£é‡Šäº†{total_variance:.1%}çš„æ–¹å·®ï¼Œä¿¡æ¯æŸå¤±è¾ƒå¤§"
            
            # ç‰¹å¾é‡è¦æ€§è§£è¯»
            if feature_importance:
                top_features = sorted(feature_importance.items(), 
                                   key=lambda x: max(abs(imp) for imp in x[1]), reverse=True)[:3]
                interpretation['feature_importance_insights'] = f"æœ€é‡è¦çš„ç‰¹å¾åŒ…æ‹¬: {', '.join([f[0] for f in top_features])}"
            
            interpretation['recommended_actions'] = [
                "åŸºäºä¸»æˆåˆ†åˆ†æç»“æœä¼˜åŒ–ç‰¹å¾é€‰æ‹©",
                "è€ƒè™‘ä½¿ç”¨é™ç»´åçš„ç‰¹å¾è¿›è¡Œæ¨¡å‹è®­ç»ƒ",
                "ç›‘æ§ç‰¹å¾é‡è¦æ€§çš„å˜åŒ–"
            ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'é™ç»´åˆ†æè§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_temporal_patterns(self, temporal: Dict) -> Dict:
        """è§£è¯»æ—¶é—´æ¨¡å¼åˆ†æç»“æœ"""
        try:
            time_columns = temporal.get('time_columns_found', [])
            patterns = temporal.get('patterns', {})
            
            interpretation = {
                'temporal_interpretation': '',
                'pattern_insights': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # æ—¶é—´åˆ—è§£è¯»
            if len(time_columns) >= 3:
                interpretation['temporal_interpretation'] = "æ•°æ®å…·æœ‰å®Œæ•´çš„æ—¶é—´ç»´åº¦ä¿¡æ¯"
            elif len(time_columns) >= 2:
                interpretation['temporal_interpretation'] = "æ•°æ®å…·æœ‰åŸºæœ¬çš„æ—¶é—´ç»´åº¦ä¿¡æ¯"
            else:
                interpretation['temporal_interpretation'] = "æ•°æ®æ—¶é—´ç»´åº¦ä¿¡æ¯æœ‰é™"
            
            # æ¨¡å¼æ´å¯Ÿ
            pattern_insights = []
            if 'yearly' in patterns:
                pattern_insights.append("å­˜åœ¨å¹´åº¦å˜åŒ–æ¨¡å¼")
            if 'monthly' in patterns:
                pattern_insights.append("å­˜åœ¨æœˆåº¦å­£èŠ‚æ€§æ¨¡å¼")
            if 'daily' in patterns:
                pattern_insights.append("å­˜åœ¨æ—¥å˜åŒ–æ¨¡å¼")
            
            interpretation['pattern_insights'] = "ï¼Œ".join(pattern_insights) if pattern_insights else "æœªå‘ç°æ˜æ˜¾çš„æ—¶é—´æ¨¡å¼"
            
            # ä¸šåŠ¡å½±å“
            if len(pattern_insights) >= 2:
                interpretation['business_implications'] = "æ•°æ®å…·æœ‰ä¸°å¯Œçš„æ—¶é—´æ¨¡å¼ï¼Œé€‚åˆå»ºç«‹æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹"
            elif len(pattern_insights) == 1:
                interpretation['business_implications'] = "æ•°æ®å…·æœ‰åŸºæœ¬çš„æ—¶é—´æ¨¡å¼ï¼Œå¯ä»¥å»ºç«‹ç®€å•çš„æ—¶é—´åºåˆ—æ¨¡å‹"
            else:
                interpretation['business_implications'] = "æ•°æ®æ—¶é—´æ¨¡å¼ä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´ç‰¹å¾æˆ–ä¸åŒçš„åˆ†ææ–¹æ³•"
            
            interpretation['recommended_actions'] = [
                "åŸºäºæ—¶é—´æ¨¡å¼ä¼˜åŒ–æ¨¡å‹æ¶æ„",
                "è€ƒè™‘æ·»åŠ æ—¶é—´ç‰¹å¾ï¼ˆå¦‚å­£èŠ‚ã€æ˜ŸæœŸç­‰ï¼‰",
                "ç›‘æ§æ—¶é—´æ¨¡å¼çš„ç¨³å®šæ€§"
            ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'æ—¶é—´æ¨¡å¼è§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_risk_mechanisms(self, risk_mechanisms: Dict) -> Dict:
        """è§£è¯»é£é™©æœºåˆ¶è¯†åˆ«ç»“æœ"""
        try:
            interpretation = {
                'overall_risk_assessment': '',
                'risk_details': {},
                'business_implications': '',
                'recommended_actions': []
            }
            
            # æ•´ä½“é£é™©è¯„ä¼°
            risk_levels = []
            if 'data_quality' in risk_mechanisms:
                data_quality = risk_mechanisms['data_quality']
                missing_rate = data_quality.get('overall_missing_rate', 0)
                risk_level = data_quality.get('risk_level', 'unknown')
                
                if missing_rate > 0.5:
                    risk_levels.append("high")
                elif missing_rate > 0.2:
                    risk_levels.append("medium")
                else:
                    risk_levels.append("low")
                
                interpretation['risk_details']['data_quality'] = {
                    'risk_level': risk_level,
                    'description': 'æ•°æ®è´¨é‡é£é™©',
                    'implications': 'å¯èƒ½å½±å“æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹å‡†ç¡®æ€§'
                }
            
            if 'extreme_values' in risk_mechanisms:
                extreme_risk = risk_mechanisms['extreme_values']
                risk_level = extreme_risk.get('risk_level', 'unknown')
                risk_levels.append(risk_level)
                interpretation['risk_details']['extreme_values'] = {
                    'risk_level': risk_level,
                    'description': 'æç«¯å€¼é£é™©',
                    'implications': 'å¯èƒ½å½±å“æ¨¡å‹å¯¹å¼‚å¸¸æƒ…å†µçš„é¢„æµ‹èƒ½åŠ›'
                }
            
            if 'temporal_continuity' in risk_mechanisms:
                temporal_risk = risk_mechanisms['temporal_continuity']
                risk_level = temporal_risk.get('risk_level', 'unknown')
                risk_levels.append(risk_level)
                interpretation['risk_details']['temporal_continuity'] = {
                    'risk_level': risk_level,
                    'description': 'æ—¶é—´è¿ç»­æ€§é£é™©',
                    'implications': 'å¯èƒ½å½±å“æ—¶é—´åºåˆ—æ¨¡å‹çš„è®­ç»ƒæ•ˆæœ'
                }
            
            # ç¡®å®šæ•´ä½“é£é™©ç­‰çº§
            if 'high' in risk_levels:
                overall_risk = 'high'
            elif 'medium' in risk_levels:
                overall_risk = 'medium'
            else:
                overall_risk = 'low'
            
            interpretation['overall_risk_assessment'] = overall_risk
            
            # ä¸šåŠ¡å½±å“
            if overall_risk == 'high':
                interpretation['business_implications'] = "æ•´ä½“é£é™©è¾ƒé«˜ï¼Œå»ºè®®ç«‹å³å…³æ³¨æ•°æ®è´¨é‡é—®é¢˜"
            elif overall_risk == 'medium':
                interpretation['business_implications'] = "æ•´ä½“é£é™©ä¸­ç­‰ï¼Œéœ€è¦æŒç»­ç›‘æ§å’Œæ”¹è¿›"
            else:
                interpretation['business_implications'] = "æ•´ä½“é£é™©è¾ƒä½ï¼Œç³»ç»Ÿè¿è¡Œç¨³å®š"
            
            # å»ºè®®è¡ŒåŠ¨
            if overall_risk == 'high':
                interpretation['recommended_actions'] = [
                    "ç«‹å³æ£€æŸ¥æ•°æ®æºå’Œæ•°æ®æ”¶é›†æµç¨‹",
                    "éªŒè¯ä¼ æ„Ÿå™¨å’Œè®¾å¤‡çŠ¶æ€",
                    "æš‚åœä½¿ç”¨æœ‰é—®é¢˜çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒ"
                ]
            elif overall_risk == 'medium':
                interpretation['recommended_actions'] = [
                    "ç›‘æ§æ•°æ®è´¨é‡è¶‹åŠ¿",
                    "æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤"
                ]
                interpretation['recommended_actions'].extend([
                    "ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹",
                    "å»ºç«‹å®šæœŸè´¨é‡è¯„ä¼°æœºåˆ¶"
                ])
            else:
                interpretation['recommended_actions'] = [
                    "ç»§ç»­ç›‘æ§æ•°æ®è´¨é‡",
                    "è®°å½•æœ€ä½³å®è·µ"
                ]
                interpretation['recommended_actions'].extend([
                    "å®šæœŸè¿›è¡Œè´¨é‡è¯„ä¼°",
                    "åˆ†äº«æˆåŠŸç»éªŒ"
                ])
            
            return interpretation
            
        except Exception as e:
            return {'error': f'é£é™©æœºåˆ¶è§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_important_factors(self, important_factors: Dict) -> Dict:
        """è§£è¯»é‡è¦å½±å“å› ç´ å‘ç°ç»“æœ"""
        try:
            new_discoveries = important_factors.get('new_discoveries', [])
            feature_importance = important_factors.get('feature_importance', {})
            interaction_effects = important_factors.get('interaction_effects', [])
            
            interpretation = {
                'factor_importance_insights': '',
                'interaction_insights': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # ç‰¹å¾é‡è¦æ€§æ´å¯Ÿ
            if feature_importance:
                top_features = sorted(feature_importance.items(), 
                                   key=lambda x: x[1].get('importance_score', 0), reverse=True)[:3]
                top_feature_names = [f[0] for f in top_features]
                interpretation['factor_importance_insights'] = f"æœ€é‡è¦çš„å½±å“å› ç´ åŒ…æ‹¬: {', '.join(top_feature_names)}"
            
            # äº¤äº’æ•ˆåº”æ´å¯Ÿ
            if interaction_effects:
                strong_interactions = [eff for eff in interaction_effects if abs(eff.get('interaction_correlation', 0)) > 0.5]
                if strong_interactions:
                    interpretation['interaction_insights'] = f"å‘ç° {len(strong_interactions)} ä¸ªå¼ºäº¤äº’æ•ˆåº”ï¼Œè¡¨æ˜ç‰¹å¾é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»"
                else:
                    interpretation['interaction_insights'] = "äº¤äº’æ•ˆåº”ç›¸å¯¹è¾ƒå¼±ï¼Œç‰¹å¾é—´å…³ç³»ç›¸å¯¹ç®€å•"
            else:
                interpretation['interaction_insights'] = "æœªå‘ç°æ˜¾è‘—çš„äº¤äº’æ•ˆåº”"
            
            # ä¸šåŠ¡å½±å“
            if len(new_discoveries) >= 3:
                interpretation['business_implications'] = "å‘ç°äº†å¤šä¸ªé‡è¦å½±å“å› ç´ ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›äº†é‡è¦ä¿¡æ¯"
            elif len(new_discoveries) >= 1:
                interpretation['business_implications'] = "å‘ç°äº†ä¸€äº›é‡è¦å½±å“å› ç´ ï¼Œæœ‰åŠ©äºæ¨¡å‹æ”¹è¿›"
            else:
                interpretation['business_implications'] = "å½±å“å› ç´ å‘ç°æœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–ä¸åŒçš„åˆ†ææ–¹æ³•"
            
            # å»ºè®®è¡ŒåŠ¨
            interpretation['recommended_actions'] = [
                "åŸºäºé‡è¦å½±å“å› ç´ ä¼˜åŒ–ç‰¹å¾é€‰æ‹©",
                "è€ƒè™‘åœ¨æ¨¡å‹ä¸­æ·»åŠ äº¤äº’æ•ˆåº”é¡¹",
                "ç›‘æ§é‡è¦å½±å“å› ç´ çš„å˜åŒ–"
            ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'é‡è¦å½±å“å› ç´ è§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_correlation_network(self, correlation_network: Dict) -> Dict:
        """è§£è¯»ç›¸å…³æ€§ç½‘ç»œåˆ†æç»“æœ"""
        try:
            network_stats = correlation_network.get('network_statistics', {})
            central_features = correlation_network.get('central_features', [])
            feature_clusters = correlation_network.get('feature_clusters', [])
            
            interpretation = {
                'network_structure_insights': '',
                'centrality_insights': '',
                'clustering_insights': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # ç½‘ç»œç»“æ„æ´å¯Ÿ
            if network_stats:
                total_features = network_stats.get('total_features', 0)
                total_connections = network_stats.get('total_connections', 0)
                network_density = network_stats.get('network_density', 0)
                
                if network_density > 0.5:
                    interpretation['network_structure_insights'] = f"ç‰¹å¾ç½‘ç»œå¯†åº¦è¾ƒé«˜ ({network_density:.2f})ï¼Œè¡¨æ˜ç‰¹å¾é—´å…³ç³»å¤æ‚"
                elif network_density > 0.3:
                    interpretation['network_structure_insights'] = f"ç‰¹å¾ç½‘ç»œå¯†åº¦ä¸­ç­‰ ({network_density:.2f})ï¼Œç‰¹å¾é—´æœ‰ä¸€å®šå…³è”"
                else:
                    interpretation['network_structure_insights'] = f"ç‰¹å¾ç½‘ç»œå¯†åº¦è¾ƒä½ ({network_density:.2f})ï¼Œç‰¹å¾é—´ç›¸å¯¹ç‹¬ç«‹"
            
            # ä¸­å¿ƒæ€§æ´å¯Ÿ
            if central_features:
                top_central = central_features[0] if central_features else {}
                if top_central:
                    interpretation['centrality_insights'] = f"ç½‘ç»œä¸­å¿ƒç‰¹å¾: {top_central.get('feature', 'unknown')}ï¼Œä¸­å¿ƒæ€§å¾—åˆ†: {top_central.get('centrality_score', 0):.3f}"
            
            # èšç±»æ´å¯Ÿ
            if feature_clusters:
                interpretation['clustering_insights'] = f"å‘ç° {len(feature_clusters)} ä¸ªç‰¹å¾èšç±»ï¼Œè¡¨æ˜å­˜åœ¨ç‰¹å¾ç»„"
            
            # ä¸šåŠ¡å½±å“
            if network_stats and network_stats.get('strong_connections', 0) > 0:
                interpretation['business_implications'] = "ç‰¹å¾é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œéœ€è¦è€ƒè™‘ç‰¹å¾å†—ä½™å’Œå¤šé‡å…±çº¿æ€§é—®é¢˜"
            else:
                interpretation['business_implications'] = "ç‰¹å¾é—´ç›¸å…³æ€§é€‚ä¸­ï¼Œç‰¹å¾é€‰æ‹©ç›¸å¯¹åˆç†"
            
            # å»ºè®®è¡ŒåŠ¨
            interpretation['recommended_actions'] = [
                "åŸºäºç½‘ç»œåˆ†æç»“æœä¼˜åŒ–ç‰¹å¾é€‰æ‹©",
                "è€ƒè™‘å»é™¤é«˜åº¦ç›¸å…³çš„å†—ä½™ç‰¹å¾",
                "ç›‘æ§ç‰¹å¾ç›¸å…³æ€§çš„å˜åŒ–"
            ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'ç›¸å…³æ€§ç½‘ç»œè§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_swe_cold_factors(self, swe_cold_factors: Dict) -> Dict:
        """è§£è¯»SWEå†·é—¨å› ç´ å‘ç°ç»“æœ"""
        try:
            potential_discoveries = swe_cold_factors.get('potential_discoveries', [])
            candidate_cold_factors = swe_cold_factors.get('candidate_cold_factors', {})
            
            interpretation = {
                'cold_factors_insights': '',
                'discovery_insights': '',
                'business_implications': '',
                'recommended_actions': []
            }
            
            # å†·é—¨å› ç´ æ´å¯Ÿ
            if candidate_cold_factors:
                interpretation['cold_factors_insights'] = f"åˆ†æäº† {len(candidate_cold_factors)} ä¸ªå†·é—¨å› ç´ ç±»åˆ«ï¼ŒåŒ…æ‹¬åœŸå£¤æ¹¿åº¦ã€ç©ºé—´å˜å¼‚æ€§ã€æ£®æ—è¦†ç›–å½±å“ç­‰"
            
            # å‘ç°æ´å¯Ÿ
            if potential_discoveries:
                hidden_effects = [d for d in potential_discoveries if d.get('type') == 'hidden_effect']
                nonlinear_interactions = [d for d in potential_discoveries if d.get('type') == 'nonlinear_interaction']
                
                insights = []
                if hidden_effects:
                    insights.append(f"å‘ç° {len(hidden_effects)} ä¸ªéšè—æ•ˆåº”")
                if nonlinear_interactions:
                    insights.append(f"å‘ç° {len(nonlinear_interactions)} ä¸ªéçº¿æ€§äº¤äº’")
                
                interpretation['discovery_insights'] = "ï¼Œ".join(insights) if insights else "å‘ç°äº†ä¸€äº›æ½œåœ¨çš„é‡è¦å†·é—¨å› ç´ "
            else:
                interpretation['discovery_insights'] = "æœªå‘ç°æ˜¾è‘—çš„å†·é—¨å› ç´ "
            
            # ä¸šåŠ¡å½±å“
            if len(potential_discoveries) >= 2:
                interpretation['business_implications'] = "å‘ç°äº†å¤šä¸ªæ½œåœ¨çš„é‡è¦å†·é—¨å› ç´ ï¼Œä¸ºSWEé¢„æµ‹æ¨¡å‹ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯"
            elif len(potential_discoveries) == 1:
                interpretation['business_implications'] = "å‘ç°äº†ä¸€äº›æ½œåœ¨çš„é‡è¦å†·é—¨å› ç´ ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶"
            else:
                interpretation['business_implications'] = "å†·é—¨å› ç´ å‘ç°æœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–ä¸åŒçš„åˆ†ææ–¹æ³•"
            
            # å»ºè®®è¡ŒåŠ¨
            interpretation['recommended_actions'] = [
                "è¿›ä¸€æ­¥ç ”ç©¶å‘ç°çš„å†·é—¨å› ç´ ",
                "è€ƒè™‘åœ¨SWEé¢„æµ‹æ¨¡å‹ä¸­é›†æˆè¿™äº›å› ç´ ",
                "å»ºç«‹é•¿æœŸç›‘æ§æœºåˆ¶è·Ÿè¸ªå†·é—¨å› ç´ çš„å˜åŒ–"
            ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'SWEå†·é—¨å› ç´ è§£è¯»å¤±è´¥: {e}'}
    
    def _interpret_data_quality(self, insights: Dict) -> Dict:
        """è§£è¯»æ•°æ®è´¨é‡æ´å¯Ÿ"""
        try:
            interpretation = {
                'overall_quality_assessment': '',
                'quality_issues': [],
                'quality_strengths': [],
                'recommended_actions': []
            }
            
            # æ•´ä½“è´¨é‡è¯„ä¼°
            if 'risk_mechanisms' in insights and 'data_quality' in insights['risk_mechanisms']:
                data_quality = insights['risk_mechanisms']['data_quality']
                missing_rate = data_quality.get('overall_missing_rate', 0)
                risk_level = data_quality.get('risk_level', 'unknown')
                
                if missing_rate > 0.5:
                    interpretation['overall_quality_assessment'] = "æ•°æ®è´¨é‡è¾ƒå·®ï¼Œç¼ºå¤±ç‡è¿‡é«˜"
                    interpretation['quality_issues'].append(f"æ€»ä½“ç¼ºå¤±ç‡: {missing_rate:.1%}")
                elif missing_rate > 0.2:
                    interpretation['overall_quality_assessment'] = "æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œå­˜åœ¨ä¸€å®šç¼ºå¤±"
                    interpretation['quality_issues'].append(f"æ€»ä½“ç¼ºå¤±ç‡: {missing_rate:.1%}")
                else:
                    interpretation['overall_quality_assessment'] = "æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç¼ºå¤±ç‡è¾ƒä½"
                    interpretation['quality_strengths'].append(f"æ€»ä½“ç¼ºå¤±ç‡: {missing_rate:.1%}")
                
                if risk_level == 'high':
                    interpretation['quality_issues'].append("æ•°æ®è´¨é‡é£é™©ç­‰çº§: é«˜")
                elif risk_level == 'medium':
                    interpretation['quality_issues'].append("æ•°æ®è´¨é‡é£é™©ç­‰çº§: ä¸­ç­‰")
                else:
                    interpretation['quality_strengths'].append("æ•°æ®è´¨é‡é£é™©ç­‰çº§: ä½")
            
            # å¼‚å¸¸æ£€æµ‹è´¨é‡
            if 'anomalies' in insights and 'anomaly_rate' in insights['anomalies']:
                anomaly_rate = insights['anomalies']['anomaly_rate']
                if anomaly_rate > 0.1:
                    interpretation['quality_issues'].append(f"å¼‚å¸¸æ•°æ®æ¯”ä¾‹è¾ƒé«˜: {anomaly_rate:.1%}")
                else:
                    interpretation['quality_strengths'].append(f"å¼‚å¸¸æ•°æ®æ¯”ä¾‹æ­£å¸¸: {anomaly_rate:.1%}")
            
            # å»ºè®®è¡ŒåŠ¨
            if interpretation['quality_issues']:
                interpretation['recommended_actions'] = [
                    "ç«‹å³æ£€æŸ¥æ•°æ®æºå’Œæ•°æ®æ”¶é›†æµç¨‹",
                    "ä¼˜åŒ–æ•°æ®é¢„å¤„ç†æ­¥éª¤",
                    "å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶"
                ]
            else:
                interpretation['recommended_actions'] = [
                    "ç»§ç»­ç›‘æ§æ•°æ®è´¨é‡",
                    "å®šæœŸè¿›è¡Œè´¨é‡è¯„ä¼°",
                    "ä¿æŒç°æœ‰æ•°æ®ç®¡ç†æµç¨‹"
                ]
            
            return interpretation
            
        except Exception as e:
            return {'error': f'æ•°æ®è´¨é‡è§£è¯»å¤±è´¥: {e}'}
    
    def _generate_actionable_recommendations(self, interpretation: Dict) -> Dict:
        """ç”Ÿæˆå¯æ“ä½œå»ºè®®"""
        try:
            recommendations = {
                'immediate_actions': [],
                'short_term_actions': [],
                'long_term_actions': [],
                'priority_levels': {}
            }
            
            # åŸºäºé£é™©ç­‰çº§ç¡®å®šè¡ŒåŠ¨ä¼˜å…ˆçº§
            overall_risk = interpretation.get('risk_assessment', {}).get('overall_risk_assessment', 'unknown')
            
            if overall_risk == 'high':
                recommendations['immediate_actions'] = [
                    "ç«‹å³æ£€æŸ¥æ•°æ®æºå’Œæ•°æ®æ”¶é›†æµç¨‹",
                    "éªŒè¯ä¼ æ„Ÿå™¨å’Œè®¾å¤‡çŠ¶æ€",
                    "æš‚åœä½¿ç”¨æœ‰é—®é¢˜çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒ"
                ]
                recommendations['short_term_actions'] = [
                    "å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶",
                    "ä¼˜åŒ–æ•°æ®é¢„å¤„ç†æµç¨‹",
                    "åŸ¹è®­æ•°æ®ç®¡ç†äººå‘˜"
                ]
                recommendations['long_term_actions'] = [
                    "å»ºç«‹æ•°æ®è´¨é‡æ ‡å‡†å’Œæµç¨‹",
                    "å®æ–½è‡ªåŠ¨åŒ–æ•°æ®è´¨é‡æ£€æŸ¥",
                    "å»ºç«‹æ•°æ®è´¨é‡æŠ¥å‘Šæœºåˆ¶"
                ]
            elif overall_risk == 'medium':
                recommendations['immediate_actions'] = [
                    "ç›‘æ§æ•°æ®è´¨é‡è¶‹åŠ¿",
                    "æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤"
                ]
                recommendations['short_term_actions'] = [
                    "ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹",
                    "å»ºç«‹å®šæœŸè´¨é‡è¯„ä¼°æœºåˆ¶"
                ]
                recommendations['long_term_actions'] = [
                    "æŒç»­æ”¹è¿›æ•°æ®è´¨é‡",
                    "å»ºç«‹æ•°æ®è´¨é‡æ–‡åŒ–"
                ]
            else:
                recommendations['immediate_actions'] = [
                    "ç»§ç»­ç›‘æ§æ•°æ®è´¨é‡",
                    "è®°å½•æœ€ä½³å®è·µ"
                ]
                recommendations['short_term_actions'] = [
                    "å®šæœŸè¿›è¡Œè´¨é‡è¯„ä¼°",
                    "åˆ†äº«æˆåŠŸç»éªŒ"
                ]
                recommendations['long_term_actions'] = [
                    "æŒç»­ä¼˜åŒ–æ•°æ®æµç¨‹",
                    "å»ºç«‹æ•°æ®è´¨é‡æ ‡æ†"
                ]
            
            # è®¾ç½®ä¼˜å…ˆçº§
            recommendations['priority_levels'] = {
                'immediate': 'é«˜ä¼˜å…ˆçº§ - 24å°æ—¶å†…æ‰§è¡Œ',
                'short_term': 'ä¸­ä¼˜å…ˆçº§ - 1å‘¨å†…æ‰§è¡Œ',
                'long_term': 'ä½ä¼˜å…ˆçº§ - 1ä¸ªæœˆå†…æ‰§è¡Œ'
            }
            
            return recommendations
            
        except Exception as e:
            return {'error': f'ç”Ÿæˆå¯æ“ä½œå»ºè®®å¤±è´¥: {e}'}
    
    def save_insights(self, output_dir: str = "insights") -> str:
        """ä¿å­˜æ´å¯Ÿç»“æœ"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            insights_file = os.path.join(output_dir, f"insights_discovery_{timestamp}.json")
            
            with open(insights_file, 'w', encoding='utf-8') as f:
                json.dump(self.insights, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… æ´å¯Ÿç»“æœå·²ä¿å­˜: {insights_file}")
            return insights_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ´å¯Ÿç»“æœå¤±è´¥: {e}")
            return ""

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æ— ç›‘ç£æ¢ç´¢æ¨¡å—...")
        
        # åŠ è½½Environment Canadaæ•°æ®
        data_path = "data/real/environment_canada/environment_canada_merged.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return
        
        # è¯»å–æ•°æ®
        data = pd.read_csv(data_path)
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {data.shape}")
        
        # ä¼°ç®—åœŸå£¤æ¹¿åº¦
        if 'Total Precip (mm)' in data.columns and 'Temp (Â°C)' in data.columns:
            # ç®€å•çš„åœŸå£¤æ¹¿åº¦ä¼°ç®—
            data['estimated_soil_moisture'] = (
                0.3 +  # åŸºç¡€æ¹¿åº¦
                0.1 * np.log1p(data['Total Precip (mm)'].fillna(0)) +  # é™æ°´å½±å“
                0.05 * (1 - (data['Temp (Â°C)'].fillna(0) + 20) / 60)  # æ¸©åº¦å½±å“
            )
            data['estimated_soil_moisture'] = np.clip(data['estimated_soil_moisture'], 0.1, 0.9)
        
        # åˆ›å»ºæ¢ç´¢æ¨¡å—
        explorer = InsightDiscoveryModule()
        
        # å‘ç°æ¨¡å¼
        insights = explorer.discover_patterns(data)
        
        if 'status' not in insights:
            # ä¿å­˜æ´å¯Ÿç»“æœ
            insights_file = explorer.save_insights()
            
            # è§£è¯»æ´å¯Ÿç»“æœ
            interpretation = explorer.interpret_insights(insights)
            
            logger.info("ğŸ‰ æ— ç›‘ç£æ¢ç´¢å®Œæˆï¼")
            logger.info(f"ğŸ“Š å‘ç° {interpretation['executive_summary']['total_discoveries']} ç±»æ´å¯Ÿ")
            logger.info(f"âš ï¸ é£é™©è¯„ä¼°: {interpretation['risk_assessment']['overall_risk_assessment']}")
            
            # æ˜¾ç¤ºå…³é”®å‘ç°
            if 'executive_summary' in interpretation and 'key_message' in interpretation['executive_summary']:
                logger.info(f"ğŸ” {interpretation['executive_summary']['key_message']}")
            
            # æ˜¾ç¤ºå»ºè®®
            if 'actionable_recommendations' in interpretation and 'immediate_actions' in interpretation['actionable_recommendations']:
                for rec in interpretation['actionable_recommendations']['immediate_actions']:
                    logger.info(f"ğŸ’¡ {rec}")
            
            return interpretation
        else:
            logger.error(f"âŒ æ¢ç´¢å¤±è´¥: {insights}")
            return None
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()
