#!/usr/bin/env python3
"""
æ— ç›‘ç£æ¢ç´¢æ¨¡å— - å‘ç°é—®é¢˜èƒŒåçš„æ¨¡å¼
å®šä½ï¼šæ¢ç´¢ + è§£é‡Šï¼Œè¡¥å……é¢„æµ‹çš„å¯ä¿¡åº¦å’Œç†è§£
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import logging
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class InsightDiscoveryModule:
    """æ— ç›‘ç£æ¢ç´¢æ¨¡å— - å‘ç°é—®é¢˜èƒŒåçš„æ¨¡å¼"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¢ç´¢æ¨¡å—"""
        self.scaler = StandardScaler()
        self.pca = None
        self.clusters = None
        self.anomalies = None
        self.insights = {}
        
        logger.info("ğŸ” æ— ç›‘ç£æ¢ç´¢æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def discover_patterns(self, data: pd.DataFrame, target_col: str = 'estimated_soil_moisture') -> Dict:
        """å‘ç°æ•°æ®èƒŒåçš„æ¨¡å¼"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ— ç›‘ç£æ¨¡å¼å‘ç°...")
            
            # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†
            logger.info("ğŸ”§ æ­¥éª¤1: æ•°æ®é¢„å¤„ç†...")
            X_processed = self._preprocess_data(data)
            
            # æ­¥éª¤2: å¼‚å¸¸æ£€æµ‹
            logger.info("ğŸ” æ­¥éª¤2: å¼‚å¸¸æ£€æµ‹...")
            anomaly_insights = self._detect_anomalies(X_processed, data)
            
            # æ­¥éª¤3: èšç±»åˆ†æ
            logger.info("ğŸ” æ­¥éª¤3: èšç±»åˆ†æ...")
            cluster_insights = self._cluster_analysis(X_processed, data)
            
            # æ­¥éª¤4: é™ç»´å¯è§†åŒ–
            logger.info("ğŸ” æ­¥éª¤4: é™ç»´å¯è§†åŒ–...")
            dimension_insights = self._dimension_reduction(X_processed, data)
            
            # æ­¥éª¤5: æ—¶é—´æ¨¡å¼åˆ†æ
            logger.info("ğŸ” æ­¥éª¤5: æ—¶é—´æ¨¡å¼åˆ†æ...")
            temporal_insights = self._temporal_patterns(data, target_col)
            
            # æ­¥éª¤6: é£é™©æœºåˆ¶è¯†åˆ«
            logger.info("ğŸ” æ­¥éª¤6: é£é™©æœºåˆ¶è¯†åˆ«...")
            risk_insights = self._identify_risk_mechanisms(data, target_col)
            
            # æ•´åˆæ‰€æœ‰æ´å¯Ÿ
            self.insights = {
                'timestamp': datetime.now().isoformat(),
                'anomalies': anomaly_insights,
                'clusters': cluster_insights,
                'dimensions': dimension_insights,
                'temporal': temporal_insights,
                'risk_mechanisms': risk_insights
            }
            
            # æ­¥éª¤7: ç”Ÿæˆæ‘˜è¦ (åœ¨æ‰€æœ‰æ´å¯Ÿæ„å»ºå®Œæˆå)
            logger.info("ğŸ” æ­¥éª¤7: ç”Ÿæˆæ‘˜è¦...")
            summary_insights = self._generate_summary()
            self.insights['summary'] = summary_insights
            
            logger.info("ğŸ‰ æ— ç›‘ç£æ¨¡å¼å‘ç°å®Œæˆï¼")
            return self.insights
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å¼å‘ç°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _preprocess_data(self, data: pd.DataFrame) -> np.ndarray:
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç† - ä¸“é—¨å¤„ç†é«˜ç¼ºå¤±ç‡æ•°æ®"""
        try:
            # é€‰æ‹©æ•°å€¼ç‰¹å¾
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            
            # ç§»é™¤ç›®æ ‡å˜é‡
            if 'estimated_soil_moisture' in numeric_cols:
                numeric_cols.remove('estimated_soil_moisture')
            
            # åˆ†æç¼ºå¤±å€¼æ¨¡å¼
            missing_rates = data[numeric_cols].isnull().sum() / len(data)
            logger.info(f"ğŸ“Š ç¼ºå¤±å€¼åˆ†æ: æ€»ç‰¹å¾æ•° {len(numeric_cols)}")
            
            # åªä¿ç•™ç¼ºå¤±ç‡ < 50% çš„ç‰¹å¾
            valid_features = missing_rates[missing_rates < 0.5].index.tolist()
            high_missing_features = missing_rates[missing_rates >= 0.5].index.tolist()
            
            logger.info(f"âœ… æœ‰æ•ˆç‰¹å¾: {len(valid_features)} ä¸ª (ç¼ºå¤±ç‡ < 50%)")
            logger.info(f"âš ï¸ é«˜ç¼ºå¤±ç‰¹å¾: {len(high_missing_features)} ä¸ª (ç¼ºå¤±ç‡ >= 50%)")
            
            if len(valid_features) == 0:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œå°è¯•æ”¾å®½æ ‡å‡†åˆ°80%ç¼ºå¤±ç‡")
                valid_features = missing_rates[missing_rates < 0.8].index.tolist()
                if len(valid_features) == 0:
                    logger.error("âŒ æ‰€æœ‰ç‰¹å¾ç¼ºå¤±ç‡éƒ½è¿‡é«˜ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
                    return np.array([])
            
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„ç¼ºå¤±å€¼å¡«å……
            X = data[valid_features].copy()
            
            # å¯¹äºè¿ç»­å˜é‡ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    median_val = X[col].median()
                    if pd.isna(median_val):
                        # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼Œä½¿ç”¨0å¡«å……
                        X[col] = X[col].fillna(0)
                    else:
                        X[col] = X[col].fillna(median_val)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNå€¼
            remaining_nans = X.isnull().sum().sum()
            if remaining_nans > 0:
                logger.warning(f"âš ï¸ ä»æœ‰ {remaining_nans} ä¸ªNaNå€¼ï¼Œä½¿ç”¨0å¡«å……")
                X = X.fillna(0)
            
            # æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            logger.info(f"âœ… æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å®Œæˆ: {X.shape[1]} ä¸ªæœ‰æ•ˆç‰¹å¾")
            logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X_scaled.shape}")
            
            return X_scaled
            
        except Exception as e:
            logger.error(f"âŒ æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return np.array([])
    
    def _detect_anomalies(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """å¼‚å¸¸æ£€æµ‹"""
        try:
            # ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(X)
            
            # ç»Ÿè®¡å¼‚å¸¸
            anomaly_count = np.sum(anomaly_labels == -1)
            total_count = len(anomaly_labels)
            anomaly_rate = anomaly_count / total_count
            
            # åˆ†æå¼‚å¸¸ç‰¹å¾
            anomaly_data = data[anomaly_labels == -1]
            normal_data = data[anomaly_labels == 1]
            
            anomaly_insights = {
                'anomaly_count': int(anomaly_count),
                'anomaly_rate': float(anomaly_rate),
                'total_count': int(total_count),
                'anomaly_features': self._analyze_anomaly_features(anomaly_data, normal_data),
                'anomaly_timestamps': anomaly_data['timestamp'].tolist() if 'timestamp' in anomaly_data.columns else []
            }
            
            logger.info(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {anomaly_count} ä¸ªå¼‚å¸¸ ({anomaly_rate:.1%})")
            return anomaly_insights
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_anomaly_features(self, anomaly_data: pd.DataFrame, normal_data: pd.DataFrame) -> Dict:
        """åˆ†æå¼‚å¸¸ç‰¹å¾"""
        try:
            numeric_cols = anomaly_data.select_dtypes(include=[np.number]).columns.tolist()
            
            feature_analysis = {}
            for col in numeric_cols:
                if col in normal_data.columns:
                    anomaly_mean = anomaly_data[col].mean()
                    normal_mean = normal_data[col].mean()
                    difference = anomaly_mean - normal_mean
                    
                    feature_analysis[col] = {
                        'anomaly_mean': float(anomaly_mean),
                        'normal_mean': float(normal_mean),
                        'difference': float(difference),
                        'deviation': float(abs(difference) / normal_mean) if normal_mean != 0 else 0
                    }
            
            return feature_analysis
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _cluster_analysis(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """æ”¹è¿›çš„èšç±»åˆ†æ - å¢åŠ ç¼ºå¤±å€¼æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
            if np.isnan(X).any():
                logger.error("âŒ è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†'
                }
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if X.size == 0:
                logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤'
                }
            
            # ç¡®å®šæœ€ä½³èšç±»æ•°
            silhouette_scores = []
            K_range = range(2, min(11, len(X) // 10 + 1))
            
            if len(K_range) == 0:
                logger.warning("âš ï¸ æ•°æ®é‡è¿‡å°‘ï¼Œä½¿ç”¨é»˜è®¤èšç±»æ•°2")
                K_range = [2]
            
            for k in K_range:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(X)
                    score = silhouette_score(X, cluster_labels)
                    silhouette_scores.append(score)
                except Exception as e:
                    logger.warning(f"âš ï¸ èšç±»æ•° {k} å¤±è´¥: {e}")
                    silhouette_scores.append(-1)
            
            if not silhouette_scores or max(silhouette_scores) == -1:
                logger.warning("âš ï¸ æ‰€æœ‰èšç±»æ•°éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤èšç±»æ•°2")
                best_k = 2
                best_score = 0
            else:
                best_k = K_range[np.argmax(silhouette_scores)]
                best_score = max(silhouette_scores)
            
            # æ‰§è¡Œæœ€ä½³èšç±»
            try:
                kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(X)
                
                # åˆ†æèšç±»ç‰¹å¾
                data_with_clusters = data.copy()
                data_with_clusters['cluster'] = cluster_labels
                
                cluster_insights = {
                    'optimal_clusters': int(best_k),
                    'silhouette_score': float(best_score),
                    'cluster_sizes': data_with_clusters['cluster'].value_counts().to_dict(),
                    'cluster_characteristics': self._analyze_cluster_characteristics(data_with_clusters)
                }
                
                self.clusters = cluster_labels
                
                logger.info(f"âœ… èšç±»åˆ†æå®Œæˆ: æœ€ä½³èšç±»æ•° {best_k}, è½®å»“ç³»æ•° {best_score:.3f}")
                return cluster_insights
                
            except Exception as e:
                logger.error(f"âŒ æœ€ç»ˆèšç±»å¤±è´¥: {e}")
                return {'status': 'error', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"âŒ èšç±»åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_cluster_characteristics(self, data: pd.DataFrame) -> Dict:
        """åˆ†æèšç±»ç‰¹å¾"""
        try:
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            if 'cluster' in numeric_cols:
                numeric_cols.remove('cluster')
            
            cluster_chars = {}
            for cluster_id in data['cluster'].unique():
                cluster_data = data[data['cluster'] == cluster_id]
                
                cluster_profile = {}
                for col in numeric_cols:
                    cluster_profile[col] = {
                        'mean': float(cluster_data[col].mean()),
                        'std': float(cluster_data[col].std()),
                        'min': float(cluster_data[col].min()),
                        'max': float(cluster_data[col].max())
                    }
                
                cluster_chars[f'cluster_{cluster_id}'] = {
                    'size': int(len(cluster_data)),
                    'profile': cluster_profile
                }
            
            return cluster_chars
            
        except Exception as e:
            logger.error(f"âŒ èšç±»ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _dimension_reduction(self, X: np.ndarray, data: pd.DataFrame) -> Dict:
        """æ”¹è¿›çš„é™ç»´åˆ†æ - å¢åŠ ç¼ºå¤±å€¼æ£€æŸ¥"""
        try:
            # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaN
            if np.isnan(X).any():
                logger.error("âŒ è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†'
                }
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if X.size == 0:
                logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return {
                    'status': 'error', 
                    'error': 'è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤'
                }
            
            # æ£€æŸ¥ç‰¹å¾æ•°é‡
            if X.shape[1] < 2:
                logger.warning("âš ï¸ ç‰¹å¾æ•°é‡å°‘äº2ï¼Œæ— æ³•è¿›è¡ŒPCAé™ç»´")
                return {
                    'status': 'warning',
                    'message': 'ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡PCAé™ç»´'
                }
            
            # PCAé™ç»´
            n_components = min(3, X.shape[1])
            self.pca = PCA(n_components=n_components)
            
            try:
                X_pca = self.pca.fit_transform(X)
                
                # åˆ†æä¸»æˆåˆ†
                explained_variance = self.pca.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                
                # ç‰¹å¾é‡è¦æ€§
                feature_importance = {}
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                
                for i, component in enumerate(self.pca.components_):
                    for j, importance in enumerate(component):
                        if j < len(numeric_cols):
                            col_name = numeric_cols[j]
                            if col_name not in feature_importance:
                                feature_importance[col_name] = []
                            feature_importance[col_name].append(float(importance))
                
                dimension_insights = {
                    'n_components': int(n_components),
                    'explained_variance': explained_variance.tolist(),
                    'cumulative_variance': cumulative_variance.tolist(),
                    'feature_importance': feature_importance,
                    'pca_data': X_pca.tolist()
                }
                
                logger.info(f"âœ… é™ç»´åˆ†æå®Œæˆ: ä¿ç•™ {n_components} ä¸ªä¸»æˆåˆ†")
                logger.info(f"ğŸ“Š è§£é‡Šæ–¹å·®: {explained_variance}")
                logger.info(f"ğŸ“Š ç´¯ç§¯æ–¹å·®: {cumulative_variance}")
                
                return dimension_insights
                
            except Exception as e:
                logger.error(f"âŒ PCAè®¡ç®—å¤±è´¥: {e}")
                return {'status': 'error', 'error': str(e)}
            
        except Exception as e:
            logger.error(f"âŒ é™ç»´åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _temporal_patterns(self, data: pd.DataFrame, target_col: str) -> Dict:
        """æ”¹è¿›çš„æ—¶é—´æ¨¡å¼åˆ†æ - å¢åŠ æ•°æ®åˆ—æ£€æŸ¥"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç›¸å…³åˆ—
            time_columns = []
            if 'Date/Time' in data.columns:
                time_columns.append('Date/Time')
            if 'Year' in data.columns:
                time_columns.append('Year')
            if 'Month' in data.columns:
                time_columns.append('Month')
            if 'Day' in data.columns:
                time_columns.append('Day')
            
            if not time_columns:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œè·³è¿‡æ—¶é—´æ¨¡å¼åˆ†æ")
                return {
                    'status': 'warning',
                    'message': 'æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—ï¼Œè·³è¿‡æ—¶é—´æ¨¡å¼åˆ†æ'
                }
            
            # æ£€æŸ¥ç›®æ ‡åˆ—
            if target_col not in data.columns:
                logger.warning(f"âš ï¸ ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—")
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    target_col = numeric_cols[0]
                else:
                    logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´æ¨¡å¼åˆ†æ")
                    return {
                        'status': 'error',
                        'error': 'æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´æ¨¡å¼åˆ†æ'
                    }
            
            # æ—¶é—´æ¨¡å¼åˆ†æ
            temporal_insights = {
                'time_columns_found': time_columns,
                'target_column': target_col,
                'patterns': {}
            }
            
            # å¹´åº¦æ¨¡å¼
            if 'Year' in data.columns:
                yearly_stats = data.groupby('Year')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['yearly'] = {
                    'yearly_stats': yearly_stats.to_dict('records'),
                    'yearly_trend': 'stable'  # ç®€åŒ–å¤„ç†
                }
            
            # æœˆåº¦æ¨¡å¼
            if 'Month' in data.columns:
                monthly_stats = data.groupby('Month')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['monthly'] = {
                    'monthly_stats': monthly_stats.to_dict('records'),
                    'seasonal_pattern': 'detected'  # ç®€åŒ–å¤„ç†
                }
            
            # æ—¥æ¨¡å¼
            if 'Day' in data.columns:
                daily_stats = data.groupby('Day')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()
                temporal_insights['patterns']['daily'] = {
                    'daily_stats': daily_stats.to_dict('records')
                }
            
            logger.info(f"âœ… æ—¶é—´æ¨¡å¼åˆ†æå®Œæˆ: åˆ†æäº† {len(time_columns)} ä¸ªæ—¶é—´åˆ—")
            return temporal_insights
            
        except Exception as e:
            logger.error(f"âŒ æ—¶é—´æ¨¡å¼åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_seasonal_trends(self, data: pd.DataFrame, target_col: str) -> Dict:
        """åˆ†æå­£èŠ‚æ€§è¶‹åŠ¿"""
        try:
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            data_sorted = data.sort_values('timestamp')
            data_sorted[f'{target_col}_ma7'] = data_sorted[target_col].rolling(window=7).mean()
            data_sorted[f'{target_col}_ma30'] = data_sorted[target_col].rolling(window=30).mean()
            
            # å­£èŠ‚æ€§ç»Ÿè®¡
            seasonal_stats = data_sorted.groupby('month')[target_col].agg(['mean', 'std', 'min', 'max']).to_dict()
            
            return {
                'seasonal_stats': seasonal_stats,
                'trend_data': {
                    'ma7': data_sorted[f'{target_col}_ma7'].dropna().tolist(),
                    'ma30': data_sorted[f'{target_col}_ma30'].dropna().tolist()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ å­£èŠ‚æ€§è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _identify_risk_mechanisms(self, data: pd.DataFrame, target_col: str) -> Dict:
        """è¯†åˆ«é£é™©æœºåˆ¶"""
        try:
            risk_mechanisms = {}
            
            # 1. æç«¯å€¼é£é™©
            if target_col in data.columns:
                target_data = data[target_col].dropna()
                q1, q3 = target_data.quantile([0.25, 0.75])
                iqr = q3 - q1
                extreme_threshold = 1.5 * iqr
                
                extreme_low = q1 - extreme_threshold
                extreme_high = q3 + extreme_threshold
                
                extreme_events = data[
                    (data[target_col] < extreme_low) | 
                    (data[target_col] > extreme_high)
                ]
                
                risk_mechanisms['extreme_values'] = {
                    'threshold_low': float(extreme_low),
                    'threshold_high': float(extreme_high),
                    'extreme_count': int(len(extreme_events)),
                    'risk_level': 'high' if len(extreme_events) > len(data) * 0.1 else 'medium'
                }
            
            # 2. æ•°æ®è´¨é‡é£é™©
            missing_rates = data.isnull().sum() / len(data)
            high_missing_features = missing_rates[missing_rates > 0.1].index.tolist()
            
            risk_mechanisms['data_quality'] = {
                'high_missing_features': high_missing_features,
                'overall_missing_rate': float(data.isnull().sum().sum() / (len(data) * len(data.columns))),
                'risk_level': 'high' if len(high_missing_features) > 0 else 'low'
            }
            
            # 3. æ—¶é—´è¿ç»­æ€§é£é™©
            if 'timestamp' in data.columns:
                data_sorted = data.sort_values('timestamp')
                time_gaps = data_sorted['timestamp'].diff().dt.total_seconds() / 3600  # å°æ—¶
                large_gaps = time_gaps[time_gaps > 24]  # è¶…è¿‡24å°æ—¶çš„é—´éš”
                
                risk_mechanisms['temporal_continuity'] = {
                    'large_gaps_count': int(len(large_gaps)),
                    'max_gap_hours': float(large_gaps.max()) if len(large_gaps) > 0 else 0,
                    'risk_level': 'high' if len(large_gaps) > 0 else 'low'
                }
            
            logger.info("âœ… é£é™©æœºåˆ¶è¯†åˆ«å®Œæˆ")
            return risk_mechanisms
            
        except Exception as e:
            logger.error(f"âŒ é£é™©æœºåˆ¶è¯†åˆ«å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _generate_summary(self) -> Dict:
        """æ”¹è¿›çš„æ‘˜è¦ç”Ÿæˆ - å¤„ç†æ–°çš„åˆ†æç»“æœæ ¼å¼"""
        try:
            summary = {
                'total_insights': 0,
                'key_findings': [],
                'risk_assessment': 'unknown',
                'recommendations': []
            }
            
            # ç»Ÿè®¡æ´å¯Ÿæ•°é‡
            insight_count = 0
            
            # å¼‚å¸¸æ£€æµ‹æ´å¯Ÿ
            if 'anomalies' in self.insights and 'anomaly_count' in self.insights['anomalies']:
                insight_count += 1
                anomaly_rate = self.insights['anomalies'].get('anomaly_rate', 0)
                if anomaly_rate > 0.1:
                    summary['key_findings'].append(f"å‘ç°å¼‚å¸¸æ•°æ®æ¯”ä¾‹è¾ƒé«˜: {anomaly_rate:.1%}")
                else:
                    summary['key_findings'].append(f"å¼‚å¸¸æ£€æµ‹æ­£å¸¸: {anomaly_rate:.1%}")
            
            # èšç±»åˆ†ææ´å¯Ÿ
            if 'clusters' in self.insights and 'optimal_clusters' in self.insights['clusters']:
                insight_count += 1
                optimal_clusters = self.insights['clusters'].get('optimal_clusters', 0)
                silhouette_score = self.insights['clusters'].get('silhouette_score', 0)
                summary['key_findings'].append(f"èšç±»åˆ†æå®Œæˆ: æœ€ä½³èšç±»æ•° {optimal_clusters}, è½®å»“ç³»æ•° {silhouette_score:.3f}")
            elif 'clusters' in self.insights and self.insights['clusters'].get('status') == 'warning':
                summary['key_findings'].append(f"èšç±»åˆ†æ: {self.insights['clusters'].get('message', 'è­¦å‘Š')}")
            
            # é™ç»´åˆ†ææ´å¯Ÿ
            if 'dimensions' in self.insights and 'n_components' in self.insights['dimensions']:
                insight_count += 1
                n_components = self.insights['dimensions'].get('n_components', 0)
                cumulative_variance = self.insights['dimensions'].get('cumulative_variance', [])
                if cumulative_variance:
                    total_variance = cumulative_variance[-1] if cumulative_variance else 0
                    summary['key_findings'].append(f"é™ç»´åˆ†æå®Œæˆ: {n_components} ä¸ªä¸»æˆåˆ†è§£é‡Š {total_variance:.1%} çš„æ–¹å·®")
            elif 'dimensions' in self.insights and self.insights['dimensions'].get('status') == 'warning':
                summary['key_findings'].append(f"é™ç»´åˆ†æ: {self.insights['dimensions'].get('message', 'è­¦å‘Š')}")
            
            # æ—¶é—´æ¨¡å¼æ´å¯Ÿ
            if 'temporal' in self.insights and 'time_columns_found' in self.insights['temporal']:
                insight_count += 1
                time_columns = self.insights['temporal'].get('time_columns_found', [])
                summary['key_findings'].append(f"æ—¶é—´æ¨¡å¼åˆ†æå®Œæˆ: åˆ†æäº† {len(time_columns)} ä¸ªæ—¶é—´ç»´åº¦")
            elif 'temporal' in self.insights and self.insights['temporal'].get('status') == 'warning':
                summary['key_findings'].append(f"æ—¶é—´æ¨¡å¼åˆ†æ: {self.insights['temporal'].get('message', 'è­¦å‘Š')}")
            
            # é£é™©æœºåˆ¶æ´å¯Ÿ
            if 'risk_mechanisms' in self.insights:
                data_quality = self.insights['risk_mechanisms'].get('data_quality', {})
                missing_rate = data_quality.get('overall_missing_rate', 0)
                risk_level = data_quality.get('risk_level', 'unknown')
                
                summary['key_findings'].append(f"æ•°æ®ç¼ºå¤±ç‡: {missing_rate:.1%}")
                summary['risk_assessment'] = risk_level
                
                if missing_rate > 0.5:
                    summary['key_findings'].append("æ•°æ®è´¨é‡é£é™©è¾ƒé«˜ï¼Œå»ºè®®æ”¹å–„æ•°æ®æ”¶é›†")
                    summary['recommendations'].append("å»ºè®®ç«‹å³æ£€æŸ¥æ•°æ®è´¨é‡å’Œå¼‚å¸¸å€¼")
                    summary['recommendations'].append("è€ƒè™‘å¢åŠ æ•°æ®éªŒè¯æœºåˆ¶")
                elif missing_rate > 0.2:
                    summary['key_findings'].append("æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦å…³æ³¨")
                    summary['recommendations'].append("å»ºè®®å®šæœŸç›‘æ§æ•°æ®è´¨é‡")
                    summary['recommendations'].append("ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹")
                else:
                    summary['key_findings'].append("æ•°æ®è´¨é‡è‰¯å¥½")
                    summary['recommendations'].append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç»§ç»­ç°æœ‰æµç¨‹")
                    summary['recommendations'].append("å»ºè®®å®šæœŸè¿›è¡Œæ¨¡å¼åˆ†æ")
            
            summary['total_insights'] = insight_count
            
            # å¦‚æœæ²¡æœ‰å…³é”®å‘ç°ï¼Œæ·»åŠ é»˜è®¤ä¿¡æ¯
            if not summary['key_findings']:
                summary['key_findings'].append("æ•°æ®æ¢ç´¢å®Œæˆï¼Œä½†å‘ç°æœ‰é™")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return {
                'status': 'error', 
                'error': str(e),
                'total_insights': 0,
                'key_findings': ['æ‘˜è¦ç”Ÿæˆå¤±è´¥'],
                'risk_assessment': 'unknown',
                'recommendations': ['è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€']
            }
    
    def save_insights(self, output_dir: str = "insights") -> str:
        """ä¿å­˜æ´å¯Ÿç»“æœ"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            insights_file = os.path.join(output_dir, f"insights_discovery_{timestamp}.json")
            
            with open(insights_file, 'w', encoding='utf-8') as f:
                json.dump(self.insights, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… æ´å¯Ÿç»“æœå·²ä¿å­˜: {insights_file}")
            return insights_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ´å¯Ÿç»“æœå¤±è´¥: {e}")
            return ""

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æ— ç›‘ç£æ¢ç´¢æ¨¡å—...")
        
        # åŠ è½½Environment Canadaæ•°æ®
        data_path = "data/real/environment_canada/environment_canada_merged.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return
        
        # è¯»å–æ•°æ®
        data = pd.read_csv(data_path)
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {data.shape}")
        
        # ä¼°ç®—åœŸå£¤æ¹¿åº¦
        if 'Total Precip (mm)' in data.columns and 'Temp (Â°C)' in data.columns:
            # ç®€å•çš„åœŸå£¤æ¹¿åº¦ä¼°ç®—
            data['estimated_soil_moisture'] = (
                0.3 +  # åŸºç¡€æ¹¿åº¦
                0.1 * np.log1p(data['Total Precip (mm)'].fillna(0)) +  # é™æ°´å½±å“
                0.05 * (1 - (data['Temp (Â°C)'].fillna(0) + 20) / 60)  # æ¸©åº¦å½±å“
            )
            data['estimated_soil_moisture'] = np.clip(data['estimated_soil_moisture'], 0.1, 0.9)
        
        # åˆ›å»ºæ¢ç´¢æ¨¡å—
        explorer = InsightDiscoveryModule()
        
        # å‘ç°æ¨¡å¼
        insights = explorer.discover_patterns(data)
        
        if 'status' not in insights:
            # ä¿å­˜æ´å¯Ÿç»“æœ
            insights_file = explorer.save_insights()
            
            logger.info("ğŸ‰ æ— ç›‘ç£æ¢ç´¢å®Œæˆï¼")
            logger.info(f"ğŸ“Š å‘ç° {insights['summary']['total_insights']} ç±»æ´å¯Ÿ")
            logger.info(f"âš ï¸ é£é™©è¯„ä¼°: {insights['summary']['risk_assessment']}")
            
            # æ˜¾ç¤ºå…³é”®å‘ç°
            for finding in insights['summary']['key_findings']:
                logger.info(f"ğŸ” {finding}")
            
            # æ˜¾ç¤ºå»ºè®®
            for rec in insights['summary']['recommendations']:
                logger.info(f"ğŸ’¡ {rec}")
            
            return insights
        else:
            logger.error(f"âŒ æ¢ç´¢å¤±è´¥: {insights}")
            return None
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    main()
