#!/usr/bin/env python3
"""
æ´ªæ°´é¢„è­¦ç³»ç»Ÿå†å²æ•°æ®äº¤å‰éªŒè¯æµ‹è¯•
ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ´ªæ°´é£é™©è¯„ä¼°æ¨¡å‹
from .flood_risk_assessment import FloodRiskAssessment

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FloodRiskCrossValidator:
    """æ´ªæ°´é£é™©äº¤å‰éªŒè¯å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–äº¤å‰éªŒè¯å™¨"""
        self.risk_assessor = FloodRiskAssessment(config_path)
        self.validation_results = []
        self.performance_metrics = {}
        
        logger.info("æ´ªæ°´é£é™©äº¤å‰éªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_historical_data(self, data_path: str) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        logger.info(f"åŠ è½½å†å²æ•°æ®: {data_path}")
        
        try:
            if data_path.endswith('.csv'):
                df = pd.read_csv(data_path)
            elif data_path.endswith('.json'):
                df = pd.read_json(data_path)
            else:
                raise ValueError("ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVæˆ–JSON")
            
            # æ•°æ®é¢„å¤„ç†
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            required_columns = ['date', 'station_id', 'flow_value']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
            
            logger.info(f"å†å²æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
            logger.info(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
            logger.info(f"ç«™ç‚¹æ•°é‡: {df['station_id'].nunique()}")
            
            return df
            
        except Exception as e:
            logger.error(f"åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
            raise
    
    def generate_forecast_scenarios(self, 
                                  historical_flows: List[float], 
                                  forecast_horizon: int = 7) -> List[List[float]]:
        """ç”Ÿæˆé¢„æµ‹åœºæ™¯"""
        scenarios = []
        
        for i in range(len(historical_flows) - forecast_horizon):
            # ä½¿ç”¨å†å²æ•°æ®ä½œä¸ºé¢„æµ‹
            scenario = historical_flows[i:i + forecast_horizon]
            scenarios.append(scenario)
        
        return scenarios
    
    def time_series_cross_validation(self, 
                                   data: pd.DataFrame,
                                   station_id: str,
                                   validation_windows: int = 5,
                                   forecast_horizon: int = 7,
                                   min_training_size: int = 30) -> Dict:
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        logger.info(f"å¼€å§‹ç«™ç‚¹ {station_id} çš„æ—¶é—´åºåˆ—äº¤å‰éªŒè¯...")
        
        # ç­›é€‰ç«™ç‚¹æ•°æ®
        station_data = data[data['station_id'] == station_id].copy()
        if len(station_data) < min_training_size + forecast_horizon:
            logger.warning(f"ç«™ç‚¹ {station_id} æ•°æ®ä¸è¶³ï¼Œè·³è¿‡éªŒè¯")
            return {"error": "æ•°æ®ä¸è¶³"}
        
        # æŒ‰æ—¶é—´æ’åº
        station_data = station_data.sort_values('date').reset_index(drop=True)
        flows = station_data['flow_value'].values
        dates = station_data['date'].values
        
        # è®¡ç®—éªŒè¯çª—å£å¤§å°
        total_size = len(flows)
        window_size = (total_size - min_training_size) // validation_windows
        
        validation_results = []
        
        for window in range(validation_windows):
            # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯çš„èµ·å§‹ä½ç½®
            train_start = window * window_size
            train_end = min_training_size + window * window_size
            val_start = train_end
            val_end = min(val_start + forecast_horizon, total_size)
            
            if val_end <= val_start:
                break
            
            # è®­ç»ƒæ•°æ®
            train_flows = flows[train_start:train_end]
            train_dates = dates[train_start:train_end]
            
            # éªŒè¯æ•°æ®
            val_flows = flows[val_start:val_end]
            val_dates = dates[val_start:val_end]
            
            # ç”Ÿæˆé¢„æµ‹åœºæ™¯
            forecast_scenarios = self.generate_forecast_scenarios(
                train_flows, forecast_horizon
            )
            
            # æ‰§è¡Œé£é™©è¯„ä¼°
            try:
                # ä½¿ç”¨æœ€åä¸€ä¸ªè®­ç»ƒæ•°æ®ä½œä¸ºå½“å‰æµé‡
                current_flow = train_flows[-1]
                
                # é€‰æ‹©æœ€ä½³é¢„æµ‹åœºæ™¯ï¼ˆåŸºäºå†å²æ¨¡å¼ï¼‰
                best_scenario = self._select_best_scenario(train_flows, forecast_scenarios)
                
                # æ‰§è¡Œé£é™©è¯„ä¼°
                assessment = self.risk_assessor.assess_risk(
                    station_id=station_id,
                    current_flow=current_flow,
                    forecast_flows=best_scenario,
                    forecast_hours=list(range(6, 6 + len(best_scenario) * 6, 6))
                )
                
                # è®¡ç®—é¢„æµ‹è¯¯å·®
                if len(best_scenario) == len(val_flows):
                    mse = mean_squared_error(val_flows, best_scenario)
                    mae = mean_absolute_error(val_flows, best_scenario)
                    r2 = r2_score(val_flows, best_scenario)
                else:
                    mse = mae = r2 = np.nan
                
                # è®°å½•éªŒè¯ç»“æœ
                window_result = {
                    "window": window,
                    "train_start": train_dates[0],
                    "train_end": train_dates[-1],
                    "val_start": val_dates[0],
                    "val_end": val_dates[-1],
                    "current_flow": current_flow,
                    "predicted_flows": best_scenario,
                    "actual_flows": val_flows.tolist(),
                    "risk_assessment": assessment,
                    "prediction_metrics": {
                        "mse": mse,
                        "mae": mae,
                        "r2": r2
                    }
                }
                
                validation_results.append(window_result)
                
                logger.info(f"  çª—å£ {window}: è®­ç»ƒ {len(train_flows)} å¤©, éªŒè¯ {len(val_flows)} å¤©")
                
            except Exception as e:
                logger.error(f"çª—å£ {window} éªŒè¯å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡
        overall_metrics = self._calculate_overall_metrics(validation_results)
        
        return {
            "station_id": station_id,
            "validation_windows": len(validation_results),
            "forecast_horizon": forecast_horizon,
            "results": validation_results,
            "overall_metrics": overall_metrics
        }
    
    def _select_best_scenario(self, 
                             historical_flows: np.ndarray, 
                             scenarios: List[List[float]]) -> List[float]:
        """é€‰æ‹©æœ€ä½³é¢„æµ‹åœºæ™¯"""
        if not scenarios:
            return []
        
        # è®¡ç®—å†å²æµé‡å˜åŒ–æ¨¡å¼
        historical_changes = np.diff(historical_flows)
        historical_pattern = np.mean(historical_changes)
        historical_volatility = np.std(historical_changes)
        
        # è®¡ç®—æ¯ä¸ªåœºæ™¯çš„ç›¸ä¼¼åº¦åˆ†æ•°
        scenario_scores = []
        
        for scenario in scenarios:
            if len(scenario) < 2:
                scenario_scores.append(0)
                continue
            
            # è®¡ç®—åœºæ™¯å˜åŒ–æ¨¡å¼
            scenario_changes = np.diff(scenario)
            scenario_pattern = np.mean(scenario_changes)
            scenario_volatility = np.std(scenario_changes)
            
            # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆåŸºäºå˜åŒ–æ¨¡å¼å’Œæ³¢åŠ¨æ€§ï¼‰
            pattern_similarity = 1 / (1 + abs(scenario_pattern - historical_pattern))
            volatility_similarity = 1 / (1 + abs(scenario_volatility - historical_volatility))
            
            # ç»¼åˆåˆ†æ•°
            score = (pattern_similarity + volatility_similarity) / 2
            scenario_scores.append(score)
        
        # é€‰æ‹©æœ€é«˜åˆ†æ•°çš„åœºæ™¯
        best_index = np.argmax(scenario_scores)
        return scenarios[best_index]
    
    def _calculate_overall_metrics(self, validation_results: List[Dict]) -> Dict:
        """è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
        if not validation_results:
            return {}
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹æŒ‡æ ‡
        mse_values = []
        mae_values = []
        r2_values = []
        risk_scores = []
        risk_levels = []
        
        for result in validation_results:
            if "prediction_metrics" in result:
                metrics = result["prediction_metrics"]
                if not np.isnan(metrics["mse"]):
                    mse_values.append(metrics["mse"])
                    mae_values.append(metrics["mae"])
                    r2_values.append(metrics["r2"])
            
            if "risk_assessment" in result:
                risk_scores.append(result["risk_assessment"]["risk_score"])
                risk_levels.append(result["risk_assessment"]["risk_level"])
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        overall_metrics = {
            "prediction_performance": {
                "mse_mean": np.mean(mse_values) if mse_values else np.nan,
                "mse_std": np.std(mse_values) if mse_values else np.nan,
                "mae_mean": np.mean(mae_values) if mae_values else np.nan,
                "mae_std": np.std(mae_values) if mae_values else np.nan,
                "r2_mean": np.mean(r2_values) if r2_values else np.nan,
                "r2_std": np.std(r2_values) if r2_values else np.nan
            },
            "risk_assessment_performance": {
                "risk_score_mean": np.mean(risk_scores) if risk_scores else np.nan,
                "risk_score_std": np.std(risk_scores) if risk_scores else np.nan,
                "risk_level_distribution": pd.Series(risk_levels).value_counts().to_dict() if risk_levels else {}
            },
            "validation_summary": {
                "total_windows": len(validation_results),
                "successful_windows": len([r for r in validation_results if "error" not in r]),
                "failed_windows": len([r for r in validation_results if "error" in r])
            }
        }
        
        return overall_metrics
    
    def run_cross_validation(self, 
                           data_path: str,
                           stations: Optional[List[str]] = None,
                           validation_windows: int = 5,
                           forecast_horizon: int = 7) -> Dict:
        """è¿è¡Œå®Œæ•´çš„äº¤å‰éªŒè¯"""
        logger.info("å¼€å§‹æ´ªæ°´é£é™©äº¤å‰éªŒè¯...")
        
        # åŠ è½½å†å²æ•°æ®
        data = self.load_historical_data(data_path)
        
        # ç¡®å®šè¦éªŒè¯çš„ç«™ç‚¹
        if stations is None:
            stations = data['station_id'].unique().tolist()
        
        # æ‰§è¡Œäº¤å‰éªŒè¯
        all_results = {}
        
        for station_id in stations:
            logger.info(f"éªŒè¯ç«™ç‚¹: {station_id}")
            
            try:
                station_result = self.time_series_cross_validation(
                    data=data,
                    station_id=station_id,
                    validation_windows=validation_windows,
                    forecast_horizon=forecast_horizon
                )
                
                all_results[station_id] = station_result
                
            except Exception as e:
                logger.error(f"ç«™ç‚¹ {station_id} éªŒè¯å¤±è´¥: {e}")
                all_results[station_id] = {"error": str(e)}
        
        # è®¡ç®—æ•´ä½“æ€§èƒ½
        overall_performance = self._calculate_cross_station_metrics(all_results)
        
        # ä¿å­˜éªŒè¯ç»“æœ
        self.validation_results = all_results
        self.performance_metrics = overall_performance
        
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        validation_report = {
            "validation_time": datetime.now().isoformat(),
            "data_source": data_path,
            "validation_parameters": {
                "validation_windows": validation_windows,
                "forecast_horizon": forecast_horizon,
                "stations": stations
            },
            "station_results": all_results,
            "overall_performance": overall_performance
        }
        
        logger.info("äº¤å‰éªŒè¯å®Œæˆ")
        return validation_report
    
    def _calculate_cross_station_metrics(self, station_results: Dict) -> Dict:
        """è®¡ç®—è·¨ç«™ç‚¹æ€§èƒ½æŒ‡æ ‡"""
        successful_stations = [s for s, r in station_results.items() if "error" not in r]
        
        if not successful_stations:
            return {"error": "æ‰€æœ‰ç«™ç‚¹éªŒè¯éƒ½å¤±è´¥"}
        
        # æ”¶é›†æ‰€æœ‰ç«™ç‚¹çš„æ€§èƒ½æŒ‡æ ‡
        all_mse = []
        all_mae = []
        all_r2 = []
        all_risk_scores = []
        
        for station_id in successful_stations:
            result = station_results[station_id]
            if "overall_metrics" in result:
                metrics = result["overall_metrics"]
                
                if "prediction_performance" in metrics:
                    pred_metrics = metrics["prediction_performance"]
                    if not np.isnan(pred_metrics["mse_mean"]):
                        all_mse.append(pred_metrics["mse_mean"])
                        all_mae.append(pred_metrics["mae_mean"])
                        all_r2.append(pred_metrics["r2_mean"])
                
                if "risk_assessment_performance" in metrics:
                    risk_metrics = metrics["risk_assessment_performance"]
                    if not np.isnan(risk_metrics["risk_score_mean"]):
                        all_risk_scores.append(risk_metrics["risk_score_mean"])
        
        # è®¡ç®—è·¨ç«™ç‚¹ç»Ÿè®¡
        cross_station_metrics = {
            "prediction_performance": {
                "mse_mean": np.mean(all_mse) if all_mse else np.nan,
                "mse_std": np.std(all_mse) if all_mse else np.nan,
                "mae_mean": np.mean(all_mae) if all_mae else np.nan,
                "mae_std": np.std(all_mae) if all_mae else np.nan,
                "r2_mean": np.mean(all_r2) if all_r2 else np.nan,
                "r2_std": np.std(all_r2) if all_r2 else np.nan
            },
            "risk_assessment_performance": {
                "risk_score_mean": np.mean(all_risk_scores) if all_risk_scores else np.nan,
                "risk_score_std": np.std(all_risk_scores) if all_risk_scores else np.nan
            },
            "validation_summary": {
                "total_stations": len(station_results),
                "successful_stations": len(successful_stations),
                "failed_stations": len(station_results) - len(successful_stations),
                "success_rate": len(successful_stations) / len(station_results)
            }
        }
        
        return cross_station_metrics
    
    def generate_validation_report(self, output_path: str):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        if not self.validation_results:
            logger.warning("æ²¡æœ‰éªŒè¯ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report = {
            "validation_summary": {
                "timestamp": datetime.now().isoformat(),
                "total_stations": len(self.validation_results),
                "overall_performance": self.performance_metrics
            },
            "detailed_results": self.validation_results
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def plot_validation_results(self, output_dir: str = "data/processed/validation_plots"):
        """ç»˜åˆ¶éªŒè¯ç»“æœå›¾è¡¨"""
        if not self.validation_results:
            logger.warning("æ²¡æœ‰éªŒè¯ç»“æœï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. é¢„æµ‹æ€§èƒ½å¯¹æ¯”å›¾
        self._plot_prediction_performance(output_path)
        
        # 2. é£é™©è¯„ä¼°æ€§èƒ½å›¾
        self._plot_risk_assessment_performance(output_path)
        
        # 3. æ—¶é—´åºåˆ—éªŒè¯å›¾
        self._plot_time_series_validation(output_path)
        
        logger.info(f"éªŒè¯ç»“æœå›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    def _plot_prediction_performance(self, output_path: Path):
        """ç»˜åˆ¶é¢„æµ‹æ€§èƒ½å›¾è¡¨"""
        successful_stations = [s for s, r in self.validation_results.items() if "error" not in r]
        
        if not successful_stations:
            return
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        station_names = []
        mse_values = []
        mae_values = []
        r2_values = []
        
        for station_id in successful_stations:
            result = self.validation_results[station_id]
            if "overall_metrics" in result:
                metrics = result["overall_metrics"]["prediction_performance"]
                station_names.append(station_id)
                mse_values.append(metrics["mse_mean"])
                mae_values.append(metrics["mae_mean"])
                r2_values.append(metrics["r2_mean"])
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # MSEå¯¹æ¯”
        axes[0].bar(station_names, mse_values, color='skyblue')
        axes[0].set_title('Mean Squared Error (MSE)')
        axes[0].set_ylabel('MSE')
        axes[0].tick_params(axis='x', rotation=45)
        
        # MAEå¯¹æ¯”
        axes[1].bar(station_names, mae_values, color='lightcoral')
        axes[1].set_title('Mean Absolute Error (MAE)')
        axes[1].set_ylabel('MAE')
        axes[1].tick_params(axis='x', rotation=45)
        
        # RÂ²å¯¹æ¯”
        axes[2].bar(station_names, r2_values, color='lightgreen')
        axes[2].set_title('RÂ² Score')
        axes[2].set_ylabel('RÂ²')
        axes[2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_path / 'prediction_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_risk_assessment_performance(self, output_path: Path):
        """ç»˜åˆ¶é£é™©è¯„ä¼°æ€§èƒ½å›¾è¡¨"""
        successful_stations = [s for s, r in self.validation_results.items() if "error" not in r]
        
        if not successful_stations:
            return
        
        # æ”¶é›†é£é™©è¯„åˆ†
        station_names = []
        risk_scores = []
        
        for station_id in successful_stations:
            result = self.validation_results[station_id]
            if "overall_metrics" in result:
                metrics = result["overall_metrics"]["risk_assessment_performance"]
                station_names.append(station_id)
                risk_scores.append(metrics["risk_score_mean"])
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(10, 6))
        bars = plt.bar(station_names, risk_scores, color='gold')
        plt.title('Average Risk Score by Station')
        plt.xlabel('Station ID')
        plt.ylabel('Risk Score')
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, risk_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{score:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(output_path / 'risk_assessment_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_time_series_validation(self, output_path: Path):
        """ç»˜åˆ¶æ—¶é—´åºåˆ—éªŒè¯å›¾è¡¨"""
        successful_stations = [s for s, r in self.validation_results.items() if "error" not in r]
        
        if not successful_stations:
            return
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæˆåŠŸç«™ç‚¹è¿›è¡Œè¯¦ç»†å±•ç¤º
        station_id = successful_stations[0]
        result = self.validation_results[station_id]
        
        if "results" not in result:
            return
        
        # åˆ›å»ºæ—¶é—´åºåˆ—å›¾
        fig, axes = plt.subplots(2, 1, figsize=(12, 10))
        
        # ç¬¬ä¸€ä¸ªå­å›¾ï¼šæµé‡é¢„æµ‹å¯¹æ¯”
        for i, window_result in enumerate(result["results"][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªçª—å£
            if "predicted_flows" in window_result and "actual_flows" in window_result:
                predicted = window_result["predicted_flows"]
                actual = window_result["actual_flows"]
                dates = pd.date_range(
                    start=window_result["val_start"], 
                    periods=len(actual), 
                    freq='D'
                )
                
                axes[0].plot(dates, predicted, 'o-', label=f'Window {i+1} (Predicted)', alpha=0.7)
                axes[0].plot(dates, actual, 's-', label=f'Window {i+1} (Actual)', alpha=0.7)
        
        axes[0].set_title(f'Flow Prediction vs Actual - Station {station_id}')
        axes[0].set_ylabel('Flow (mÂ³/s)')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # ç¬¬äºŒä¸ªå­å›¾ï¼šé£é™©è¯„ä¼°æ—¶é—´åºåˆ—
        risk_scores = []
        dates = []
        
        for window_result in result["results"]:
            if "risk_assessment" in window_result:
                risk_scores.append(window_result["risk_assessment"]["risk_score"])
                dates.append(window_result["val_start"])
        
        if risk_scores:
            axes[1].plot(dates, risk_scores, 'o-', color='red', linewidth=2, markersize=8)
            axes[1].set_title(f'Risk Score Time Series - Station {station_id}')
            axes[1].set_ylabel('Risk Score')
            axes[1].set_xlabel('Date')
            axes[1].grid(True, alpha=0.3)
            
            # æ·»åŠ é£é™©ç­‰çº§é˜ˆå€¼çº¿
            axes[1].axhline(y=20, color='green', linestyle='--', alpha=0.7, label='Low Risk')
            axes[1].axhline(y=40, color='orange', linestyle='--', alpha=0.7, label='Medium Risk')
            axes[1].axhline(y=60, color='red', linestyle='--', alpha=0.7, label='High Risk')
            axes[1].axhline(y=80, color='darkred', linestyle='--', alpha=0.7, label='Extreme Risk')
            axes[1].legend()
        
        plt.tight_layout()
        plt.savefig(output_path / 'time_series_validation.png', dpi=300, bbox_inches='tight')
        plt.close()

def main():
    """æµ‹è¯•äº¤å‰éªŒè¯åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ´ªæ°´é£é™©äº¤å‰éªŒè¯...")
    
    # åˆ›å»ºéªŒè¯å™¨å®ä¾‹
    validator = FloodRiskCrossValidator()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å†å²æ•°æ®
    data_paths = [
        "data/raw/manitoba_streamflow_processed.csv",
        "data/raw/manitoba_streamflow_sample.csv",
        "data/processed/hydat_streamflow_processed.csv"
    ]
    
    available_data = None
    for path in data_paths:
        if Path(path).exists():
            available_data = path
            break
    
    if not available_data:
        print("âŒ æœªæ‰¾åˆ°å†å²æ•°æ®ï¼Œè¯·å…ˆè¿è¡ŒHYDATæ•°æ®ä¸‹è½½")
        return
    
    print(f"ğŸ“Š ä½¿ç”¨å†å²æ•°æ®: {available_data}")
    
    # è¿è¡Œäº¤å‰éªŒè¯
    try:
        validation_report = validator.run_cross_validation(
            data_path=available_data,
            validation_windows=3,  # å‡å°‘çª—å£æ•°ä»¥åŠ å¿«æµ‹è¯•
            forecast_horizon=7
        )
        
        print("âœ… äº¤å‰éªŒè¯å®Œæˆ")
        print(f"   éªŒè¯ç«™ç‚¹æ•°: {len(validation_report['station_results'])}")
        
        if "overall_performance" in validation_report:
            perf = validation_report["overall_performance"]
            if "validation_summary" in perf:
                summary = perf["validation_summary"]
                print(f"   æˆåŠŸç‡: {summary.get('success_rate', 0):.1%}")
        
        # ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
        validator.generate_validation_report("data/processed/flood_risk_validation_report.json")
        validator.plot_validation_results()
        
        print("ğŸ“Š éªŒè¯æŠ¥å‘Šå’Œå›¾è¡¨å·²ç”Ÿæˆ")
        
    except Exception as e:
        print(f"âŒ äº¤å‰éªŒè¯å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
