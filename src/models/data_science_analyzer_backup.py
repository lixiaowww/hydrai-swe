#!/usr/bin/env python3
"""
ä¸“ä¸šæ•°æ®ç§‘å­¦åˆ†ææ¨¡å—
é›†æˆæ— ç›‘ç£å­¦ä¹ ã€å¼‚å¸¸æ£€æµ‹ã€èšç±»åˆ†æã€æ—¶é—´æ¨¡å¼åˆ†æç­‰åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from scipy import stats
from scipy.signal import periodogram, welch
from scipy.stats import chi2_contingency, kruskal, mannwhitneyu
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import os
import warnings
warnings.filterwarnings('ignore')

class DataScienceAnalyzer:
    """ä¸“ä¸šæ•°æ®ç§‘å­¦åˆ†æå™¨"""
    
    def __init__(self, data_path=None):
        """
        åˆå§‹åŒ–æ•°æ®ç§‘å­¦åˆ†æå™¨
        
        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data = None
        self.analysis_results = {}
        self.scaler = StandardScaler()
        
        if data_path:
            self.load_data(data_path)
    
    def load_data(self, data_path):
        """åŠ è½½æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {data_path}")
        
        try:
            if not os.path.exists(data_path):
                # å°è¯•å¤‡ç”¨æ•°æ®è·¯å¾„
                backup_paths = [
                    "data/processed/eccc_manitoba_snow_processed.csv",
                    "data/raw/eccc_recent/eccc_recent_combined.csv"
                ]
                
                for backup_path in backup_paths:
                    if os.path.exists(backup_path):
                        print(f"ğŸ“‚ ä½¿ç”¨å¤‡ç”¨æ•°æ®: {backup_path}")
                        data_path = backup_path
                        break
                else:
                    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            
            self.data = pd.read_csv(data_path)
            
            # å¤„ç†æ—¥æœŸåˆ—
            date_col = None
            for col_name in ['date', 'Date/Time', 'Date']:
                if col_name in self.data.columns:
                    date_col = col_name
                    break
            
            if date_col is None:
                raise ValueError("æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
            
            self.data[date_col] = pd.to_datetime(self.data[date_col], errors='coerce')
            self.data.set_index(date_col, inplace=True)
            
            # åˆ›å»ºæˆ–æ‰¾åˆ°snow_water_equivalent_mmåˆ—ï¼ˆä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
            if 'snow_water_equivalent_mm' not in self.data.columns:
                swe_candidates = [
                    'Snow on Grnd (mm)', 'Snow on Grnd (cm)', 
                    'Total Snow (mm)', 'Total Snow (cm)'
                ]
                
                for candidate in swe_candidates:
                    if candidate in self.data.columns:
                        if 'cm' in candidate:
                            self.data['snow_water_equivalent_mm'] = self.data[candidate] * 10.0
                        else:
                            self.data['snow_water_equivalent_mm'] = self.data[candidate]
                        print(f"âœ… ä» {candidate} åˆ›å»º snow_water_equivalent_mm åˆ—")
                        break
                else:
                    raise ValueError("æœªæ‰¾åˆ°çœŸå®çš„é›ªæ°´å½“é‡åˆ—ï¼Œç¦æ­¢ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            
            # æ•°æ®æ¸…ç†
            self.data['snow_water_equivalent_mm'] = pd.to_numeric(
                self.data['snow_water_equivalent_mm'], errors='coerce'
            )
            
            original_len = len(self.data)
            self.data = self.data.dropna(subset=['snow_water_equivalent_mm'])
            
            if len(self.data) == 0:
                raise ValueError("å¤„ç†åæ•°æ®ä¸ºç©º")
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data)} æ¡è®°å½• (åŸå§‹: {original_len} æ¡)")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.data.index.min()} åˆ° {self.data.index.max()}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            self.data = None
    
    def _series_to_dict(self, series):
        """å°†pandas Seriesè½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„å­—å…¸æ ¼å¼ï¼Œå¤„ç†NaNå€¼"""
        # å¤„ç†NaNå€¼ï¼Œç¡®ä¿JSONåºåˆ—åŒ–æˆåŠŸ
        clean_values = []
        for val in series.values:
            if pd.isna(val) or np.isnan(val) or np.isinf(val):
                clean_values.append(None)  # ä½¿ç”¨Noneæ›¿ä»£NaN/Inf
            else:
                clean_values.append(float(val))  # ç¡®ä¿æ˜¯æ™®é€šæµ®ç‚¹æ•°
        
        return {
            'index': series.index.tolist(),
            'values': clean_values
        }
    
    def advanced_time_series_decomposition(self, column='snow_water_equivalent_mm'):
        """
        é«˜çº§æ—¶é—´åºåˆ—åˆ†è§£ - å¤šå°ºåº¦åˆ†æ
        
        Args:
            column (str): è¦åˆ†æçš„åˆ—å
            
        Returns:
            dict: åˆ†è§£ç»“æœ
        """
        print(f"\nğŸ” æ‰§è¡Œé«˜çº§æ—¶é—´åºåˆ—åˆ†è§£: {column}")
        print("=" * 60)
        
        if self.data is None or column not in self.data.columns:
            return {}
        
        series = self.data[column].dropna()
        if len(series) == 0:
            return {}
        
        results = {}
        
        # 1. STLåˆ†è§£
        try:
            results['stl_decomposition'] = self._stl_decomposition(series)
        except Exception as e:
            print(f"âš ï¸ STLåˆ†è§£å¤±è´¥: {e}. ä½¿ç”¨ç®€åŒ–åˆ†è§£æ›¿ä»£")
            results['stl_decomposition'] = self._simple_decomposition(series)

        # 2. å°æ³¢åˆ†è§£ï¼ˆä½¿ç”¨pywaveletsï¼‰
        try:
            results['wavelet_decomposition'] = self._wavelet_decomposition(series)
        except Exception as e:
            print(f"âš ï¸ å°æ³¢åˆ†è§£å¤±è´¥: {e}")

        # 3. ç»éªŒæ¨¡æ€åˆ†è§£ï¼ˆå¯é€‰ï¼‰
        try:
            results['emd_decomposition'] = self._emd_decomposition(series)
        except Exception as e:
            print(f"âš ï¸ EMDåˆ†è§£å¤±è´¥: {e}")

        # 4. å¤šå°ºåº¦è¶‹åŠ¿
        try:
            results['multi_scale_trends'] = self._multi_scale_trend_analysis(series)
        except Exception as e:
            print(f"âš ï¸ å¤šå°ºåº¦è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")

        # 5. å‘¨æœŸæ€§æ£€æµ‹
        try:
            results['periodicity_analysis'] = self._periodicity_detection(series)
        except Exception as e:
            print(f"âš ï¸ å‘¨æœŸæ€§æ£€æµ‹å¤±è´¥: {e}")

        # Add interpretation for decomposition analysis
        results['interpretation'] = self._interpret_decomposition_results(results)

        self.analysis_results['advanced_decomposition'] = results
        print("âœ… Advanced time series decomposition completed")
        return results
    
    def _stl_decomposition(self, series):
        """STLåˆ†è§£"""
        print("ğŸ“Š æ‰§è¡ŒSTLåˆ†è§£...")
        
        try:
            from statsmodels.tsa.seasonal import STL
            
            # æ•°æ®è´¨é‡æ£€æŸ¥å’Œé¢„å¤„ç†ç­–ç•¥
            print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡: é•¿åº¦={len(series)}, é¢‘ç‡={series.index.inferred_freq}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡é‡‡æ ·
            original_freq = series.index.inferred_freq
            if original_freq is None:
                # æ¨æ–­é¢‘ç‡
                time_diffs = series.index.to_series().diff().dropna()
                median_diff = time_diffs.median()
                if median_diff <= pd.Timedelta('1D'):
                    target_freq = 'D'
                elif median_diff <= pd.Timedelta('7D'):
                    target_freq = 'W'
                elif median_diff <= pd.Timedelta('30D'):
                    target_freq = 'M'
                else:
                    target_freq = 'D'  # é»˜è®¤æ—¥é¢‘ç‡
                print(f"âš ï¸ æ¨æ–­é¢‘ç‡: {target_freq}")
            else:
                target_freq = original_freq
            
            # æ™ºèƒ½é‡é‡‡æ ·ï¼šä¿æŒæ•°æ®å®Œæ•´æ€§
            if target_freq == 'D':
                series_resampled = series.resample('D').mean()
            elif target_freq == 'W':
                series_resampled = series.resample('W').mean()
            elif target_freq == 'M':
                series_resampled = series.resample('M').mean()
            else:
                series_resampled = series.resample('D').mean()
            
            print(f"ğŸ“Š é‡é‡‡æ ·åç»Ÿè®¡: é•¿åº¦={len(series_resampled)}, ç¼ºå¤±å€¼={series_resampled.isna().sum()}")
            
            # æ™ºèƒ½å¡«å……ç­–ç•¥
            if series_resampled.isna().sum() > 0:
                missing_ratio = series_resampled.isna().sum() / len(series_resampled)
                print(f"ğŸ“Š ç¼ºå¤±å€¼æ¯”ä¾‹: {missing_ratio:.3f}")
                
                if missing_ratio < 0.05:  # ç¼ºå¤±å°‘äº5%
                    # ä½¿ç”¨çº¿æ€§æ’å€¼
                    series_resampled = series_resampled.interpolate(method='linear')
                    print("âœ… ä½¿ç”¨çº¿æ€§æ’å€¼å¡«å……å°‘é‡ç¼ºå¤±å€¼")
                elif missing_ratio < 0.2:  # ç¼ºå¤±å°‘äº20%
                    # ä½¿ç”¨æ ·æ¡æ’å€¼
                    series_resampled = series_resampled.interpolate(method='spline', order=2)
                    print("âœ… ä½¿ç”¨æ ·æ¡æ’å€¼å¡«å……ä¸­ç­‰ç¼ºå¤±å€¼")
                else:
                    # å¤§é‡ç¼ºå¤±ï¼Œä½¿ç”¨å‰å‘å¡«å……ä½†ä¸¥æ ¼é™åˆ¶
                    series_resampled = series_resampled.fillna(method='ffill', limit=3)
                    print("âš ï¸ å¤§é‡ç¼ºå¤±å€¼ï¼Œä½¿ç”¨é™åˆ¶æ€§å‰å‘å¡«å……")
                    
                # éªŒè¯å¡«å……ç»“æœ
                remaining_missing = series_resampled.isna().sum()
                print(f"ğŸ“Š å¡«å……åå‰©ä½™ç¼ºå¤±å€¼: {remaining_missing}")
                
                if remaining_missing > 0:
                    # å¦‚æœä»æœ‰ç¼ºå¤±ï¼Œä½¿ç”¨å‡å€¼å¡«å……
                    series_resampled = series_resampled.fillna(series_resampled.mean())
                    print("âš ï¸ ä½¿ç”¨å‡å€¼å¡«å……å‰©ä½™ç¼ºå¤±å€¼")
            
            # æ¨æ–­å‘¨æœŸï¼šæŒ‰æ—¥æ•°æ®ç”¨365ï¼ŒæŒ‰æœˆæ•°æ®ç”¨12ï¼Œå¦åˆ™å›é€€ä¸ºè¿‘ä¼¼å‘¨æœŸ
            inferred = series_resampled.index.inferred_freq
            period = 365
            if inferred is not None and inferred.upper().startswith('M'):
                period = 12
            elif inferred is not None and inferred.upper().startswith('D'):
                period = 365
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: é•¿åº¦={len(series_resampled)}, å‡å€¼={series_resampled.mean():.2f}, æ ‡å‡†å·®={series_resampled.std():.2f}")
            
            # å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
            Q1 = series_resampled.quantile(0.25)
            Q3 = series_resampled.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = (series_resampled < lower_bound) | (series_resampled > upper_bound)
            if outliers.sum() > 0:
                print(f"âš ï¸ æ£€æµ‹åˆ° {outliers.sum()} ä¸ªå¼‚å¸¸å€¼ï¼Œä½¿ç”¨ç¨³å¥STLåˆ†è§£")
                robust_flag = True
            else:
                robust_flag = False
            
            # æ‰§è¡ŒSTLåˆ†è§£ - ä¿®å¤å‚æ•°è®¾ç½®
            # STLçš„seasonalå‚æ•°å¿…é¡»æ˜¯å¥‡æ•°ï¼Œä¸” >= 3
            seasonal_window = min(period // 2, 365 // 2)  # å­£èŠ‚æ€§çª—å£å¤§å°
            if seasonal_window % 2 == 0:  # ç¡®ä¿æ˜¯å¥‡æ•°
                seasonal_window = seasonal_window + 1
            if seasonal_window < 3:  # ç¡®ä¿ >= 3
                seasonal_window = 3
            print(f"ğŸ“Š STLå‚æ•°: seasonal={seasonal_window}, period={period}")
            stl = STL(series_resampled, seasonal=seasonal_window, period=period, robust=robust_flag)
            result = stl.fit()
            
            # è®¡ç®—å­£èŠ‚æ€§å¼ºåº¦å’Œè¶‹åŠ¿å¼ºåº¦ - ä¿®å¤è®¡ç®—æ–¹å¼
            total_variance = np.var(series_resampled.dropna())
            if total_variance > 0:
                seasonal_strength = np.var(result.seasonal.dropna()) / total_variance
                trend_strength = np.var(result.trend.dropna()) / total_variance
            else:
                seasonal_strength = 0.0
                trend_strength = 0.0
            
            # ä½¿ç”¨é€šç”¨çš„series_to_dictå‡½æ•°
            
            # ç»“æœéªŒè¯å’Œåˆç†æ€§æ£€æŸ¥
            decomposition_result = {
                'trend': self._series_to_dict(result.trend),
                'seasonal': self._series_to_dict(result.seasonal),
                'resid': self._series_to_dict(result.resid),
                'seasonal_strength': seasonal_strength,
                'trend_strength': trend_strength
            }
            
            # éªŒè¯åˆ†è§£ç»“æœçš„åˆç†æ€§
            validation_result = self._validate_decomposition_result(decomposition_result, series_resampled)
            if not validation_result['is_valid']:
                print(f"âš ï¸ åˆ†è§£ç»“æœéªŒè¯å¤±è´¥: {validation_result['issues']}")
                # å°è¯•ä½¿ç”¨ç®€åŒ–åˆ†è§£
                print("ğŸ”„ å°è¯•ä½¿ç”¨ç®€åŒ–åˆ†è§£...")
                return self._simple_decomposition(series)
            
            print("âœ… åˆ†è§£ç»“æœéªŒè¯é€šè¿‡")
            return decomposition_result
        except ImportError:
            print("âš ï¸ statsmodelsæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–åˆ†è§£")
            return self._simple_decomposition(series)
        except Exception as e:
            print(f"âŒ STLåˆ†è§£å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°ç®€åŒ–åˆ†è§£...")
            return self._simple_decomposition(series)
    
    def _validate_decomposition_result(self, result, original_series):
        """éªŒè¯STLåˆ†è§£ç»“æœçš„åˆç†æ€§"""
        try:
            issues = []
            
            # 1. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            if not all(key in result for key in ['trend', 'seasonal', 'resid']):
                issues.append("Missing decomposition components")
                return {'is_valid': False, 'issues': issues}
            
            # 2. æ£€æŸ¥æ•°æ®é•¿åº¦ä¸€è‡´æ€§
            trend_len = len(result['trend']['values'])
            seasonal_len = len(result['seasonal']['values'])
            resid_len = len(result['resid']['values'])
            original_len = len(original_series)
            
            if not (trend_len == seasonal_len == resid_len == original_len):
                issues.append(f"Length mismatch: trend={trend_len}, seasonal={seasonal_len}, resid={resid_len}, original={original_len}")
            
            # 3. æ£€æŸ¥é‡å»ºè¯¯å·®
            trend_values = np.array(result['trend']['values'])
            seasonal_values = np.array(result['seasonal']['values'])
            resid_values = np.array(result['resid']['values'])
            original_values = original_series.values
            
            # é‡å»ºæ•°æ®
            reconstructed = trend_values + seasonal_values + resid_values
            
            # è®¡ç®—é‡å»ºè¯¯å·®
            reconstruction_error = np.mean(np.abs(reconstructed - original_values))
            original_std = np.std(original_values)
            
            if original_std > 0:
                relative_error = reconstruction_error / original_std
                if relative_error > 0.1:  # ç›¸å¯¹è¯¯å·®è¶…è¿‡10%
                    issues.append(f"High reconstruction error: {relative_error:.3f}")
            
            # 4. æ£€æŸ¥å­£èŠ‚æ€§å¼ºåº¦åˆç†æ€§
            seasonal_strength = result.get('seasonal_strength', 0)
            if seasonal_strength > 0.95:
                issues.append(f"Unusually high seasonal strength: {seasonal_strength:.3f}")
            elif seasonal_strength < 0.01:
                issues.append(f"Unusually low seasonal strength: {seasonal_strength:.3f}")
            
            # 5. æ£€æŸ¥è¶‹åŠ¿å¼ºåº¦åˆç†æ€§ - è¿›ä¸€æ­¥æ”¾å®½é˜ˆå€¼
            trend_strength = result.get('trend_strength', 0)
            if trend_strength > 0.95:
                issues.append(f"Unusually high trend strength: {trend_strength:.3f}")
            elif trend_strength < 0.0001:  # ä»0.001è¿›ä¸€æ­¥æ”¾å®½åˆ°0.0001
                issues.append(f"Unusually low trend strength: {trend_strength:.3f}")
            
            # 6. æ£€æŸ¥æ®‹å·®çš„ç™½å™ªå£°ç‰¹æ€§
            resid_values_clean = resid_values[~np.isnan(resid_values)]
            if len(resid_values_clean) > 0:
                resid_std = np.std(resid_values_clean)
                if resid_std < 1e-6:  # æ®‹å·®å‡ ä¹ä¸º0
                    issues.append("Residuals are almost zero, decomposition may be overfitting")
            
            is_valid = len(issues) == 0
            
            if not is_valid:
                print(f"âŒ åˆ†è§£ç»“æœéªŒè¯å¤±è´¥:")
                for issue in issues:
                    print(f"   - {issue}")
            else:
                print(f"âœ… åˆ†è§£ç»“æœéªŒè¯é€šè¿‡")
            
            return {
                'is_valid': is_valid,
                'issues': issues,
                'reconstruction_error': reconstruction_error if 'reconstruction_error' in locals() else None,
                'relative_error': relative_error if 'relative_error' in locals() else None
            }
            
        except Exception as e:
            print(f"âš ï¸ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            return {'is_valid': False, 'issues': [f"Validation error: {e}"]}
    
    def _wavelet_decomposition(self, series):
        """å°æ³¢åˆ†è§£ - ä½¿ç”¨æˆç†Ÿçš„pywaveletsåº“"""
        print("ğŸŒŠ æ‰§è¡Œå°æ³¢åˆ†è§£...")
        
        try:
            import pywt
            
            # ç¡®ä¿æ•°æ®é•¿åº¦æ˜¯2çš„å¹‚æ¬¡
            n = len(series)
            next_power = 2 ** int(np.ceil(np.log2(n)))
            series_padded = np.pad(series.values, (0, next_power - n), 'edge')
            
            # æ‰§è¡Œå°æ³¢åˆ†è§£
            coeffs = pywt.wavedec(series_padded, 'db4', level=4)
            
            # é‡æ„å„ä¸ªåˆ†é‡
            reconstructed = []
            for i, coeff in enumerate(coeffs):
                coeff_list = [np.zeros_like(c) for c in coeffs]
                coeff_list[i] = coeff
                reconstructed.append(pywt.waverec(coeff_list, 'db4')[:n])
            
            # ä½¿ç”¨tsfreshæå–å°æ³¢ç‰¹å¾
            try:
                import tsfresh
                from tsfresh.feature_extraction import MinimalFCParameters
                
                df = pd.DataFrame({
                    'id': 1,
                    'time': range(len(series)),
                    'value': series.values
                })
                
                # æå–å°æ³¢ç›¸å…³ç‰¹å¾
                wavelet_features = tsfresh.extract_features(df, column_id='id', column_sort='time', 
                                                         column_value='value', 
                                                         default_fc_parameters=MinimalFCParameters())
                
                return {
                    'approximation': reconstructed[0].tolist(),
                    'details': [detail.tolist() for detail in reconstructed[1:]],
                    'wavelet_type': 'db4',
                    'levels': len(coeffs) - 1,
                    'tsfresh_wavelet_features': wavelet_features.to_dict('records')[0] if not wavelet_features.empty else {}
                }
                
            except Exception as e:
                print(f"âš ï¸ tsfreshå°æ³¢ç‰¹å¾æå–å¤±è´¥: {e}")
                return {
                    'approximation': reconstructed[0].tolist(),
                    'details': [detail.tolist() for detail in reconstructed[1:]],
                'wavelet_type': 'db4',
                'levels': len(coeffs) - 1
            }
            
        except ImportError:
            print("âš ï¸ PyWaveletsæœªå®‰è£…ï¼Œè·³è¿‡å°æ³¢åˆ†è§£")
            return {}
        except Exception as e:
            print(f"âŒ å°æ³¢åˆ†è§£å¤±è´¥: {e}")
            return {}
    
    def _emd_decomposition(self, series):
        """ç»éªŒæ¨¡æ€åˆ†è§£"""
        print("ğŸ”„ æ‰§è¡Œç»éªŒæ¨¡æ€åˆ†è§£...")
        
        try:
            from PyEMD import EMD
            
            emd = EMD()
            IMFs = emd(series.values)
            
            return {
                'imfs': IMFs,
                'residue': IMFs[-1] if len(IMFs) > 0 else None,
                'n_imfs': len(IMFs)
            }
        except ImportError:
            print("âš ï¸ PyEMDæœªå®‰è£…ï¼Œè·³è¿‡EMDåˆ†è§£")
            return {}
    
    def _multi_scale_trend_analysis(self, series):
        """å¤šå°ºåº¦è¶‹åŠ¿åˆ†æ"""
        print("ğŸ“ˆ æ‰§è¡Œå¤šå°ºåº¦è¶‹åŠ¿åˆ†æ...")
        
        # ä¸åŒæ—¶é—´å°ºåº¦çš„è¶‹åŠ¿
        scales = [7, 30, 90, 365]  # å‘¨ã€æœˆã€å­£ã€å¹´
        trends = {}
        
        for scale in scales:
            if len(series) > scale:
                rolling_mean = series.rolling(window=scale, center=True).mean()
                trends[f'{scale}_day'] = rolling_mean
        
        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
        trend_strength = {}
        for scale, trend in trends.items():
            if not trend.isna().all():
                # è®¡ç®—è¶‹åŠ¿çš„æ–¹å·®ä¸åŸå§‹åºåˆ—æ–¹å·®çš„æ¯”å€¼
                strength = trend.var() / series.var() if series.var() > 0 else 0
                trend_strength[scale] = strength
        
        return {
            'trends': trends,
            'trend_strength': trend_strength
        }
    
    def _periodicity_detection(self, series):
        """å‘¨æœŸæ€§æ£€æµ‹ - ä½¿ç”¨æˆç†Ÿçš„tsfreshåº“"""
        print("ğŸ”„ æ‰§è¡Œå‘¨æœŸæ€§æ£€æµ‹...")
        
        try:
            import tsfresh
            from tsfresh.feature_extraction import MinimalFCParameters
            
            # å‡†å¤‡æ•°æ®æ ¼å¼
            df = pd.DataFrame({
                'id': 1,
                'time': range(len(series)),
                'value': series.values
            })
            
            # ä½¿ç”¨tsfreshçš„ç‰¹å¾æå–
            features = tsfresh.extract_features(df, column_id='id', column_sort='time', 
                                             column_value='value', 
                                             default_fc_parameters=MinimalFCParameters())
            
            # æå–å…³é”®å‘¨æœŸæ€§ç‰¹å¾
            periodicity_features = {
                'fft_coefficient': features.get('value__fft_coefficient__real_0', [0])[0],
                'fft_aggregated': features.get('value__fft_aggregated__aggtype_centroid', [0])[0],
                'fft_aggregated_peaks': features.get('value__fft_aggregated__aggtype_peaks', [0])[0],
                'fft_aggregated_centroid': features.get('value__fft_aggregated__aggtype_centroid', [0])[0]
            }
            
            # è®¡ç®—ä¸»è¦å‘¨æœŸ
            try:
                # ä½¿ç”¨scipyçš„welchæ–¹æ³•ä½œä¸ºå¤‡é€‰
                from scipy.signal import welch
                series_resampled = series.resample('D').mean().fillna(method='ffill')
                frequencies, power = welch(series_resampled.dropna(), nperseg=min(256, len(series_resampled)//4))
                
                main_freq_idx = np.argmax(power)
                main_frequency = frequencies[main_freq_idx]
                main_period = 1.0 / main_frequency if main_frequency > 0 else 365.0
                
                return {
                    'tsfresh_features': periodicity_features,
                    'main_frequency': main_frequency,
                    'main_period': main_period,
                    'power_spectrum': {
                        'frequencies': frequencies.tolist(),
                        'power': power.tolist()
                    }
                }
            except Exception as e:
                print(f"âš ï¸ Welchæ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨tsfreshç‰¹å¾: {e}")
                return {
                    'tsfresh_features': periodicity_features,
                    'main_period': 365.0,  # é»˜è®¤å¹´å‘¨æœŸ
                    'method': 'tsfresh_only'
                }
                
        except ImportError:
            print("âš ï¸ tsfreshæœªå®‰è£…ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            try:
                from scipy.signal import welch
                series_resampled = series.resample('D').mean().fillna(method='ffill')
                frequencies, power = welch(series_resampled.dropna(), nperseg=min(256, len(series_resampled)//4))
                
                main_freq_idx = np.argmax(power)
                main_frequency = frequencies[main_freq_idx]
                main_period = 1.0 / main_frequency if main_frequency > 0 else 365.0
                
                return {
                    'frequencies': frequencies.tolist(),
                    'power': power.tolist(),
                    'main_frequency': main_frequency,
                    'main_period': main_period,
                    'method': 'traditional'
                }
            except Exception as e:
                print(f"âš ï¸ ä¼ ç»Ÿæ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
                return {
                    'main_period': 365.0,
                    'method': 'fallback',
                    'error': str(e)
        }
    
    def _simple_decomposition(self, series):
        """ç®€åŒ–åˆ†è§£ï¼ˆå½“STLä¸å¯ç”¨æ—¶ï¼‰"""
        print("ğŸ“Š æ‰§è¡Œç®€åŒ–åˆ†è§£...")
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡ä½œä¸ºè¶‹åŠ¿
        window = min(365, len(series) // 4)
        trend = series.rolling(window=window, center=True).mean()
        
        # è®¡ç®—å­£èŠ‚æ€§ - ä¿®å¤ç®—æ³•
        # ä½¿ç”¨å»è¶‹åŠ¿åçš„æ•°æ®è¿›è¡Œå­£èŠ‚æ€§åˆ†æ
        detrended = series - trend
        
        # è®¡ç®—å­£èŠ‚æ€§ï¼šæŒ‰å¹´åˆ†ç»„è®¡ç®—å¹³å‡å€¼
        if len(series) >= 365:
            # æŒ‰å¹´åˆ†ç»„ - ä¿®å¤å…ƒç»„é—®é¢˜
            try:
                # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                if not isinstance(series.index, pd.DatetimeIndex):
                    series.index = pd.to_datetime(series.index)
                
                yearly_groups = detrended.groupby(series.index.year)
                seasonal_pattern = yearly_groups.mean()
                
                # æ‰©å±•åˆ°æ•´ä¸ªæ—¶é—´åºåˆ—
                seasonal = pd.Series(index=series.index, dtype=float)
                for year in yearly_groups.groups:
                    year_mask = series.index.year == year
                    seasonal[year_mask] = seasonal_pattern[year]
            except Exception as e:
                print(f"âš ï¸ å¹´åˆ†ç»„å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•: {e}")
                # å›é€€åˆ°ç®€å•æ–¹æ³•
                seasonal = detrended.rolling(window=30, center=True).mean()
        else:
            # æ•°æ®ä¸è¶³ä¸€å¹´ï¼Œä½¿ç”¨ç®€å•çš„å‘¨æœŸæ€§æ¨¡å¼
            seasonal = detrended.rolling(window=7, center=True).mean()
        
        # è®¡ç®—æ®‹å·® - ä¿®å¤è®¡ç®—
        residual = series - trend - seasonal
        
        # ä½¿ç”¨é€šç”¨çš„series_to_dictå‡½æ•°
        
        return {
            'trend': self._series_to_dict(trend),
            'seasonal': self._series_to_dict(seasonal),
            'resid': self._series_to_dict(residual),
            'seasonal_strength': 0.5,  # é»˜è®¤å€¼
            'trend_strength': 0.5
        }
    
    def advanced_anomaly_detection(self, column='snow_water_equivalent_mm'):
        """
        é«˜çº§å¼‚å¸¸æ£€æµ‹ - å¤šç§ç®—æ³•é›†æˆ
        
        Args:
            column (str): è¦æ£€æµ‹çš„åˆ—å
            
        Returns:
            dict: å¼‚å¸¸æ£€æµ‹ç»“æœ
        """
        print(f"\nğŸš¨ æ‰§è¡Œé«˜çº§å¼‚å¸¸æ£€æµ‹: {column}")
        print("=" * 60)
        
        if self.data is None or column not in self.data.columns:
            return {}
        
        series = self.data[column].dropna()
        if len(series) == 0:
            return {}
        
        try:
            # 1. ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹
            statistical_anomalies = self._statistical_anomaly_detection(series)
            
            # 2. æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
            ml_anomalies = self._ml_anomaly_detection(series)
            
            # 3. æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
            ts_anomalies = self._timeseries_anomaly_detection(series)
            
            # 4. é›†æˆå¼‚å¸¸æ£€æµ‹
            ensemble_anomalies = self._ensemble_anomaly_detection(
                statistical_anomalies, ml_anomalies, ts_anomalies
            )
            
            # 5. å¼‚å¸¸è§£é‡Š
            anomaly_explanations = self._explain_anomalies(series, ensemble_anomalies)
            
            results = {
                'statistical': statistical_anomalies,
                'machine_learning': ml_anomalies,
                'timeseries': ts_anomalies,
                'ensemble': ensemble_anomalies,
                'explanations': anomaly_explanations
            }
            
            # Add interpretation for anomaly detection
            results['interpretation'] = self._interpret_anomaly_detection_results(results)
            
            self.analysis_results['advanced_anomaly_detection'] = results
            print("âœ… Advanced anomaly detection completed")
            return results
            
        except Exception as e:
            print(f"âŒ é«˜çº§å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def _statistical_anomaly_detection(self, series):
        """ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹"""
        print("ğŸ“Š ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹...")
        
        # Z-scoreæ–¹æ³• (é™ä½é˜ˆå€¼ä½¿å…¶æ›´æ•æ„Ÿ)
        z_scores = np.abs(stats.zscore(series))
        z_anomalies = z_scores > 2.0  # ä»3é™åˆ°2ï¼Œæ›´åˆç†çš„é˜ˆå€¼
        
        # Modified Z-score (ä½¿ç”¨ä¸­ä½æ•°)
        median = np.median(series)
        mad = np.median(np.abs(series - median))
        modified_z_scores = 0.6745 * (series - median) / mad
        modified_z_anomalies = np.abs(modified_z_scores) > 2.0  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
        
        # IQRæ–¹æ³•
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        iqr_anomalies = (series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))
        
        # æç«¯IQRæ–¹æ³•
        extreme_iqr_anomalies = (series < (Q1 - 3 * IQR)) | (series > (Q3 + 3 * IQR))
        
        # ç§»åŠ¨çª—å£æ–¹æ³•
        window = min(30, len(series) // 10)
        rolling_mean = series.rolling(window=window, center=True).mean()
        rolling_std = series.rolling(window=window, center=True).std()
        rolling_anomalies = np.abs(series - rolling_mean) > 3 * rolling_std
        
        return {
            'z_score_anomalies': z_anomalies,
            'modified_z_score_anomalies': modified_z_anomalies,
            'iqr_anomalies': iqr_anomalies,
            'extreme_iqr_anomalies': extreme_iqr_anomalies,
            'rolling_anomalies': rolling_anomalies,
            'z_scores': z_scores,
            'modified_z_scores': modified_z_scores
        }
    
    def _ml_anomaly_detection(self, series):
        """æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹"""
        print("ğŸ¤– æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹...")
        
        # å‡†å¤‡æ•°æ®
        X = series.values.reshape(-1, 1)
        X_scaled = self.scaler.fit_transform(X)
        
        results = {}
        
        # Isolation Forest
        try:
            iso_forest = IsolationForest(contamination=0.02, random_state=42)  # 2%æ›´åˆç†
            iso_predictions = iso_forest.fit_predict(X_scaled)
            iso_anomalies = iso_predictions == -1
            iso_scores = iso_forest.decision_function(X_scaled)
            
            results['isolation_forest'] = {
                'anomalies': iso_anomalies,
                'scores': iso_scores
            }
        except Exception as e:
            print(f"âš ï¸ Isolation Forestå¤±è´¥: {e}")
        
        # One-Class SVM
        try:
            oc_svm = OneClassSVM(nu=0.02, kernel='rbf')  # 2%æ›´åˆç†
            svm_predictions = oc_svm.fit_predict(X_scaled)
            svm_anomalies = svm_predictions == -1
            svm_scores = oc_svm.decision_function(X_scaled)
            
            results['one_class_svm'] = {
                'anomalies': svm_anomalies,
                'scores': svm_scores
            }
        except Exception as e:
            print(f"âš ï¸ One-Class SVMå¤±è´¥: {e}")
        
        # Local Outlier Factor
        try:
            lof = LocalOutlierFactor(n_neighbors=min(20, len(X)//2), contamination=0.1)
            lof_predictions = lof.fit_predict(X_scaled)
            lof_anomalies = lof_predictions == -1
            lof_scores = lof.negative_outlier_factor_
            
            results['local_outlier_factor'] = {
                'anomalies': lof_anomalies,
                'scores': lof_scores
            }
        except Exception as e:
            print(f"âš ï¸ Local Outlier Factorå¤±è´¥: {e}")
        
        return results
    
    def _timeseries_anomaly_detection(self, series):
        """æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹"""
        print("â° æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹...")
        
        # åŸºäºè¶‹åŠ¿çš„å¼‚å¸¸æ£€æµ‹
        window = min(30, len(series) // 10)
        rolling_mean = series.rolling(window=window, center=True).mean()
        trend_anomalies = np.abs(series - rolling_mean) > 2 * series.std()
        
        # åŸºäºå­£èŠ‚æ€§çš„å¼‚å¸¸æ£€æµ‹
        monthly_means = series.groupby(series.index.month).mean()
        monthly_stds = series.groupby(series.index.month).std()
        seasonal_anomalies = np.abs(series - monthly_means[series.index.month].values) > 2 * monthly_stds[series.index.month].values
        
        # åŸºäºå˜åŒ–ç‡çš„å¼‚å¸¸æ£€æµ‹
        diff = series.diff()
        diff_anomalies = np.abs(diff) > 3 * diff.std()
        
        # åŸºäºè‡ªç›¸å…³çš„å¼‚å¸¸æ£€æµ‹
        autocorr_anomalies = self._autocorrelation_anomaly_detection(series)
        
        return {
            'trend_anomalies': trend_anomalies,
            'seasonal_anomalies': seasonal_anomalies,
            'change_rate_anomalies': diff_anomalies,
            'autocorr_anomalies': autocorr_anomalies
        }
    
    def _autocorrelation_anomaly_detection(self, series):
        """åŸºäºè‡ªç›¸å…³çš„å¼‚å¸¸æ£€æµ‹"""
        try:
            # è®¡ç®—è‡ªç›¸å…³
            autocorr = series.autocorr(lag=1)
            
            # å¦‚æœè‡ªç›¸å…³å¾ˆä½ï¼Œå¯èƒ½å­˜åœ¨å¼‚å¸¸
            autocorr_anomalies = pd.Series([False] * len(series), index=series.index)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è‡ªç›¸å…³å¼‚å¸¸æ£€æµ‹é€»è¾‘
            return autocorr_anomalies
        except:
            return pd.Series([False] * len(series), index=series.index)
    
    def _ensemble_anomaly_detection(self, statistical, ml, ts):
        """é›†æˆå¼‚å¸¸æ£€æµ‹"""
        print("ğŸ”— é›†æˆå¼‚å¸¸æ£€æµ‹...")
        
        # æ”¶é›†æ‰€æœ‰å¼‚å¸¸æ£€æµ‹ç»“æœ
        all_anomalies = []
        
        # ç»Ÿè®¡æ–¹æ³•
        for method in ['z_score_anomalies', 'iqr_anomalies', 'rolling_anomalies']:
            if method in statistical:
                all_anomalies.append(statistical[method].astype(int))
        
        # æœºå™¨å­¦ä¹ æ–¹æ³•
        for method in ['isolation_forest', 'one_class_svm', 'local_outlier_factor']:
            if method in ml and 'anomalies' in ml[method]:
                all_anomalies.append(ml[method]['anomalies'].astype(int))
        
        # æ—¶é—´åºåˆ—æ–¹æ³•
        for method in ['trend_anomalies', 'seasonal_anomalies']:
            if method in ts:
                all_anomalies.append(ts[method].astype(int))
        
        if not all_anomalies:
            return {'ensemble_anomalies': pd.Series([False] * len(self.data), index=self.data.index)}
        
        # è®¡ç®—é›†æˆåˆ†æ•°
        ensemble_scores = np.mean(all_anomalies, axis=0)
        
        # åŠ¨æ€é˜ˆå€¼
        threshold = np.percentile(ensemble_scores, 90)  # å‰10%ä½œä¸ºå¼‚å¸¸
        ensemble_anomalies = ensemble_scores > threshold
        
        return {
            'ensemble_scores': ensemble_scores,
            'ensemble_anomalies': ensemble_anomalies,
            'threshold': threshold,
            'n_methods': len(all_anomalies)
        }
    
    def _explain_anomalies(self, series, ensemble_anomalies):
        """å¼‚å¸¸è§£é‡Š"""
        print("ğŸ” å¼‚å¸¸è§£é‡Š...")
        
        if 'ensemble_anomalies' not in ensemble_anomalies:
            return {}
        
        anomalies = ensemble_anomalies['ensemble_anomalies']
        # ä¿®å¤æ•°ç»„æ¯”è¾ƒé—®é¢˜
        if isinstance(anomalies, (list, np.ndarray)):
            anomalies = np.array(anomalies)
        else:
            return {}
            
        anomaly_indices = series.index[anomalies]
        
        explanations = []
        for idx in anomaly_indices[:10]:  # åªè§£é‡Šå‰10ä¸ªå¼‚å¸¸
            value = series.loc[idx]
            mean_val = series.mean()
            std_val = series.std()
            
            explanation = {
                'timestamp': idx,
                'value': value,
                'deviation': (value - mean_val) / std_val if std_val > 0 else 0,
                'type': 'high' if value > mean_val else 'low',
                'context': f"å€¼ {value:.2f} åç¦»å‡å€¼ {mean_val:.2f} çº¦ {abs(value - mean_val)/std_val:.1f} ä¸ªæ ‡å‡†å·®"
            }
            explanations.append(explanation)
        
        return {
            'total_anomalies': anomalies.sum(),
            'anomaly_rate': anomalies.sum() / len(anomalies),
            'explanations': explanations
        }
    
    def clustering_analysis(self, columns=None):
        """
        èšç±»åˆ†æ - å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼
        
        Args:
            columns (list): è¦åˆ†æçš„åˆ—ååˆ—è¡¨
            
        Returns:
            dict: èšç±»åˆ†æç»“æœ
        """
        print(f"\nğŸ” æ‰§è¡Œèšç±»åˆ†æ")
        print("=" * 60)
        
        if self.data is None:
            return {}
        
        # é€‰æ‹©æ•°å€¼åˆ—
        if columns is None:
            numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()
        else:
            numeric_columns = [col for col in columns if col in self.data.columns]
        
        if not numeric_columns:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—")
            return {}
        
        # å‡†å¤‡æ•°æ®
        data_subset = self.data[numeric_columns].dropna()
        if len(data_subset) == 0:
            print("âŒ æ•°æ®ä¸ºç©º")
            return {}
        
        try:
            # æ•°æ®æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(data_subset)
            
            # 1. K-meansèšç±»
            kmeans_results = self._kmeans_clustering(X_scaled, data_subset)
            
            # 2. DBSCANèšç±»
            dbscan_results = self._dbscan_clustering(X_scaled, data_subset)
            
            # 3. å±‚æ¬¡èšç±»
            hierarchical_results = self._hierarchical_clustering(X_scaled, data_subset)
            
            # 4. èšç±»è¯„ä¼°
            clustering_evaluation = self._evaluate_clustering(X_scaled, kmeans_results, dbscan_results, hierarchical_results)
            
            # 5. èšç±»è§£é‡Š
            cluster_interpretations = self._interpret_clusters(data_subset, kmeans_results, dbscan_results, hierarchical_results)
            
            results = {
                'kmeans': kmeans_results,
                'dbscan': dbscan_results,
                'hierarchical': hierarchical_results,
                'evaluation': clustering_evaluation,
                'interpretations': cluster_interpretations,
                'features_used': numeric_columns
            }
            
            # Add interpretation for clustering analysis
            results['interpretation'] = self._interpret_clustering_results(results)
            
            self.analysis_results['clustering_analysis'] = results
            print("âœ… Clustering analysis completed")
            return results
            
        except Exception as e:
            print(f"âŒ èšç±»åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _kmeans_clustering(self, X_scaled, data_subset):
        """K-meansèšç±»"""
        print("ğŸ¯ K-meansèšç±»...")
        
        # ç¡®å®šæœ€ä¼˜èšç±»æ•°
        n_clusters_range = range(2, min(11, len(data_subset)//10))
        inertias = []
        silhouette_scores = []
        
        for n_clusters in n_clusters_range:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)
            
            inertias.append(kmeans.inertia_)
            if n_clusters > 1:
                silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))
            else:
                silhouette_scores.append(0)
        
        # é€‰æ‹©æœ€ä¼˜èšç±»æ•°ï¼ˆè‚˜éƒ¨æ³•åˆ™ + è½®å»“ç³»æ•°ï¼‰
        optimal_k = n_clusters_range[np.argmax(silhouette_scores)]
        
        # æ‰§è¡Œæœ€ç»ˆèšç±»
        final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        final_labels = final_kmeans.fit_predict(X_scaled)
        
        return {
            'n_clusters': optimal_k,
            'labels': final_labels,
            'centers': final_kmeans.cluster_centers_,
            'inertia': final_kmeans.inertia_,
            'silhouette_score': silhouette_score(X_scaled, final_labels),
            'inertias': inertias,
            'silhouette_scores': silhouette_scores,
            'n_clusters_range': list(n_clusters_range)
        }
    
    def _dbscan_clustering(self, X_scaled, data_subset):
        """DBSCANèšç±»"""
        print("ğŸŒ DBSCANèšç±»...")
        
        # å°è¯•ä¸åŒçš„epså€¼
        eps_values = [0.1, 0.2, 0.5, 1.0, 2.0]
        best_eps = 0.5
        best_n_clusters = 0
        
        for eps in eps_values:
            dbscan = DBSCAN(eps=eps, min_samples=5)
            labels = dbscan.fit_predict(X_scaled)
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            
            if n_clusters > best_n_clusters and n_clusters > 1:
                best_eps = eps
                best_n_clusters = n_clusters
        
        # æ‰§è¡Œæœ€ç»ˆèšç±»
        final_dbscan = DBSCAN(eps=best_eps, min_samples=5)
        final_labels = final_dbscan.fit_predict(X_scaled)
        
        return {
            'eps': best_eps,
            'labels': final_labels,
            'n_clusters': len(set(final_labels)) - (1 if -1 in final_labels else 0),
            'n_noise': list(final_labels).count(-1),
            'core_samples': final_dbscan.core_sample_indices_
        }
    
    def _hierarchical_clustering(self, X_scaled, data_subset):
        """å±‚æ¬¡èšç±»"""
        print("ğŸŒ³ å±‚æ¬¡èšç±»...")
        
        # ä½¿ç”¨ä¸åŒçš„é“¾æ¥æ–¹æ³•
        linkage_methods = ['ward', 'complete', 'average', 'single']
        best_method = 'ward'
        best_score = -1
        
        for method in linkage_methods:
            try:
                clustering = AgglomerativeClustering(n_clusters=3, linkage=method)
                labels = clustering.fit_predict(X_scaled)
                
                if len(set(labels)) > 1:
                    score = silhouette_score(X_scaled, labels)
                    if score > best_score:
                        best_score = score
                        best_method = method
            except:
                continue
        
        # æ‰§è¡Œæœ€ç»ˆèšç±»
        final_clustering = AgglomerativeClustering(n_clusters=3, linkage=best_method)
        final_labels = final_clustering.fit_predict(X_scaled)
        
        return {
            'linkage_method': best_method,
            'labels': final_labels,
            'n_clusters': len(set(final_labels)),
            'silhouette_score': best_score
        }
    
    def _evaluate_clustering(self, X_scaled, kmeans_results, dbscan_results, hierarchical_results):
        """èšç±»è¯„ä¼°"""
        print("ğŸ“Š èšç±»è¯„ä¼°...")
        
        evaluation = {}
        
        # K-meansè¯„ä¼°
        if 'labels' in kmeans_results:
            kmeans_labels = kmeans_results['labels']
            evaluation['kmeans'] = {
                'silhouette_score': silhouette_score(X_scaled, kmeans_labels),
                'calinski_harabasz_score': calinski_harabasz_score(X_scaled, kmeans_labels),
                'n_clusters': len(set(kmeans_labels))
            }
        
        # DBSCANè¯„ä¼°
        if 'labels' in dbscan_results:
            dbscan_labels = dbscan_results['labels']
            if len(set(dbscan_labels)) > 1:
                evaluation['dbscan'] = {
                    'silhouette_score': silhouette_score(X_scaled, dbscan_labels),
                    'calinski_harabasz_score': calinski_harabasz_score(X_scaled, dbscan_labels),
                    'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0),
                    'n_noise': list(dbscan_labels).count(-1)
                }
        
        # å±‚æ¬¡èšç±»è¯„ä¼°
        if 'labels' in hierarchical_results:
            hierarchical_labels = hierarchical_results['labels']
            evaluation['hierarchical'] = {
                'silhouette_score': silhouette_score(X_scaled, hierarchical_labels),
                'calinski_harabasz_score': calinski_harabasz_score(X_scaled, hierarchical_labels),
                'n_clusters': len(set(hierarchical_labels))
            }
        
        return evaluation
    
    def _interpret_clusters(self, data_subset, kmeans_results, dbscan_results, hierarchical_results):
        """èšç±»è§£é‡Š"""
        print("ğŸ” èšç±»è§£é‡Š...")
        
        interpretations = {}
        
        # K-meansèšç±»è§£é‡Š
        if 'labels' in kmeans_results:
            kmeans_labels = kmeans_results['labels']
            cluster_stats = data_subset.groupby(kmeans_labels).describe()
            interpretations['kmeans'] = {
                'cluster_statistics': cluster_stats,
                'cluster_characteristics': self._describe_cluster_characteristics(data_subset, kmeans_labels)
            }
        
        # DBSCANèšç±»è§£é‡Š
        if 'labels' in dbscan_results:
            dbscan_labels = dbscan_results['labels']
            if len(set(dbscan_labels)) > 1:
                cluster_stats = data_subset.groupby(dbscan_labels).describe()
                interpretations['dbscan'] = {
                    'cluster_statistics': cluster_stats,
                    'cluster_characteristics': self._describe_cluster_characteristics(data_subset, dbscan_labels)
                }
        
        # å±‚æ¬¡èšç±»è§£é‡Š
        if 'labels' in hierarchical_results:
            hierarchical_labels = hierarchical_results['labels']
            cluster_stats = data_subset.groupby(hierarchical_labels).describe()
            interpretations['hierarchical'] = {
                'cluster_statistics': cluster_stats,
                'cluster_characteristics': self._describe_cluster_characteristics(data_subset, hierarchical_labels)
            }
        
        return interpretations
    
    def _describe_cluster_characteristics(self, data_subset, labels):
        """æè¿°èšç±»ç‰¹å¾"""
        characteristics = {}
        
        for cluster_id in set(labels):
            if cluster_id == -1:  # è·³è¿‡å™ªå£°ç‚¹
                continue
                
            # ä¿®å¤æ•°ç»„æ¯”è¾ƒé—®é¢˜
            cluster_mask = np.array(labels) == cluster_id
            cluster_data = data_subset[cluster_mask]
            cluster_characteristics = {}
            
            for column in cluster_data.columns:
                values = cluster_data[column]
                cluster_characteristics[column] = {
                    'mean': values.mean(),
                    'std': values.std(),
                    'min': values.min(),
                    'max': values.max(),
                    'median': values.median()
                }
            
            characteristics[f'cluster_{cluster_id}'] = cluster_characteristics
        
        return characteristics
    
    def dimensionality_reduction_analysis(self, columns=None):
        """
        é™ç»´åˆ†æ - å‘ç°æ•°æ®çš„ä¸»è¦æˆåˆ†å’Œç»“æ„
        
        Args:
            columns (list): è¦åˆ†æçš„åˆ—ååˆ—è¡¨
            
        Returns:
            dict: é™ç»´åˆ†æç»“æœ
        """
        print(f"\nğŸ“‰ æ‰§è¡Œé™ç»´åˆ†æ")
        print("=" * 60)
        
        if self.data is None:
            return {}
        
        # é€‰æ‹©æ•°å€¼åˆ—
        if columns is None:
            numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()
        else:
            numeric_columns = [col for col in columns if col in self.data.columns]
        
        if not numeric_columns:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—")
            return {}
        
        # å‡†å¤‡æ•°æ®
        data_subset = self.data[numeric_columns].dropna()
        if len(data_subset) == 0:
            print("âŒ æ•°æ®ä¸ºç©º")
            return {}
        
        try:
            # æ•°æ®æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(data_subset)
            
            # 1. ä¸»æˆåˆ†åˆ†æ (PCA)
            pca_results = self._pca_analysis(X_scaled, data_subset)
            
            # 2. ç‹¬ç«‹æˆåˆ†åˆ†æ (ICA)
            ica_results = self._ica_analysis(X_scaled, data_subset)
            
            # 3. t-SNEé™ç»´
            tsne_results = self._tsne_analysis(X_scaled, data_subset)
            
            # 4. é™ç»´è¯„ä¼°
            reduction_evaluation = self._evaluate_dimensionality_reduction(X_scaled, pca_results, ica_results, tsne_results)
            
            results = {
                'pca': pca_results,
                'ica': ica_results,
                'tsne': tsne_results,
                'evaluation': reduction_evaluation,
                'features_used': numeric_columns
            }
            
            self.analysis_results['dimensionality_reduction'] = results
            print("âœ… é™ç»´åˆ†æå®Œæˆ")
            return results
            
        except Exception as e:
            print(f"âŒ é™ç»´åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _pca_analysis(self, X_scaled, data_subset):
        """ä¸»æˆåˆ†åˆ†æ"""
        print("ğŸ“Š ä¸»æˆåˆ†åˆ†æ...")
        
        # æ‰§è¡ŒPCA
        pca = PCA()
        pca_transformed = pca.fit_transform(X_scaled)
        
        # è®¡ç®—ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”
        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)
        
        # æ‰¾åˆ°è§£é‡Š95%æ–¹å·®çš„ä¸»æˆåˆ†æ•°
        n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
        
        # æ‰¾åˆ°è§£é‡Š90%æ–¹å·®çš„ä¸»æˆåˆ†æ•°
        n_components_90 = np.argmax(cumulative_variance_ratio >= 0.90) + 1
        
        return {
            'transformed_data': pca_transformed,
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'cumulative_variance_ratio': cumulative_variance_ratio,
            'n_components_95': n_components_95,
            'n_components_90': n_components_90,
            'components': pca.components_,
            'feature_names': data_subset.columns.tolist()
        }
    
    def _ica_analysis(self, X_scaled, data_subset):
        """ç‹¬ç«‹æˆåˆ†åˆ†æ"""
        print("ğŸ”„ ç‹¬ç«‹æˆåˆ†åˆ†æ...")
        
        try:
            # æ‰§è¡ŒICA
            ica = FastICA(n_components=min(5, X_scaled.shape[1]), random_state=42)
            ica_transformed = ica.fit_transform(X_scaled)
            
            return {
                'transformed_data': ica_transformed,
                'components': ica.components_,
                'mixing_matrix': ica.mixing_,
                'feature_names': data_subset.columns.tolist()
            }
        except Exception as e:
            print(f"âš ï¸ ICAåˆ†æå¤±è´¥: {e}")
            return {}
    
    def _tsne_analysis(self, X_scaled, data_subset):
        """t-SNEé™ç»´"""
        print("ğŸ¨ t-SNEé™ç»´...")
        
        try:
            # å¦‚æœæ•°æ®ç‚¹å¤ªå¤šï¼Œå…ˆè¿›è¡Œé‡‡æ ·
            if len(X_scaled) > 1000:
                indices = np.random.choice(len(X_scaled), 1000, replace=False)
                X_sample = X_scaled[indices]
            else:
                X_sample = X_scaled
                indices = np.arange(len(X_scaled))
            
            # æ‰§è¡Œt-SNE
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_sample)//4))
            tsne_transformed = tsne.fit_transform(X_sample)
            
            return {
                'transformed_data': tsne_transformed,
                'sample_indices': indices,
                'perplexity': tsne.perplexity,
                'kl_divergence': tsne.kl_divergence_
            }
        except Exception as e:
            print(f"âš ï¸ t-SNEåˆ†æå¤±è´¥: {e}")
            return {}
    
    def _evaluate_dimensionality_reduction(self, X_scaled, pca_results, ica_results, tsne_results):
        """é™ç»´è¯„ä¼°"""
        print("ğŸ“Š é™ç»´è¯„ä¼°...")
        
        evaluation = {}
        
        # PCAè¯„ä¼°
        if 'explained_variance_ratio' in pca_results:
            evaluation['pca'] = {
                'total_variance_explained': pca_results['explained_variance_ratio'].sum(),
                'first_component_variance': pca_results['explained_variance_ratio'][0],
                'n_components_95': pca_results['n_components_95'],
                'n_components_90': pca_results['n_components_90']
            }
        
        # ICAè¯„ä¼°
        if 'transformed_data' in ica_results:
            evaluation['ica'] = {
                'n_components': ica_results['transformed_data'].shape[1],
                'component_independence': 'ICA components are statistically independent'
            }
        
        # t-SNEè¯„ä¼°
        if 'kl_divergence' in tsne_results:
            evaluation['tsne'] = {
                'kl_divergence': tsne_results['kl_divergence'],
                'n_samples': len(tsne_results['transformed_data'])
            }
        
        return evaluation
    
    def statistical_hypothesis_testing(self, column='snow_water_equivalent_mm'):
        """
        ç»Ÿè®¡å‡è®¾æ£€éªŒ - ä¸“ä¸šç»Ÿè®¡æ–¹æ³•
        
        Args:
            column (str): è¦åˆ†æçš„åˆ—å
            
        Returns:
            dict: å‡è®¾æ£€éªŒç»“æœ
        """
        print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡å‡è®¾æ£€éªŒ: {column}")
        print("=" * 60)
        
        if self.data is None or column not in self.data.columns:
            return {}
        
        series = self.data[column].dropna()
        if len(series) == 0:
            return {}
        
        try:
            # 1. æ­£æ€æ€§æ£€éªŒ
            normality_tests = self._normality_tests(series)
            
            # 2. å¹³ç¨³æ€§æ£€éªŒ
            stationarity_tests = self._stationarity_tests(series)
            
            # 3. å­£èŠ‚æ€§æ£€éªŒ
            seasonality_tests = self._seasonality_tests(series)
            
            # 4. è¶‹åŠ¿æ£€éªŒ
            trend_tests = self._trend_tests(series)
            
            # 5. å¤šé‡æ¯”è¾ƒæ ¡æ­£
            multiple_comparison_correction = self._multiple_comparison_correction(
                normality_tests, stationarity_tests, seasonality_tests, trend_tests
            )
            
            results = {
                'normality_tests': normality_tests,
                'stationarity_tests': stationarity_tests,
                'seasonality_tests': seasonality_tests,
                'trend_tests': trend_tests,
                'multiple_comparison_correction': multiple_comparison_correction
            }
            
            # Add interpretation for statistical hypothesis testing
            results['interpretation'] = self._interpret_statistical_test_results(results)
            
            self.analysis_results['statistical_hypothesis_testing'] = results
            print("âœ… Statistical hypothesis testing completed")
            return results
            
        except Exception as e:
            print(f"âŒ ç»Ÿè®¡å‡è®¾æ£€éªŒå¤±è´¥: {e}")
            return {}
    
    def _normality_tests(self, series):
        """æ­£æ€æ€§æ£€éªŒ"""
        print("ğŸ“Š æ­£æ€æ€§æ£€éªŒ...")
        
        tests = {}
        
        # Shapiro-Wilkæ£€éªŒ
        try:
            from scipy.stats import shapiro
            stat, p_value = shapiro(series.sample(min(5000, len(series))))
            tests['shapiro_wilk'] = {
                'statistic': stat,
                'p_value': p_value,
                'is_normal': p_value > 0.05
            }
        except Exception as e:
            print(f"âš ï¸ Shapiro-Wilkæ£€éªŒå¤±è´¥: {e}")
        
        # Kolmogorov-Smirnovæ£€éªŒ
        try:
            from scipy.stats import kstest, norm
            mean, std = series.mean(), series.std()
            stat, p_value = kstest(series, lambda x: norm.cdf(x, mean, std))
            tests['kolmogorov_smirnov'] = {
                'statistic': stat,
                'p_value': p_value,
                'is_normal': p_value > 0.05
            }
        except Exception as e:
            print(f"âš ï¸ Kolmogorov-Smirnovæ£€éªŒå¤±è´¥: {e}")
        
        # Anderson-Darlingæ£€éªŒ
        try:
            from scipy.stats import anderson
            result = anderson(series, dist='norm')
            tests['anderson_darling'] = {
                'statistic': result.statistic,
                'critical_values': result.critical_values,
                'significance_levels': result.significance_level,
                'is_normal': result.statistic < result.critical_values[2]  # 5%æ˜¾è‘—æ€§æ°´å¹³
            }
        except Exception as e:
            print(f"âš ï¸ Anderson-Darlingæ£€éªŒå¤±è´¥: {e}")
        
        return tests
    
    def _stationarity_tests(self, series):
        """å¹³ç¨³æ€§æ£€éªŒ"""
        print("ğŸ“Š å¹³ç¨³æ€§æ£€éªŒ...")
        
        tests = {}
        
        # ADFæ£€éªŒ (Augmented Dickey-Fuller)
        try:
            from statsmodels.tsa.stattools import adfuller
            result = adfuller(series.dropna())
            tests['adf'] = {
                'statistic': result[0],
                'p_value': result[1],
                'critical_values': result[4],
                'is_stationary': result[1] < 0.05
            }
        except Exception as e:
            print(f"âš ï¸ ADFæ£€éªŒå¤±è´¥: {e}")
        
        # KPSSæ£€éªŒ
        try:
            from statsmodels.tsa.stattools import kpss
            result = kpss(series.dropna(), regression='c')
            tests['kpss'] = {
                'statistic': result[0],
                'p_value': result[1],
                'critical_values': result[3],
                'is_stationary': result[1] > 0.05
            }
        except Exception as e:
            print(f"âš ï¸ KPSSæ£€éªŒå¤±è´¥: {e}")
        
        return tests
    
    def _seasonality_tests(self, series):
        """å­£èŠ‚æ€§æ£€éªŒ"""
        print("ğŸ“Š å­£èŠ‚æ€§æ£€éªŒ...")
        
        tests = {}
        
        # æœˆåº¦Kruskal-Wallisæ£€éªŒ
        try:
            monthly_groups = [series[series.index.month == month].values for month in range(1, 13)]
            monthly_groups = [group for group in monthly_groups if len(group) > 0]
            
            if len(monthly_groups) > 1:
                stat, p_value = kruskal(*monthly_groups)
                tests['monthly_kruskal_wallis'] = {
                    'statistic': stat,
                    'p_value': p_value,
                    'has_seasonality': p_value < 0.05
                }
        except Exception as e:
            print(f"âš ï¸ æœˆåº¦Kruskal-Wallisæ£€éªŒå¤±è´¥: {e}")
        
        # å­£èŠ‚æ€§å¼ºåº¦æ£€éªŒ
        try:
            # è®¡ç®—å­£èŠ‚æ€§å¼ºåº¦
            monthly_means = series.groupby(series.index.month).mean()
            seasonal_strength = monthly_means.std() / series.std()
            
            tests['seasonal_strength'] = {
                'strength': seasonal_strength,
                'is_strong_seasonal': seasonal_strength > 0.3
            }
        except Exception as e:
            print(f"âš ï¸ å­£èŠ‚æ€§å¼ºåº¦æ£€éªŒå¤±è´¥: {e}")
        
        return tests
    
    def _trend_tests(self, series):
        """è¶‹åŠ¿æ£€éªŒ"""
        print("ğŸ“Š è¶‹åŠ¿æ£€éªŒ...")
        
        tests = {}
        
        # Mann-Kendallè¶‹åŠ¿æ£€éªŒ
        try:
            from scipy.stats import kendalltau
            x = np.arange(len(series))
            tau, p_value = kendalltau(x, series.values)
            
            tests['mann_kendall'] = {
                'tau': tau,
                'p_value': p_value,
                'has_trend': p_value < 0.05,
                'trend_direction': 'increasing' if tau > 0 else 'decreasing'
            }
        except Exception as e:
            print(f"âš ï¸ Mann-Kendallè¶‹åŠ¿æ£€éªŒå¤±è´¥: {e}")
        
        # çº¿æ€§è¶‹åŠ¿æ£€éªŒ
        try:
            from scipy.stats import linregress
            x = np.arange(len(series))
            slope, intercept, r_value, p_value, std_err = linregress(x, series.values)
            
            tests['linear_trend'] = {
                'slope': slope,
                'intercept': intercept,
                'r_squared': r_value**2,
                'p_value': p_value,
                'std_error': std_err,
                'has_trend': p_value < 0.05,
                'trend_direction': 'increasing' if slope > 0 else 'decreasing'
            }
        except Exception as e:
            print(f"âš ï¸ çº¿æ€§è¶‹åŠ¿æ£€éªŒå¤±è´¥: {e}")
        
        return tests
    
    def _multiple_comparison_correction(self, *test_results):
        """å¤šé‡æ¯”è¾ƒæ ¡æ­£"""
        print("ğŸ“Š å¤šé‡æ¯”è¾ƒæ ¡æ­£...")
        
        # æ”¶é›†æ‰€æœ‰på€¼
        all_p_values = []
        test_names = []
        
        for test_group in test_results:
            for test_name, test_data in test_group.items():
                if isinstance(test_data, dict) and 'p_value' in test_data:
                    all_p_values.append(test_data['p_value'])
                    test_names.append(test_name)
        
        if not all_p_values:
            return {}
        
        # Bonferroniæ ¡æ­£
        from statsmodels.stats.multitest import multipletests
        try:
            rejected, p_corrected, alpha_sidak, alpha_bonf = multipletests(
                all_p_values, alpha=0.05, method='bonferroni'
            )
            
            # FDRæ ¡æ­£ (Benjamini-Hochberg)
            rejected_fdr, p_corrected_fdr, alpha_sidak_fdr, alpha_bonf_fdr = multipletests(
                all_p_values, alpha=0.05, method='fdr_bh'
            )
            
            return {
                'bonferroni': {
                    'corrected_p_values': dict(zip(test_names, p_corrected)),
                    'rejected': dict(zip(test_names, rejected))
                },
                'fdr_bh': {
                    'corrected_p_values': dict(zip(test_names, p_corrected_fdr)),
                    'rejected': dict(zip(test_names, rejected_fdr))
                },
                'original_p_values': dict(zip(test_names, all_p_values))
            }
        except Exception as e:
            print(f"âš ï¸ å¤šé‡æ¯”è¾ƒæ ¡æ­£å¤±è´¥: {e}")
            return {}
    
    def create_interactive_visualizations(self, save_path=None):
        """
        åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–
        
        Args:
            save_path (str): ä¿å­˜è·¯å¾„
            
        Returns:
            dict: å¯è§†åŒ–ç»“æœ
        """
        print("\nğŸ“Š åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–")
        print("=" * 60)
        
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
            return {}
        
        visualizations = {}
        
        try:
            # 1. æ—¶é—´åºåˆ—åˆ†è§£å¯è§†åŒ–
            if 'advanced_decomposition' in self.analysis_results:
                visualizations['decomposition'] = self._create_decomposition_plot()
            
            # 2. å¼‚å¸¸æ£€æµ‹å¯è§†åŒ–
            if 'advanced_anomaly_detection' in self.analysis_results:
                visualizations['anomaly_detection'] = self._create_anomaly_plot()
            
            # 3. èšç±»åˆ†æå¯è§†åŒ–
            if 'clustering_analysis' in self.analysis_results:
                visualizations['clustering'] = self._create_clustering_plot()
            
            # 4. é™ç»´åˆ†æå¯è§†åŒ–
            if 'dimensionality_reduction' in self.analysis_results:
                visualizations['dimensionality_reduction'] = self._create_dimensionality_reduction_plot()
            
            # 5. ç»Ÿè®¡æ£€éªŒå¯è§†åŒ–
            if 'statistical_hypothesis_testing' in self.analysis_results:
                visualizations['statistical_tests'] = self._create_statistical_tests_plot()
            
            # ä¿å­˜å¯è§†åŒ–
            if save_path:
                self._save_visualizations(visualizations, save_path)
            
            print("âœ… äº¤äº’å¼å¯è§†åŒ–åˆ›å»ºå®Œæˆ")
            return visualizations
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")
            return {}
    
    def _create_decomposition_plot(self):
        """åˆ›å»ºæ—¶é—´åºåˆ—åˆ†è§£å›¾"""
        decomposition = self.analysis_results['advanced_decomposition']
        
        if 'stl_decomposition' not in decomposition:
            return {}
        
        stl = decomposition['stl_decomposition']
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=4, cols=1,
            subplot_titles=['åŸå§‹æ•°æ®', 'è¶‹åŠ¿', 'å­£èŠ‚æ€§', 'æ®‹å·®'],
            vertical_spacing=0.05
        )
        
        # åŸå§‹æ•°æ®
        fig.add_trace(
            go.Scatter(
                x=self.data.index,
                y=self.data['snow_water_equivalent_mm'],
                mode='lines',
                name='åŸå§‹æ•°æ®',
                line=dict(color='blue')
            ),
            row=1, col=1
        )
        
        # è¶‹åŠ¿
        if 'trend' in stl:
            fig.add_trace(
                go.Scatter(
                    x=stl['trend'].index,
                    y=stl['trend'].values,
                    mode='lines',
                    name='è¶‹åŠ¿',
                    line=dict(color='red')
                ),
                row=2, col=1
            )
        
        # å­£èŠ‚æ€§
        if 'seasonal' in stl:
            fig.add_trace(
                go.Scatter(
                    x=stl['seasonal'].index,
                    y=stl['seasonal'].values,
                    mode='lines',
                    name='å­£èŠ‚æ€§',
                    line=dict(color='green')
                ),
                row=3, col=1
            )
        
        # æ®‹å·®
        if 'resid' in stl:
            fig.add_trace(
                go.Scatter(
                    x=stl['resid'].index,
                    y=stl['resid'].values,
                    mode='lines',
                    name='æ®‹å·®',
                    line=dict(color='orange')
                ),
                row=4, col=1
            )
        
        fig.update_layout(
            title='é«˜çº§æ—¶é—´åºåˆ—åˆ†è§£',
            height=800,
            showlegend=False
        )
        
        return fig
    
    def _create_anomaly_plot(self):
        """åˆ›å»ºå¼‚å¸¸æ£€æµ‹å›¾"""
        anomaly_detection = self.analysis_results['advanced_anomaly_detection']
        
        if 'ensemble' not in anomaly_detection:
            return {}
        
        ensemble = anomaly_detection['ensemble']
        
        # åˆ›å»ºæ•£ç‚¹å›¾
        fig = go.Figure()
        
        # æ­£å¸¸ç‚¹
        normal_mask = ~ensemble['ensemble_anomalies']
        fig.add_trace(
            go.Scatter(
                x=self.data.index[normal_mask],
                y=self.data['snow_water_equivalent_mm'][normal_mask],
                mode='markers',
                name='æ­£å¸¸ç‚¹',
                marker=dict(color='blue', size=4)
            )
        )
        
        # å¼‚å¸¸ç‚¹
        anomaly_mask = ensemble['ensemble_anomalies']
        fig.add_trace(
            go.Scatter(
                x=self.data.index[anomaly_mask],
                y=self.data['snow_water_equivalent_mm'][anomaly_mask],
                mode='markers',
                name='å¼‚å¸¸ç‚¹',
                marker=dict(color='red', size=8)
            )
        )
        
        fig.update_layout(
            title='å¼‚å¸¸æ£€æµ‹ç»“æœ',
            xaxis_title='æ—¶é—´',
            yaxis_title='é›ªæ°´å½“é‡ (mm)',
            height=500
        )
        
        return fig
    
    def _create_clustering_plot(self):
        """åˆ›å»ºèšç±»åˆ†æå›¾"""
        clustering = self.analysis_results['clustering_analysis']
        
        if 'kmeans' not in clustering:
            return {}
        
        kmeans = clustering['kmeans']
        
        # åˆ›å»ºæ•£ç‚¹å›¾
        fig = go.Figure()
        
        # ä¸ºæ¯ä¸ªèšç±»æ·»åŠ ä¸åŒçš„é¢œè‰²
        colors = px.colors.qualitative.Set1
        
        for cluster_id in set(kmeans['labels']):
            mask = kmeans['labels'] == cluster_id
            fig.add_trace(
                go.Scatter(
                    x=self.data.index[mask],
                    y=self.data['snow_water_equivalent_mm'][mask],
                    mode='markers',
                    name=f'èšç±» {cluster_id}',
                    marker=dict(color=colors[cluster_id % len(colors)], size=6)
                )
            )
        
        fig.update_layout(
            title='K-meansèšç±»ç»“æœ',
            xaxis_title='æ—¶é—´',
            yaxis_title='é›ªæ°´å½“é‡ (mm)',
            height=500
        )
        
        return fig
    
    def _create_dimensionality_reduction_plot(self):
        """åˆ›å»ºé™ç»´åˆ†æå›¾"""
        dim_reduction = self.analysis_results['dimensionality_reduction']
        
        if 'pca' not in dim_reduction:
            return {}
        
        pca = dim_reduction['pca']
        
        # åˆ›å»ºPCAè§£é‡Šæ–¹å·®å›¾
        fig = go.Figure()
        
        fig.add_trace(
            go.Bar(
                x=list(range(1, len(pca['explained_variance_ratio']) + 1)),
                y=pca['explained_variance_ratio'],
                name='è§£é‡Šæ–¹å·®æ¯”'
            )
        )
        
        fig.add_trace(
            go.Scatter(
                x=list(range(1, len(pca['cumulative_variance_ratio']) + 1)),
                y=pca['cumulative_variance_ratio'],
                mode='lines+markers',
                name='ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”',
                line=dict(color='red')
            )
        )
        
        fig.update_layout(
            title='PCAè§£é‡Šæ–¹å·®åˆ†æ',
            xaxis_title='ä¸»æˆåˆ†',
            yaxis_title='è§£é‡Šæ–¹å·®æ¯”',
            height=500
        )
        
        return fig
    
    def _create_statistical_tests_plot(self):
        """åˆ›å»ºç»Ÿè®¡æ£€éªŒå›¾"""
        statistical_tests = self.analysis_results['statistical_hypothesis_testing']
        
        if 'multiple_comparison_correction' not in statistical_tests:
            return {}
        
        correction = statistical_tests['multiple_comparison_correction']
        
        if 'original_p_values' not in correction:
            return {}
        
        # åˆ›å»ºpå€¼æ¯”è¾ƒå›¾
        fig = go.Figure()
        
        test_names = list(correction['original_p_values'].keys())
        original_p_values = list(correction['original_p_values'].values())
        
        if 'bonferroni' in correction and 'corrected_p_values' in correction['bonferroni']:
            bonferroni_p_values = [correction['bonferroni']['corrected_p_values'].get(name, 0) for name in test_names]
        else:
            bonferroni_p_values = [0] * len(test_names)
        
        fig.add_trace(
            go.Bar(
                x=test_names,
                y=original_p_values,
                name='åŸå§‹på€¼',
                marker=dict(color='blue')
            )
        )
        
        fig.add_trace(
            go.Bar(
                x=test_names,
                y=bonferroni_p_values,
                name='Bonferroniæ ¡æ­£på€¼',
                marker=dict(color='red')
            )
        )
        
        # æ·»åŠ æ˜¾è‘—æ€§æ°´å¹³çº¿
        fig.add_hline(y=0.05, line_dash="dash", line_color="green", annotation_text="Î± = 0.05")
        
        fig.update_layout(
            title='ç»Ÿè®¡æ£€éªŒpå€¼æ¯”è¾ƒ',
            xaxis_title='æ£€éªŒæ–¹æ³•',
            yaxis_title='på€¼',
            height=500
        )
        
        return fig
    
    def _save_visualizations(self, visualizations, save_path):
        """ä¿å­˜å¯è§†åŒ–"""
        import os
        
        os.makedirs(save_path, exist_ok=True)
        
        for name, fig in visualizations.items():
            if fig:
                file_path = os.path.join(save_path, f"{name}.html")
                fig.write_html(file_path)
                print(f"ğŸ“Š ä¿å­˜å¯è§†åŒ–: {file_path}")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        print("=" * 80)
        
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
            return
        
        # æŠ¥å‘Šæ ‡é¢˜
        print("ğŸ”¬ æ•°æ®ç§‘å­¦åˆ†æç»¼åˆæŠ¥å‘Š")
        print("=" * 80)
        
        # æ•°æ®æ¦‚è§ˆ
        if self.data is not None:
            print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
            print(f"  æ•°æ®ç‚¹æ•°é‡: {len(self.data):,}")
            print(f"  æ—¶é—´èŒƒå›´: {self.data.index.min()} åˆ° {self.data.index.max()}")
            print(f"  æ•°æ®åˆ—æ•°: {len(self.data.columns)}")
        
        # é«˜çº§æ—¶é—´åºåˆ—åˆ†è§£æŠ¥å‘Š
        if 'advanced_decomposition' in self.analysis_results:
            self._report_decomposition()
        
        # å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
        if 'advanced_anomaly_detection' in self.analysis_results:
            self._report_anomaly_detection()
        
        # èšç±»åˆ†ææŠ¥å‘Š
        if 'clustering_analysis' in self.analysis_results:
            self._report_clustering()
        
        # é™ç»´åˆ†ææŠ¥å‘Š
        if 'dimensionality_reduction' in self.analysis_results:
            self._report_dimensionality_reduction()
        
        # ç»Ÿè®¡æ£€éªŒæŠ¥å‘Š
        if 'statistical_hypothesis_testing' in self.analysis_results:
            self._report_statistical_tests()
        
        print("\n" + "=" * 80)
        print("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    def _report_decomposition(self):
        """æŠ¥å‘Šæ—¶é—´åºåˆ—åˆ†è§£ç»“æœ"""
        print(f"\nğŸ” é«˜çº§æ—¶é—´åºåˆ—åˆ†è§£:")
        
        decomposition = self.analysis_results['advanced_decomposition']
        
        if 'stl_decomposition' in decomposition:
            stl = decomposition['stl_decomposition']
            print(f"  STLåˆ†è§£:")
            print(f"    å­£èŠ‚æ€§å¼ºåº¦: {stl.get('seasonal_strength', 0):.3f}")
            print(f"    è¶‹åŠ¿å¼ºåº¦: {stl.get('trend_strength', 0):.3f}")
        
        if 'periodicity_analysis' in decomposition:
            periodicity = decomposition['periodicity_analysis']
            print(f"  å‘¨æœŸæ€§åˆ†æ:")
            print(f"    ä¸»è¦å‘¨æœŸ: {periodicity.get('main_period', 0):.1f} å¤©")
            print(f"    ä¸»è¦é¢‘ç‡: {periodicity.get('main_frequency', 0):.6f}")
    
    def _report_anomaly_detection(self):
        """æŠ¥å‘Šå¼‚å¸¸æ£€æµ‹ç»“æœ"""
        print(f"\nğŸš¨ å¼‚å¸¸æ£€æµ‹åˆ†æ:")
        
        anomaly_detection = self.analysis_results['advanced_anomaly_detection']
        
        if 'explanations' in anomaly_detection:
            explanations = anomaly_detection['explanations']
            print(f"  æ£€æµ‹åˆ°å¼‚å¸¸: {explanations.get('total_anomalies', 0)} ä¸ª")
            print(f"  å¼‚å¸¸ç‡: {explanations.get('anomaly_rate', 0):.2%}")
        
        if 'ensemble' in anomaly_detection:
            ensemble = anomaly_detection['ensemble']
            print(f"  é›†æˆæ–¹æ³•æ•°: {ensemble.get('n_methods', 0)}")
            print(f"  å¼‚å¸¸é˜ˆå€¼: {ensemble.get('threshold', 0):.3f}")
    
    def _report_clustering(self):
        """æŠ¥å‘Šèšç±»åˆ†æç»“æœ"""
        print(f"\nğŸ” èšç±»åˆ†æ:")
        
        clustering = self.analysis_results['clustering_analysis']
        
        if 'kmeans' in clustering:
            kmeans = clustering['kmeans']
            print(f"  K-meansèšç±»:")
            print(f"    æœ€ä¼˜èšç±»æ•°: {kmeans.get('n_clusters', 0)}")
            print(f"    è½®å»“ç³»æ•°: {kmeans.get('silhouette_score', 0):.3f}")
        
        if 'evaluation' in clustering:
            evaluation = clustering['evaluation']
            print(f"  èšç±»è¯„ä¼°:")
            for method, metrics in evaluation.items():
                if 'silhouette_score' in metrics:
                    print(f"    {method}: è½®å»“ç³»æ•° = {metrics['silhouette_score']:.3f}")
    
    def _report_dimensionality_reduction(self):
        """æŠ¥å‘Šé™ç»´åˆ†æç»“æœ"""
        print(f"\nğŸ“‰ é™ç»´åˆ†æ:")
        
        dim_reduction = self.analysis_results['dimensionality_reduction']
        
        if 'pca' in dim_reduction:
            pca = dim_reduction['pca']
            print(f"  PCAåˆ†æ:")
            print(f"    è§£é‡Š95%æ–¹å·®éœ€è¦: {pca.get('n_components_95', 0)} ä¸ªä¸»æˆåˆ†")
            print(f"    è§£é‡Š90%æ–¹å·®éœ€è¦: {pca.get('n_components_90', 0)} ä¸ªä¸»æˆåˆ†")
            if 'explained_variance_ratio' in pca:
                first_component = pca['explained_variance_ratio'][0]
                print(f"    ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {first_component:.1%}")
    
    def _report_statistical_tests(self):
        """æŠ¥å‘Šç»Ÿè®¡æ£€éªŒç»“æœ"""
        print(f"\nğŸ“Š ç»Ÿè®¡å‡è®¾æ£€éªŒ:")
        
        statistical_tests = self.analysis_results['statistical_hypothesis_testing']
        
        if 'normality_tests' in statistical_tests:
            normality = statistical_tests['normality_tests']
            print(f"  æ­£æ€æ€§æ£€éªŒ:")
            for test_name, test_data in normality.items():
                if 'is_normal' in test_data:
                    print(f"    {test_name}: {'æ­£æ€' if test_data['is_normal'] else 'éæ­£æ€'}")
        
        if 'trend_tests' in statistical_tests:
            trend_tests = statistical_tests['trend_tests']
            print(f"  è¶‹åŠ¿æ£€éªŒ:")
            if 'mann_kendall' in trend_tests:
                mk = trend_tests['mann_kendall']
                if 'has_trend' in mk:
                    print(f"    Mann-Kendall: {'æœ‰è¶‹åŠ¿' if mk['has_trend'] else 'æ— è¶‹åŠ¿'}")
                    if 'trend_direction' in mk:
                        print(f"      è¶‹åŠ¿æ–¹å‘: {mk['trend_direction']}")
    
    def run_comprehensive_analysis(self, column='snow_water_equivalent_mm'):
        """
        è¿è¡Œç»¼åˆåˆ†æ
        
        Args:
            column (str): è¦åˆ†æçš„åˆ—å
            
        Returns:
            dict: æ‰€æœ‰åˆ†æç»“æœ
        """
        print("ğŸš€ å¼€å§‹æ•°æ®ç§‘å­¦ç»¼åˆåˆ†æ")
        print("=" * 80)
        
        if self.data is None:
            print("âŒ æ•°æ®æœªåŠ è½½")
            return {}
        
        try:
            # 1. é«˜çº§æ—¶é—´åºåˆ—åˆ†è§£
            self.advanced_time_series_decomposition(column)
            
            # 2. å¼‚å¸¸æ£€æµ‹
            self.advanced_anomaly_detection(column)
            
            # 3. èšç±»åˆ†æ
            self.clustering_analysis()
            
            # 4. é™ç»´åˆ†æ
            self.dimensionality_reduction_analysis()
            
            # 5. ç»Ÿè®¡å‡è®¾æ£€éªŒ
            self.statistical_hypothesis_testing(column)
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            self.generate_comprehensive_report()
            
            # 7. åˆ›å»ºå¯è§†åŒ–
            visualizations = self.create_interactive_visualizations()
            
            print("\nâœ… æ•°æ®ç§‘å­¦ç»¼åˆåˆ†æå®Œæˆ!")
            return self.analysis_results
            
        except Exception as e:
            print(f"âŒ ç»¼åˆåˆ†æå¤±è´¥: {e}")
            return {}

    def discover_cold_factors(self, target_column='snow_water_equivalent_mm', top_k=10):
        """æ— ç›‘ç£å†·é—¨å½±å“å› ç´ å‘ç°ï¼šåŸºäºç›¸å…³æ€§ç¨€æœ‰æ€§ä¸èšç±»å¼‚è´¨æ€§ã€‚

        è¿”å›æŒ‰"å†·é—¨åº¦"æ’åºçš„å€™é€‰è¦ç´ ï¼ˆè¶Šé«˜è¶Šå†·é—¨ä½†æœ‰å½±å“ï¼‰ã€‚
        """
        if self.data is None or target_column not in self.data.columns:
            return {}

        # ä»…æ•°å€¼ç‰¹å¾
        df = self.data.select_dtypes(include=[np.number]).dropna()
        if target_column not in df.columns:
            return {}

        target = df[target_column]

        # è¯†åˆ«æ—¶é—´ç‰¹å¾ï¼ˆè¿™äº›é€šå¸¸ä¸æ˜¯çœŸæ­£çš„"å†·é—¨"å› ç´ ï¼‰
        time_features = ['year', 'month', 'day', 'day_of_year', 'hour', 'minute']
        time_cols = [col for col in df.columns if any(tf in col.lower() for tf in time_features)]

        # 1) å½±å“åŠ›ï¼š|Spearman| ç›¸å…³æ€§
        impact_scores = {}
        for col in df.columns:
            if col == target_column:
                continue
            try:
                rho, _ = stats.spearmanr(df[col], target)
                impact_scores[col] = float(abs(rho))
            except Exception:
                continue

        # 2) å†·é—¨åº¦ï¼šé‡æ–°è®¾è®¡ - åŸºäºçœŸæ­£çš„ä¸šåŠ¡å®šä¹‰
        coldness_scores = {}
        for col in df.columns:
            if col == target_column:
                continue
            try:
                values = df[col]
                
                # è®¡ç®—å€¼çš„ç¨€æœ‰æ€§ï¼ˆçœŸæ­£çš„å†·é—¨åº¦ï¼‰
                value_counts = values.value_counts()
                total_count = len(values)
                
                # ç¨€æœ‰æ€§åˆ†æ•°ï¼šå€¼å‡ºç°é¢‘ç‡è¶Šä½ï¼Œåˆ†æ•°è¶Šé«˜
                rarity_scores = []
                for val, count in value_counts.items():
                    # ç¨€æœ‰æ€§ = 1 - (å‡ºç°æ¬¡æ•°/æ€»æ•°)ï¼Œè¶Šç¨€æœ‰åˆ†æ•°è¶Šé«˜
                    rarity = 1 - (count / total_count)
                    rarity_scores.append(rarity)
                
                # å¹³å‡ç¨€æœ‰æ€§
                avg_rarity = np.mean(rarity_scores) if rarity_scores else 0
                
                # è®¡ç®—ä¿¡æ¯ç†µï¼ˆç‰¹å¾çš„ä¿¡æ¯é‡ï¼‰
                value_probs = value_counts / total_count
                entropy = -np.sum(value_probs * np.log2(value_probs + 1e-10))
                
                # æ ‡å‡†åŒ–ç†µå€¼åˆ°0-1èŒƒå›´
                max_entropy = np.log2(len(value_counts) + 1e-10)
                normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
                
                # æ—¶é—´ç‰¹å¾æƒ©ç½šï¼šå¤§å¹…é™ä½æ—¶é—´ç‰¹å¾çš„å†·é—¨åº¦
                time_penalty = 0.1 if col in time_cols else 1.0
                
                # æ–°çš„å†·é—¨åº¦è®¡ç®—ï¼šç¨€æœ‰æ€§ + ä¿¡æ¯é‡
                coldness_scores[col] = float((0.7 * avg_rarity + 0.3 * normalized_entropy) * time_penalty)
                
            except Exception:
                coldness_scores[col] = 0.0

        # 3) é¢„æµ‹ä»·å€¼ï¼šåŸºäºç‰¹å¾å¯¹ç›®æ ‡çš„é¢„æµ‹èƒ½åŠ›
        predictive_scores = {}
        for col in df.columns:
            if col == target_column:
                continue
            try:
                # ä½¿ç”¨ç®€å•çš„çº¿æ€§å›å½’RÂ²ä½œä¸ºé¢„æµ‹èƒ½åŠ›æŒ‡æ ‡
                from sklearn.linear_model import LinearRegression
                from sklearn.metrics import r2_score
                
                X = df[col].values.reshape(-1, 1)
                y = target.values
                
                # å¤„ç†ç¼ºå¤±å€¼
                mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))
                if np.sum(mask) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ ·æœ¬
                    X_clean = X[mask]
                    y_clean = y[mask]
                    
                    model = LinearRegression()
                    model.fit(X_clean, y_clean)
                    y_pred = model.predict(X_clean)
                    r2 = r2_score(y_clean, y_pred)
                    predictive_scores[col] = float(max(0, r2))  # RÂ²å¯èƒ½ä¸ºè´Ÿï¼Œæˆ‘ä»¬åªå…³å¿ƒæ­£å€¼
                else:
                    predictive_scores[col] = 0.0
                    
        except Exception:
                predictive_scores[col] = 0.0

        # ç»¼åˆå¾—åˆ†ï¼šæ•°æ®é©±åŠ¨çš„åŠ¨æ€æƒé‡åˆ†é…
        all_cols = [c for c in df.columns if c != target_column]
        def nz(d, c):
            return d.get(c, 0.0)
        
        # è®¡ç®—å„ç»´åº¦çš„ç»Ÿè®¡ç‰¹å¾ï¼Œç”¨äºåŠ¨æ€æƒé‡è°ƒæ•´
        impact_values = [nz(impact_scores, col) for col in all_cols]
        coldness_values = [nz(coldness_scores, col) for col in all_cols]
        predictive_values = [nz(predictive_scores, col) for col in all_cols]
        
        # è®¡ç®—å„ç»´åº¦çš„å˜å¼‚ç³»æ•°ï¼ˆæ ‡å‡†å·®/å‡å€¼ï¼‰ï¼Œç”¨äºæƒé‡è°ƒæ•´
        def get_cv(values):
            if not values or np.mean(values) == 0:
                return 1.0
            return np.std(values) / np.mean(values)
        
        impact_cv = get_cv(impact_values)
        coldness_cv = get_cv(coldness_values)
        predictive_cv = get_cv(predictive_values)
        
        # åŠ¨æ€æƒé‡ï¼šå˜å¼‚ç³»æ•°è¶Šé«˜ï¼Œæƒé‡è¶Šå¤§ï¼ˆåŒºåˆ†åº¦æ›´å¥½ï¼‰
        total_cv = impact_cv + coldness_cv + predictive_cv
        if total_cv > 0:
            impact_weight = impact_cv / total_cv
            coldness_weight = coldness_cv / total_cv
            predictive_weight = predictive_cv / total_cv
        else:
            # é»˜è®¤æƒé‡
            impact_weight = 0.4
            coldness_weight = 0.3
            predictive_weight = 0.3
        
        print(f"ğŸ“Š åŠ¨æ€æƒé‡åˆ†é…: Impact={impact_weight:.3f}, Coldness={coldness_weight:.3f}, Predictive={predictive_weight:.3f}")
        
        combined = []
        for col in all_cols:
            impact = nz(impact_scores, col)
            coldness = nz(coldness_scores, col)
            predictive = nz(predictive_scores, col)
            
            # æ—¶é—´ç‰¹å¾ç‰¹æ®Šå¤„ç†ï¼šå¤§å¹…é™ä½æƒé‡
            if col in time_cols:
                impact *= 0.2  # æ—¶é—´ç‰¹å¾çš„å½±å“åŠ›æƒé‡å¤§å¹…é™ä½
                coldness *= 0.1  # æ—¶é—´ç‰¹å¾çš„å†·é—¨åº¦æƒé‡æä½
            
            # åŠ¨æ€æƒé‡ç»¼åˆå¾—åˆ†
            score = impact_weight * impact + coldness_weight * coldness + predictive_weight * predictive
            
            combined.append((col, score))
        
        combined.sort(key=lambda x: x[1], reverse=True)

        # Add interpretation content
        interpretation = self._interpret_cold_factors_discovery(
            combined[:top_k], impact_scores, coldness_scores, predictive_scores, target_column
        )

        return {
            'target': target_column,
            'top_candidates': combined[:top_k],
            'impact_scores': impact_scores,
            'coldness_scores': coldness_scores,
            'predictive_scores': predictive_scores,
            'interpretation': interpretation
        }
    
    def _interpret_cold_factors_discovery(self, top_candidates, impact_scores, coldness_scores, predictive_scores, target_column):
        """Interpret cold factors discovery results"""
        try:
            interpretation = {
                'summary': '',
                'key_insights': [],
                'business_implications': '',
                'recommendations': [],
                'factor_categories': {}
            }
            
            if not top_candidates:
                interpretation['summary'] = 'No significant cold factors discovered'
                return interpretation
            
            # Generate summary
            top_factor = top_candidates[0]
            interpretation['summary'] = f"Discovered {len(top_candidates)} potential cold factors, with {top_factor[0]} being the most important"
            
            # Key insights
            interpretation['key_insights'] = [
                f"Most important cold factor: {top_factor[0]} (comprehensive score: {top_factor[1]:.3f})",
                f"Impact score: {impact_scores.get(top_factor[0], 0):.3f}",
                f"Coldness score: {coldness_scores.get(top_factor[0], 0):.3f}",
                f"Predictive value score: {predictive_scores.get(top_factor[0], 0):.3f}"
            ]
            
            # Business implications
            if top_factor[1] > 0.5:
                interpretation['business_implications'] = "High-value cold factors discovered that may significantly improve prediction model performance"
            elif top_factor[1] > 0.3:
                interpretation['business_implications'] = "Medium-value cold factors discovered that are worth considering in the model"
            else:
                interpretation['business_implications'] = "Discovered cold factors have limited value, recommend further analysis"
            
            # Recommendations
            interpretation['recommendations'] = [
                f"Consider adding {top_factor[0]} feature to {target_column} prediction model",
                "Perform feature engineering to create derived features based on cold factors",
                "Monitor actual contribution of these factors in the model",
                "Regularly reassess importance of cold factors"
            ]
            
            # Factor categories
            interpretation['factor_categories'] = {
                'high_impact': [col for col, score in top_candidates if impact_scores.get(col, 0) > 0.5],
                'high_coldness': [col for col, score in top_candidates if coldness_scores.get(col, 0) > 0.5],
                'high_predictive': [col for col, score in top_candidates if predictive_scores.get(col, 0) > 0.3]
            }
            
            return interpretation
            
        except Exception as e:
            print(f"âš ï¸ Cold factors interpretation failed: {e}")
            return {
                'summary': 'Interpretation generation failed',
                'key_insights': [],
                'business_implications': 'Further analysis needed',
                'recommendations': ['Check data quality', 'Re-run analysis'],
                'factor_categories': {}
            }
    
    def _interpret_decomposition_results(self, results):
        """Interpret time series decomposition results"""
        try:
            interpretation = {
                'summary': '',
                'key_insights': [],
                'business_implications': '',
                'recommendations': []
            }
            
            if 'stl_decomposition' in results:
                stl = results['stl_decomposition']
                seasonal_strength = stl.get('seasonal_strength', 0)
                trend_strength = stl.get('trend_strength', 0)
                
                interpretation['summary'] = f"Time series decomposition completed with seasonal strength {seasonal_strength:.3f} and trend strength {trend_strength:.3f}"
                
                interpretation['key_insights'] = [
                    f"Seasonal component strength: {seasonal_strength:.3f}",
                    f"Trend component strength: {trend_strength:.3f}",
                    "STL decomposition successfully extracted trend, seasonal, and residual components"
                ]
                
                if seasonal_strength > 0.5:
                    interpretation['business_implications'] = "Strong seasonal patterns detected, seasonal forecasting models recommended"
                elif seasonal_strength > 0.2:
                    interpretation['business_implications'] = "Moderate seasonal patterns, consider seasonal adjustments in forecasting"
                else:
                    interpretation['business_implications'] = "Weak seasonal patterns, focus on trend-based forecasting"
                
                interpretation['recommendations'] = [
                    "Use seasonal decomposition for pattern identification",
                    "Apply seasonal adjustments in forecasting models",
                    "Monitor seasonal strength changes over time"
                ]
            else:
                interpretation['summary'] = "Decomposition analysis completed but STL results not available"
                interpretation['key_insights'] = ["Basic decomposition performed", "Check data quality and seasonality"]
                interpretation['business_implications'] = "Limited decomposition insights available"
                interpretation['recommendations'] = ["Verify data completeness", "Check for sufficient data points"]
            
            return interpretation
            
        except Exception as e:
            print(f"âš ï¸ Decomposition interpretation failed: {e}")
            return {
                'summary': 'Decomposition interpretation failed',
                'key_insights': [],
                'business_implications': 'Further analysis needed',
                'recommendations': ['Check decomposition results', 'Re-run analysis']
            }
    
    def _interpret_anomaly_detection_results(self, results):
        """Interpret anomaly detection results"""
        try:
            interpretation = {
                'summary': '',
                'key_insights': [],
                'business_implications': '',
                'recommendations': []
            }
            
            if 'ensemble' in results:
                ensemble = results['ensemble']
                anomalies = ensemble.get('ensemble_anomalies', [])
                anomaly_count = sum(anomalies) if anomalies else 0
                total_points = len(anomalies) if anomalies else 0
                anomaly_rate = anomaly_count / total_points if total_points > 0 else 0
                
                interpretation['summary'] = f"Anomaly detection completed: {anomaly_count} anomalies found in {total_points} data points ({anomaly_rate:.2%})"
                
                interpretation['key_insights'] = [
                    f"Total anomalies detected: {anomaly_count}",
                    f"Anomaly rate: {anomaly_rate:.2%}",
                    "Ensemble method used for robust anomaly detection"
                ]
                
                if anomaly_rate > 0.1:
                    interpretation['business_implications'] = "High anomaly rate detected, data quality issues or significant events may be present"
                elif anomaly_rate > 0.05:
                    interpretation['business_implications'] = "Moderate anomaly rate, some data quality issues detected"
                else:
                    interpretation['business_implications'] = "Low anomaly rate, data appears to be of good quality"
                
                interpretation['recommendations'] = [
                    "Investigate detected anomalies for data quality issues",
                    "Consider anomaly removal for model training",
                    "Monitor anomaly patterns for system health"
                ]
            else:
                interpretation['summary'] = "Anomaly detection completed but ensemble results not available"
                interpretation['key_insights'] = ["Basic anomaly detection performed", "Check detection method results"]
                interpretation['business_implications'] = "Limited anomaly insights available"
                interpretation['recommendations'] = ["Verify anomaly detection method", "Check for sufficient data"]
            
            return interpretation
            
        except Exception as e:
            print(f"âš ï¸ Anomaly detection interpretation failed: {e}")
            return {
                'summary': 'Anomaly detection interpretation failed',
                'key_insights': [],
                'business_implications': 'Further analysis needed',
                'recommendations': ['Check anomaly detection results', 'Re-run analysis']
            }
    
    def _interpret_clustering_results(self, results):
        """Interpret clustering analysis results"""
        try:
            interpretation = {
                'summary': '',
                'key_insights': [],
                'business_implications': '',
                'recommendations': []
            }
            
            if 'kmeans' in results:
                kmeans = results['kmeans']
                labels = kmeans.get('labels', [])
                silhouette = kmeans.get('silhouette_score', 0)
                n_clusters = len(set(labels)) if labels else 0
                
                interpretation['summary'] = f"Clustering analysis completed: {n_clusters} clusters identified with silhouette score {silhouette:.3f}"
                
                interpretation['key_insights'] = [
                    f"Number of clusters: {n_clusters}",
                    f"Silhouette score: {silhouette:.3f}",
                    "K-means clustering performed successfully"
                ]
                
                if silhouette > 0.7:
                    interpretation['business_implications'] = "Excellent cluster separation, clustering results highly reliable"
                elif silhouette > 0.5:
                    interpretation['business_implications'] = "Good cluster separation, clustering results reliable"
                elif silhouette > 0.3:
                    interpretation['business_implications'] = "Fair cluster separation, clustering results moderately reliable"
                else:
                    interpretation['business_implications'] = "Poor cluster separation, consider different clustering parameters"
                
                interpretation['recommendations'] = [
                    "Use cluster labels for feature engineering",
                    "Analyze cluster characteristics for insights",
                    "Consider adjusting number of clusters if silhouette score is low"
                ]
            else:
                interpretation['summary'] = "Clustering analysis completed but K-means results not available"
                interpretation['key_insights'] = ["Basic clustering performed", "Check clustering method results"]
                interpretation['business_implications'] = "Limited clustering insights available"
                interpretation['recommendations'] = ["Verify clustering method", "Check for sufficient data"]
            
            return interpretation
            
        except Exception as e:
            print(f"âš ï¸ Clustering interpretation failed: {e}")
            return {
                'summary': 'Clustering interpretation failed',
                'key_insights': [],
                'business_implications': 'Further analysis needed',
                'recommendations': ['Check clustering results', 'Re-run analysis']
            }
    
    def _interpret_statistical_test_results(self, results):
        """Interpret statistical hypothesis testing results"""
        try:
            interpretation = {
                'summary': '',
                'key_insights': [],
                'business_implications': '',
                'recommendations': []
            }
            
            if 'normality_tests' in results:
                normality = results['normality_tests']
                stationarity = results.get('stationarity_tests', {})
                
                # Check normality
                normal_vars = []
                for test_name, test_result in normality.items():
                    if isinstance(test_result, dict) and 'p_value' in test_result:
                        if test_result['p_value'] > 0.05:
                            normal_vars.append(test_name)
                
                interpretation['summary'] = f"Statistical testing completed: {len(normal_vars)} variables show normal distribution"
                
                interpretation['key_insights'] = [
                    f"Normal variables: {len(normal_vars)}",
                    f"Non-normal variables: {len(normality) - len(normal_vars)}",
                    "Multiple statistical tests performed for comprehensive analysis"
                ]
                
                if len(normal_vars) > len(normality) / 2:
                    interpretation['business_implications'] = "Most variables are normally distributed, parametric tests recommended"
                else:
                    interpretation['business_implications'] = "Many variables are non-normal, non-parametric tests recommended"
                
                interpretation['recommendations'] = [
                    "Use appropriate statistical tests based on distribution",
                    "Consider data transformations for non-normal variables",
                    "Apply multiple comparison corrections for multiple tests"
                ]
            else:
                interpretation['summary'] = "Statistical testing completed but detailed results not available"
                interpretation['key_insights'] = ["Basic statistical tests performed", "Check test method results"]
                interpretation['business_implications'] = "Limited statistical insights available"
                interpretation['recommendations'] = ["Verify statistical test methods", "Check for sufficient data"]
            
            return interpretation
            
        except Exception as e:
            print(f"âš ï¸ Statistical test interpretation failed: {e}")
            return {
                'summary': 'Statistical test interpretation failed',
                'key_insights': [],
                'business_implications': 'Further analysis needed',
                'recommendations': ['Check statistical test results', 'Re-run analysis']
        }


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸš€ æ•°æ®ç§‘å­¦åˆ†æå™¨")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = DataScienceAnalyzer()
    
    # åŠ è½½æ•°æ®
    data_path = "src/neuralhydrology/data/red_river_basin/timeseries.csv"
    analyzer.load_data(data_path)
    
    if analyzer.data is not None:
        # è¿è¡Œç»¼åˆåˆ†æ
        results = analyzer.run_comprehensive_analysis('snow_water_equivalent_mm')
        
        print(f"\nğŸ“Š åˆ†æç»“æœåŒ…å« {len(results)} ä¸ªæ¨¡å—")
        for module_name in results.keys():
            print(f"  - {module_name}")


if __name__ == "__main__":
    main()
