"""
ç®€åŒ–ç‰ˆæ•°æ®ç§‘å­¦åˆ†æå™¨ - ä¿®å¤ç»ˆç«¯æŠ¥é”™
åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼Œç§»é™¤å¤æ‚çš„ç¼©è¿›é—®é¢˜
"""
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import logging

class DataScienceAnalyzer:
    def __init__(self):
        self.data = None
        self.scaler = StandardScaler()
        self.analysis_results = {}
        
        # åˆ—åæ˜ å°„ - å¤„ç†ä¸åŒæ•°æ®æºçš„åˆ—åå·®å¼‚
        self.column_mapping = {
            'snow_water_equivalent_mm': ['Snow on Grnd (cm)', 'Total Snow (cm)', 'snow_water_equivalent_mm'],
            'snow_depth_mm': ['Snow on Grnd (cm)', 'Total Snow (cm)', 'snow_depth_mm'],
            'snow_fall_mm': ['Total Snow (cm)', 'snow_fall_mm'],
            'streamflow_m3s': ['streamflow_m3s', 'flow_m3s', 'discharge_m3s']
        }
    
    def _find_matching_column(self, target_column):
        """æ‰¾åˆ°åŒ¹é…çš„å®é™…åˆ—å"""
        if target_column in self.data.columns:
            return target_column
        
        # æ£€æŸ¥æ˜ å°„
        if target_column in self.column_mapping:
            for possible_name in self.column_mapping[target_column]:
                if possible_name in self.data.columns:
                    print(f"âœ… æ‰¾åˆ°åŒ¹é…åˆ—: {target_column} -> {possible_name}")
                    return possible_name
        
        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        for col in self.data.columns:
            if target_column.lower() in col.lower() or col.lower() in target_column.lower():
                print(f"âœ… æ¨¡ç³ŠåŒ¹é…åˆ—: {target_column} -> {col}")
                return col
        
        return None
    
    def load_data(self, data_path):
        """åŠ è½½æ•°æ®"""
        try:
            import pandas as pd
            self.data = pd.read_csv(data_path)
            if 'date' in self.data.columns:
                self.data['date'] = pd.to_datetime(self.data['date'])
                self.data.set_index('date', inplace=True)
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def advanced_anomaly_detection(self, column='snow_water_equivalent_mm'):
        """ç®€åŒ–çš„å¼‚å¸¸æ£€æµ‹"""
        print(f"\nğŸš¨ æ‰§è¡Œé«˜çº§å¼‚å¸¸æ£€æµ‹: {column}")
        print("=" * 60)
        
        # å°è¯•æ‰¾åˆ°åŒ¹é…çš„åˆ—å
        actual_column = self._find_matching_column(column)
        if actual_column is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…åˆ—: {column}")
            return {}
        
        if self.data is None or actual_column not in self.data.columns:
            return {}
        
        series = self.data[actual_column].dropna()
        if len(series) == 0:
            return {}
        
        # Z-scoreå¼‚å¸¸æ£€æµ‹
        z_scores = np.abs(stats.zscore(series))
        z_anomalies = (z_scores > 2.0).astype(float)
        
        # Isolation Forest
        X = series.values.reshape(-1, 1)
        iso_forest = IsolationForest(contamination=0.02, random_state=42)
        iso_predictions = iso_forest.fit_predict(X)
        iso_anomalies = (iso_predictions == -1).astype(float)
        
        # é›†æˆç»“æœ
        ensemble_anomalies = ((z_anomalies + iso_anomalies) >= 1).astype(float)
        
        # è®¡ç®—å¼‚å¸¸åˆ†æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
        ensemble_scores = z_scores.tolist()  # ä½¿ç”¨Zåˆ†æ•°ä½œä¸ºå¼‚å¸¸åˆ†æ•°
        
        results = {
            'statistical': {
                'z_score_anomalies': z_anomalies.tolist()
            },
            'machine_learning': {
                'isolation_forest_anomalies': iso_anomalies.tolist()
            },
            'ensemble': {
                'ensemble_anomalies': ensemble_anomalies.tolist(),
                'ensemble_scores': ensemble_scores,
                'threshold': 2.0
            },
            'interpretation': f"æ£€æµ‹åˆ° {int(ensemble_anomalies.sum())} ä¸ªå¼‚å¸¸ç‚¹ï¼Œå æ€»æ•°æ®çš„ {ensemble_anomalies.sum()/len(ensemble_anomalies)*100:.1f}%"
        }
        
        print("âœ… Advanced anomaly detection completed")
        return results
    
    def clustering_analysis(self, columns=None):
        """ç®€åŒ–çš„èšç±»åˆ†æ"""
        print("\nğŸ” æ‰§è¡Œèšç±»åˆ†æ")
        print("=" * 60)
        
        if self.data is None:
            return {}
        
        # é€‰æ‹©æ•°å€¼åˆ—
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return {}
        
        data_subset = self.data[numeric_cols].dropna()
        if len(data_subset) == 0:
            return {}
        
        # æ ‡å‡†åŒ–æ•°æ®
        X_scaled = self.scaler.fit_transform(data_subset)
        
        # K-meansèšç±»
        print("ğŸ¯ K-meansèšç±»...")
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        kmeans_labels = kmeans.fit_predict(X_scaled)
        
        # è®¡ç®—silhouetteåˆ†æ•°
        from sklearn.metrics import silhouette_score
        try:
            kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)
        except:
            kmeans_silhouette = 0.0
        
        # DBSCANèšç±» (è°ƒæ•´å‚æ•°)
        print("ğŸŒ DBSCANèšç±»...")
        dbscan = DBSCAN(eps=0.3, min_samples=10)
        dbscan_labels = dbscan.fit_predict(X_scaled)
        
        # å±‚æ¬¡èšç±»
        hierarchical = AgglomerativeClustering(n_clusters=4)
        hierarchical_labels = hierarchical.fit_predict(X_scaled)
        
        try:
            hierarchical_silhouette = silhouette_score(X_scaled, hierarchical_labels)
        except:
            hierarchical_silhouette = 0.0
        
        results = {
            'kmeans': {
                'labels': kmeans_labels.tolist(),
                'n_clusters': len(set(kmeans_labels)),
                'silhouette_score': float(kmeans_silhouette)
            },
            'dbscan': {
                'labels': dbscan_labels.tolist(),
                'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0),
                'n_noise': int(np.sum(dbscan_labels == -1))
            },
            'hierarchical': {
                'labels': hierarchical_labels.tolist(),
                'n_clusters': len(set(hierarchical_labels)),
                'silhouette_score': float(hierarchical_silhouette)
            },
            'features_used': list(numeric_cols),
            'interpretation': {
                'summary': f"Clustering analysis identified distinct data patterns using multiple algorithms",
                'key_insights': [
                    f"K-means found {len(set(kmeans_labels))} clusters with silhouette score {kmeans_silhouette:.3f}",
                    f"DBSCAN found {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)} clusters with {np.sum(dbscan_labels == -1)} noise points",
                    f"Hierarchical clustering identified {len(set(hierarchical_labels))} clusters with silhouette score {hierarchical_silhouette:.3f}"
                ],
                'business_implications': "Different clustering algorithms reveal complementary perspectives on data structure",
                'recommendations': [
                    "Use K-means for balanced cluster sizes",
                    "Use DBSCAN for density-based grouping with outlier detection",
                    "Use hierarchical clustering for nested cluster relationships"
                ]
            }
        }
        
        print("âœ… Clustering analysis completed")
        return results
    
    def statistical_hypothesis_testing(self, column='snow_water_equivalent_mm'):
        """ç®€åŒ–çš„ç»Ÿè®¡å‡è®¾æ£€éªŒ"""
        print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡å‡è®¾æ£€éªŒ: {column}")
        print("=" * 60)
        
        # å°è¯•æ‰¾åˆ°åŒ¹é…çš„åˆ—å
        actual_column = self._find_matching_column(column)
        if actual_column is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…åˆ—: {column}")
            return {}
        
        if self.data is None or actual_column not in self.data.columns:
            return {}
        
        series = self.data[actual_column].dropna()
        if len(series) == 0:
            return {}
        
        # æ­£æ€æ€§æ£€éªŒ
        print("ğŸ“Š æ­£æ€æ€§æ£€éªŒ...")
        shapiro_stat, shapiro_p = stats.shapiro(series[:5000] if len(series) > 5000 else series)
        
        # å¹³ç¨³æ€§æ£€éªŒï¼ˆç®€åŒ–ç‰ˆï¼‰
        print("ğŸ“Š å¹³ç¨³æ€§æ£€éªŒ...")
        # ä½¿ç”¨ADFæ£€éªŒçš„ç®€åŒ–ç‰ˆæœ¬
        try:
            from statsmodels.tsa.stattools import adfuller
            adf_stat, adf_p, _, _, _, _ = adfuller(series)
            stationarity_test = {'adf_statistic': adf_stat, 'p_value': adf_p}
        except:
            stationarity_test = {'adf_statistic': 0, 'p_value': 1.0}
        
        results = {
            'normality': {
                'shapiro_statistic': float(shapiro_stat),
                'shapiro_p_value': float(shapiro_p)
            },
            'stationarity': stationarity_test,
            'test_names': ['Shapiro-Wilk', 'ADF'],
            'original_p': [float(shapiro_p), float(stationarity_test['p_value'])],
            'bonferroni_p': [float(shapiro_p * 2), float(stationarity_test['p_value'] * 2)],
            'fdr_bh_p': [float(shapiro_p), float(stationarity_test['p_value'])],
            'interpretation': f"æ•°æ®{'ç¬¦åˆ' if shapiro_p > 0.05 else 'ä¸ç¬¦åˆ'}æ­£æ€åˆ†å¸ƒ (p={shapiro_p:.4f})ï¼Œ{'æ˜¯' if stationarity_test['p_value'] < 0.05 else 'ä¸æ˜¯'}å¹³ç¨³åºåˆ—"
        }
        
        print("âœ… Statistical hypothesis testing completed")
        return results
    
    def advanced_time_series_decomposition(self, column='snow_water_equivalent_mm'):
        """ç®€åŒ–çš„æ—¶é—´åºåˆ—åˆ†è§£"""
        print(f"\nğŸ” æ‰§è¡Œé«˜çº§æ—¶é—´åºåˆ—åˆ†è§£: {column}")
        print("=" * 60)
        
        # å°è¯•æ‰¾åˆ°åŒ¹é…çš„åˆ—å
        actual_column = self._find_matching_column(column)
        if actual_column is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…åˆ—: {column}")
            return {}
        
        if self.data is None or actual_column not in self.data.columns:
            return {}
        
        series = self.data[actual_column].dropna()
        if len(series) == 0:
            return {}
        
        # ç®€å•çš„ç§»åŠ¨å¹³å‡åˆ†è§£
        print("ğŸ“Š æ‰§è¡Œç®€åŒ–åˆ†è§£...")
        window = min(365, len(series) // 4)
        
        trend = series.rolling(window=window, center=True).mean()
        seasonal = series - trend
        seasonal = seasonal.rolling(window=7, center=True).mean()  # å‘¨æœŸæ€§
        residual = series - trend - seasonal
        
        # å¡«å……ç¼ºå¤±å€¼
        trend = trend.fillna(method='bfill').fillna(method='ffill')
        seasonal = seasonal.fillna(0)
        residual = residual.fillna(0)
        
        results = {
            'stl_decomposition': {
                'trend': {
                    'index': [t.isoformat() for t in series.index],
                    'values': trend.tolist()
                },
                'seasonal': {
                    'index': [t.isoformat() for t in series.index],
                    'values': seasonal.tolist()
                },
                'resid': {
                    'index': [t.isoformat() for t in series.index],
                    'values': residual.tolist()
                }
            },
            'interpretation': f"æ—¶é—´åºåˆ—åˆ†è§£å®Œæˆï¼šè¶‹åŠ¿èŒƒå›´ {trend.min():.1f} åˆ° {trend.max():.1f}ï¼Œå­£èŠ‚æ€§èŒƒå›´ {seasonal.min():.1f} åˆ° {seasonal.max():.1f}"
        }
        
        print("âœ… Advanced time series decomposition completed")
        return results
    
    def discover_cold_factors(self, target_column='snow_water_equivalent_mm', top_k=10):
        """ç®€åŒ–çš„å› å­å‘ç°"""
        print(f"\nğŸ“Š æ‰§è¡Œå› å­å‘ç°: {target_column}")
        
        if self.data is None or target_column not in self.data.columns:
            return {}
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        factors = []
        
        for col in numeric_cols:
            if col == target_column:
                continue
            
            try:
                correlation = stats.spearmanr(self.data[col].dropna(), 
                                           self.data[target_column].dropna())[0]
                if not np.isnan(correlation):
                    factors.append({
                        'factor': col,
                        'correlation': abs(correlation),
                        'score': abs(correlation)
                    })
            except:
                continue
        
        # æ’åºå¹¶å–å‰kä¸ª
        factors.sort(key=lambda x: x['score'], reverse=True)
        factors = factors[:top_k]
        
        results = {
            'high_predictive': factors,
            'interpretation': f"å‘ç° {len(factors)} ä¸ªé‡è¦å› å­ï¼Œæœ€å¼ºç›¸å…³å› å­æ˜¯ {factors[0]['factor'] if factors else 'None'}"
        }
        
        print("âœ… Factor discovery completed")
        return results
