#!/usr/bin/env python3
"""
HydrAI-SWE é¢„æµ‹ç»“æœéªŒè¯å™¨
ç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¸­é¢„æµ‹ç»“æœçš„è´¨é‡å’Œå¯ä¿¡åº¦
"""

import pandas as pd
import numpy as np
import logging
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœæ•°æ®ç±»"""
    is_valid: bool
    confidence_score: float
    validation_details: Dict[str, Any]
    warnings: List[str]
    errors: List[str]
    recommendations: List[str]
    timestamp: datetime

class PhysicalConstraintValidator:
    """ç‰©ç†çº¦æŸéªŒè¯å™¨"""
    
    def __init__(self):
        # å®šä¹‰ç‰©ç†çº¦æŸèŒƒå›´
        self.constraints = {
            'soil_moisture': {
                'min': 0.0,
                'max': 1.0,
                'unit': 'mÂ³/mÂ³',
                'description': 'åœŸå£¤æ¹¿åº¦åº”åœ¨0-1ä¹‹é—´'
            },
            'snow_water_equivalent': {
                'min': 0.0,
                'max': 2000.0,
                'unit': 'mm',
                'description': 'ç§¯é›ªæ°´å½“é‡åº”åœ¨0-2000mmä¹‹é—´'
            },
            'runoff': {
                'min': 0.0,
                'max': 10000.0,
                'unit': 'mÂ³/s',
                'description': 'å¾„æµåº”åœ¨0-10000mÂ³/sä¹‹é—´'
            },
            'temperature': {
                'min': -50.0,
                'max': 50.0,
                'unit': 'Â°C',
                'description': 'æ¸©åº¦åº”åœ¨-50åˆ°50Â°Cä¹‹é—´'
            },
            'precipitation': {
                'min': 0.0,
                'max': 500.0,
                'unit': 'mm/day',
                'description': 'æ—¥é™æ°´é‡åº”åœ¨0-500mmä¹‹é—´'
            }
        }
    
    def validate_physical_constraints(self, predictions: pd.DataFrame, 
                                   variable_type: str) -> Dict[str, Any]:
        """
        éªŒè¯é¢„æµ‹ç»“æœçš„ç‰©ç†åˆç†æ€§
        
        Args:
            predictions: é¢„æµ‹ç»“æœDataFrame
            variable_type: å˜é‡ç±»å‹ (soil_moisture, snow_water_equivalent, runoff, temperature, precipitation)
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” å¼€å§‹ç‰©ç†çº¦æŸéªŒè¯: {variable_type}")
        
        if variable_type not in self.constraints:
            raise ValueError(f"æœªçŸ¥çš„å˜é‡ç±»å‹: {variable_type}")
        
        constraint = self.constraints[variable_type]
        min_val = constraint['min']
        max_val = constraint['max']
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        out_of_range = predictions[
            (predictions < min_val) | (predictions > max_val)
        ]
        
        # æ£€æŸ¥å¼‚å¸¸è·³è·ƒ
        if len(predictions) > 1:
            diff = predictions.diff().abs()
            jump_threshold = (max_val - min_val) * 0.5  # 50%çš„åˆç†èŒƒå›´ä½œä¸ºè·³è·ƒé˜ˆå€¼
            large_jumps = diff[diff > jump_threshold]
        else:
            large_jumps = pd.Series(dtype=float)
        
        # è®¡ç®—éªŒè¯åˆ†æ•°
        total_points = len(predictions)
        valid_points = total_points - len(out_of_range)
        physical_score = valid_points / total_points if total_points > 0 else 0.0
        
        result = {
            'is_valid': len(out_of_range) == 0,
            'physical_score': physical_score,
            'total_points': total_points,
            'valid_points': valid_points,
            'out_of_range_count': len(out_of_range),
            'out_of_range_values': out_of_range.to_dict() if len(out_of_range) > 0 else {},
            'large_jumps_count': len(large_jumps),
            'large_jumps_values': large_jumps.to_dict() if len(large_jumps) > 0 else {},
            'constraint': constraint,
            'warnings': [],
            'errors': []
        }
        
        # ç”Ÿæˆè­¦å‘Šå’Œé”™è¯¯ä¿¡æ¯
        if len(out_of_range) > 0:
            result['errors'].append(
                f"å‘ç° {len(out_of_range)} ä¸ªè¶…å‡ºç‰©ç†èŒƒå›´çš„é¢„æµ‹å€¼ "
                f"({min_val} - {max_val} {constraint['unit']})"
            )
        
        if len(large_jumps) > 0:
            result['warnings'].append(
                f"å‘ç° {len(large_jumps)} ä¸ªå¼‚å¸¸è·³è·ƒï¼Œå¯èƒ½è¡¨ç¤ºé¢„æµ‹ä¸ç¨³å®š"
            )
        
        if physical_score < 0.95:
            result['warnings'].append(
                f"ç‰©ç†åˆç†æ€§åˆ†æ•°è¾ƒä½: {physical_score:.2%}ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹"
            )
        
        logger.info(f"âœ… ç‰©ç†çº¦æŸéªŒè¯å®Œæˆ: åˆ†æ•° {physical_score:.2%}")
        return result

class StatisticalAnomalyDetector:
    """ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, contamination: float = 0.1):
        self.contamination = contamination
        self.isolation_forest = IsolationForest(
            contamination=contamination,
            random_state=42
        )
        self.scaler = StandardScaler()
        self.is_fitted = False
    
    def fit(self, historical_data: pd.DataFrame):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        logger.info("ğŸ”§ è®­ç»ƒç»Ÿè®¡å¼‚å¸¸æ£€æµ‹æ¨¡å‹...")
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaled_data = self.scaler.fit_transform(historical_data)
        
        # è®­ç»ƒéš”ç¦»æ£®æ—
        self.isolation_forest.fit(scaled_data)
        self.is_fitted = True
        
        logger.info("âœ… ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def detect_anomalies(self, predictions: pd.DataFrame) -> Dict[str, Any]:
        """
        æ£€æµ‹é¢„æµ‹ç»“æœä¸­çš„ç»Ÿè®¡å¼‚å¸¸
        
        Args:
            predictions: é¢„æµ‹ç»“æœDataFrame
        
        Returns:
            å¼‚å¸¸æ£€æµ‹ç»“æœå­—å…¸
        """
        if not self.is_fitted:
            raise RuntimeError("å¼‚å¸¸æ£€æµ‹æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
        
        logger.info("ğŸ” å¼€å§‹ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹...")
        
        # æ ‡å‡†åŒ–é¢„æµ‹æ•°æ®
        scaled_predictions = self.scaler.transform(predictions)
        
        # æ£€æµ‹å¼‚å¸¸
        anomaly_labels = self.isolation_forest.predict(scaled_predictions)
        anomaly_scores = self.isolation_forest.decision_function(scaled_predictions)
        
        # ç»Ÿè®¡å¼‚å¸¸
        normal_count = np.sum(anomaly_labels == 1)
        anomaly_count = np.sum(anomaly_labels == -1)
        total_count = len(anomaly_labels)
        
        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
        anomaly_score = anomaly_count / total_count if total_count > 0 else 0.0
        
        # è¯†åˆ«å¼‚å¸¸ç‚¹
        anomaly_indices = np.where(anomaly_labels == -1)[0]
        anomaly_values = predictions.iloc[anomaly_indices] if len(anomaly_indices) > 0 else pd.DataFrame()
        
        result = {
            'is_valid': anomaly_score < self.contamination,
            'anomaly_score': anomaly_score,
            'total_count': total_count,
            'normal_count': normal_count,
            'anomaly_count': anomaly_count,
            'anomaly_indices': anomaly_indices.tolist(),
            'anomaly_values': anomaly_values.to_dict() if len(anomaly_values) > 0 else {},
            'anomaly_scores': anomaly_scores.tolist(),
            'warnings': [],
            'errors': []
        }
        
        # ç”Ÿæˆè­¦å‘Šå’Œé”™è¯¯ä¿¡æ¯
        if anomaly_score > self.contamination:
            result['warnings'].append(
                f"å¼‚å¸¸æ£€æµ‹åˆ†æ•°è¾ƒé«˜: {anomaly_score:.2%}ï¼Œè¶…è¿‡é˜ˆå€¼ {self.contamination:.2%}"
            )
        
        if anomaly_count > 0:
            result['warnings'].append(
                f"å‘ç° {anomaly_count} ä¸ªç»Ÿè®¡å¼‚å¸¸ç‚¹ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ"
            )
        
        logger.info(f"âœ… ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å®Œæˆ: å¼‚å¸¸åˆ†æ•° {anomaly_score:.2%}")
        return result

class MultiSourceConsistencyValidator:
    """å¤šæºä¸€è‡´æ€§éªŒè¯å™¨"""
    
    def __init__(self, tolerance: float = 0.1):
        self.tolerance = tolerance
    
    def validate_consistency(self, predictions: Dict[str, pd.DataFrame], 
                           variable_type: str) -> Dict[str, Any]:
        """
        éªŒè¯å¤šä¸ªæ•°æ®æºé¢„æµ‹ç»“æœçš„ä¸€è‡´æ€§
        
        Args:
            predictions: ä¸åŒæ•°æ®æºçš„é¢„æµ‹ç»“æœå­—å…¸
            variable_type: å˜é‡ç±»å‹
        
        Returns:
            ä¸€è‡´æ€§éªŒè¯ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” å¼€å§‹å¤šæºä¸€è‡´æ€§éªŒè¯: {variable_type}")
        
        if len(predictions) < 2:
            return {
                'is_valid': True,
                'consistency_score': 1.0,
                'message': 'åªæœ‰ä¸€ä¸ªæ•°æ®æºï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§éªŒè¯'
            }
        
        # è·å–æ‰€æœ‰æ•°æ®æºçš„æ—¶é—´ç´¢å¼•
        all_indices = set()
        for source, data in predictions.items():
            all_indices.update(data.index)
        
        # æ‰¾åˆ°å…±åŒçš„æ—¶é—´ç´¢å¼•
        common_indices = all_indices
        for source, data in predictions.items():
            common_indices = common_indices.intersection(data.index)
        
        if len(common_indices) == 0:
            return {
                'is_valid': False,
                'consistency_score': 0.0,
                'message': 'æ²¡æœ‰å…±åŒçš„æ—¶é—´ç´¢å¼•ï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§éªŒè¯'
            }
        
        # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        consistency_scores = []
        source_pairs = list(predictions.keys())
        
        for i in range(len(source_pairs)):
            for j in range(i + 1, len(source_pairs)):
                source1, source2 = source_pairs[i], source_pairs[j]
                
                # è·å–å…±åŒæ—¶é—´çš„æ•°æ®
                data1 = predictions[source1].loc[common_indices]
                data2 = predictions[source2].loc[common_indices]
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                correlation = data1.corr(data2)
                if pd.isna(correlation):
                    correlation = 0.0
                
                # è®¡ç®—ç›¸å¯¹è¯¯å·®
                relative_error = np.mean(np.abs(data1 - data2) / (np.abs(data1) + 1e-8))
                
                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
                pair_score = (correlation + (1 - relative_error)) / 2
                consistency_scores.append(pair_score)
        
        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§åˆ†æ•°
        overall_consistency = np.mean(consistency_scores) if consistency_scores else 0.0
        
        result = {
            'is_valid': overall_consistency > (1 - self.tolerance),
            'consistency_score': overall_consistency,
            'total_sources': len(predictions),
            'common_time_points': len(common_indices),
            'pairwise_scores': consistency_scores,
            'tolerance': self.tolerance,
            'warnings': [],
            'errors': []
        }
        
        # ç”Ÿæˆè­¦å‘Šå’Œé”™è¯¯ä¿¡æ¯
        if overall_consistency < (1 - self.tolerance):
            result['warnings'].append(
                f"å¤šæºä¸€è‡´æ€§åˆ†æ•°è¾ƒä½: {overall_consistency:.2%}ï¼Œä½äºé˜ˆå€¼ {(1 - self.tolerance):.2%}"
            )
        
        if len(common_indices) < 10:
            result['warnings'].append(
                f"å…±åŒæ—¶é—´ç‚¹è¾ƒå°‘: {len(common_indices)}ï¼Œå¯èƒ½å½±å“ä¸€è‡´æ€§éªŒè¯çš„å¯é æ€§"
            )
        
        logger.info(f"âœ… å¤šæºä¸€è‡´æ€§éªŒè¯å®Œæˆ: åˆ†æ•° {overall_consistency:.2%}")
        return result

class PredictionQualityValidator:
    """é¢„æµ‹è´¨é‡ç»¼åˆéªŒè¯å™¨"""
    
    def __init__(self):
        self.physical_validator = PhysicalConstraintValidator()
        self.anomaly_detector = StatisticalAnomalyDetector()
        self.consistency_validator = MultiSourceConsistencyValidator()
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs("validation_results", exist_ok=True)
        os.makedirs("validation_logs", exist_ok=True)
    
    def validate_prediction_quality(self, 
                                  predictions: Union[pd.DataFrame, Dict[str, pd.DataFrame]],
                                  variable_type: str,
                                  historical_data: Optional[pd.DataFrame] = None,
                                  source_name: str = "unknown") -> ValidationResult:
        """
        ç»¼åˆéªŒè¯é¢„æµ‹ç»“æœè´¨é‡
        
        Args:
            predictions: é¢„æµ‹ç»“æœï¼ˆå•ä¸ªDataFrameæˆ–å¤šä¸ªæ•°æ®æºçš„å­—å…¸ï¼‰
            variable_type: å˜é‡ç±»å‹
            historical_data: å†å²æ•°æ®ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ï¼‰
            source_name: æ•°æ®æºåç§°
        
        Returns:
            ç»¼åˆéªŒè¯ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹ç»¼åˆé¢„æµ‹è´¨é‡éªŒè¯: {variable_type} from {source_name}")
        
        start_time = datetime.now()
        validation_details = {}
        warnings = []
        errors = []
        recommendations = []
        
        try:
            # 1. ç‰©ç†çº¦æŸéªŒè¯
            if isinstance(predictions, pd.DataFrame):
                physical_result = self.physical_validator.validate_physical_constraints(
                    predictions, variable_type
                )
                validation_details['physical_constraints'] = physical_result
                
                if not physical_result['is_valid']:
                    errors.extend(physical_result['errors'])
                warnings.extend(physical_result['warnings'])
                
                # 2. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
                if historical_data is not None:
                    try:
                        # è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
                        self.anomaly_detector.fit(historical_data)
                        
                        # æ£€æµ‹å¼‚å¸¸
                        anomaly_result = self.anomaly_detector.detect_anomalies(predictions)
                        validation_details['statistical_anomalies'] = anomaly_result
                        
                        if not anomaly_result['is_valid']:
                            errors.extend(anomaly_result['errors'])
                        warnings.extend(anomaly_result['warnings'])
                        
                    except Exception as e:
                        logger.warning(f"ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
                        warnings.append(f"ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
                
                # 3. è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
                physical_score = physical_result.get('physical_score', 0.0)
                anomaly_score = validation_details.get('statistical_anomalies', {}).get('anomaly_score', 0.0)
                
                # ç»¼åˆåˆ†æ•°ï¼šç‰©ç†çº¦æŸæƒé‡70%ï¼Œå¼‚å¸¸æ£€æµ‹æƒé‡30%
                overall_score = physical_score * 0.7 + (1 - anomaly_score) * 0.3
                
            else:
                # å¤šæ•°æ®æºéªŒè¯
                consistency_result = self.consistency_validator.validate_consistency(
                    predictions, variable_type
                )
                validation_details['multi_source_consistency'] = consistency_result
                
                overall_score = consistency_result.get('consistency_score', 0.0)
                
                if not consistency_result['is_valid']:
                    warnings.extend(consistency_result['warnings'])
            
            # 4. ç”Ÿæˆå»ºè®®
            if overall_score < 0.7:
                recommendations.append("é¢„æµ‹è´¨é‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹è®­ç»ƒæ•°æ®å’Œå‚æ•°")
            elif overall_score < 0.9:
                recommendations.append("é¢„æµ‹è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
            else:
                recommendations.append("é¢„æµ‹è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨")
            
            # 5. åˆ¤æ–­æ•´ä½“æœ‰æ•ˆæ€§
            is_valid = overall_score > 0.7 and len(errors) == 0
            
            # 6. ä¿å­˜éªŒè¯ç»“æœ
            self._save_validation_result(
                source_name, variable_type, validation_details, 
                overall_score, warnings, errors, recommendations
            )
            
            validation_result = ValidationResult(
                is_valid=is_valid,
                confidence_score=overall_score,
                validation_details=validation_details,
                warnings=warnings,
                errors=errors,
                recommendations=recommendations,
                timestamp=start_time
            )
            
            logger.info(f"âœ… é¢„æµ‹è´¨é‡éªŒè¯å®Œæˆ: åˆ†æ•° {overall_score:.2%}, æœ‰æ•ˆ: {is_valid}")
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹è´¨é‡éªŒè¯å¤±è´¥: {e}")
            errors.append(f"éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            
            return ValidationResult(
                is_valid=False,
                confidence_score=0.0,
                validation_details={'error': str(e)},
                warnings=warnings,
                errors=errors,
                recommendations=["éªŒè¯è¿‡ç¨‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œç³»ç»ŸçŠ¶æ€"],
                timestamp=start_time
            )
    
    def _save_validation_result(self, source_name: str, variable_type: str,
                               validation_details: Dict, overall_score: float,
                               warnings: List[str], errors: List[str],
                               recommendations: List[str]):
        """ä¿å­˜éªŒè¯ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"validation_results/{source_name}_{variable_type}_{timestamp}.json"
        
        result_data = {
            'source_name': source_name,
            'variable_type': variable_type,
            'timestamp': timestamp,
            'overall_score': overall_score,
            'validation_details': validation_details,
            'warnings': warnings,
            'errors': errors,
            'recommendations': recommendations
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"âœ… éªŒè¯ç»“æœå·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")
    
    def generate_validation_report(self, validation_result: ValidationResult) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = f"""
# é¢„æµ‹è´¨é‡éªŒè¯æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- éªŒè¯æ—¶é—´: {validation_result.timestamp}
- æ•´ä½“æœ‰æ•ˆæ€§: {'âœ… æœ‰æ•ˆ' if validation_result.is_valid else 'âŒ æ— æ•ˆ'}
- ç½®ä¿¡åº¦åˆ†æ•°: {validation_result.confidence_score:.2%}

## éªŒè¯è¯¦æƒ…
"""
        
        for category, details in validation_result.validation_details.items():
            report += f"\n### {category.replace('_', ' ').title()}\n"
            if isinstance(details, dict):
                for key, value in details.items():
                    if key not in ['warnings', 'errors']:
                        report += f"- {key}: {value}\n"
            else:
                report += f"- {details}\n"
        
        if validation_result.warnings:
            report += "\n## âš ï¸ è­¦å‘Š\n"
            for warning in validation_result.warnings:
                report += f"- {warning}\n"
        
        if validation_result.errors:
            report += "\n## âŒ é”™è¯¯\n"
            for error in validation_result.errors:
                report += f"- {error}\n"
        
        if validation_result.recommendations:
            report += "\n## ğŸ’¡ å»ºè®®\n"
            for rec in validation_result.recommendations:
                report += f"- {rec}\n"
        
        return report

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºéªŒè¯å™¨ä½¿ç”¨"""
    logger.info("ğŸš€ å¯åŠ¨é¢„æµ‹ç»“æœéªŒè¯å™¨æ¼”ç¤º")
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = PredictionQualityValidator()
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=100, freq='D')
    
    # ç¤ºä¾‹1ï¼šæ­£å¸¸çš„åœŸå£¤æ¹¿åº¦é¢„æµ‹
    normal_predictions = pd.DataFrame({
        'soil_moisture': np.random.uniform(0.1, 0.8, 100)
    }, index=dates)
    
    # ç¤ºä¾‹2ï¼šå¼‚å¸¸çš„åœŸå£¤æ¹¿åº¦é¢„æµ‹ï¼ˆåŒ…å«è¶…å‡ºèŒƒå›´çš„å€¼ï¼‰
    abnormal_predictions = pd.DataFrame({
        'soil_moisture': np.random.uniform(-0.1, 1.2, 100)  # åŒ…å«è´Ÿå€¼å’Œè¶…è¿‡1çš„å€¼
    }, index=dates)
    
    # ç¤ºä¾‹3ï¼šå†å²æ•°æ®
    historical_data = pd.DataFrame({
        'soil_moisture': np.random.uniform(0.1, 0.8, 200)
    }, index=pd.date_range('2023-01-01', periods=200, freq='D'))
    
    # éªŒè¯æ­£å¸¸é¢„æµ‹
    logger.info("\n" + "="*50)
    logger.info("éªŒè¯æ­£å¸¸é¢„æµ‹ç»“æœ")
    normal_result = validator.validate_prediction_quality(
        normal_predictions, 'soil_moisture', historical_data, 'normal_model'
    )
    
    # éªŒè¯å¼‚å¸¸é¢„æµ‹
    logger.info("\n" + "="*50)
    logger.info("éªŒè¯å¼‚å¸¸é¢„æµ‹ç»“æœ")
    abnormal_result = validator.validate_prediction_quality(
        abnormal_predictions, 'soil_moisture', historical_data, 'abnormal_model'
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    logger.info("\n" + "="*50)
    logger.info("ç”ŸæˆéªŒè¯æŠ¥å‘Š")
    
    normal_report = validator.generate_validation_report(normal_result)
    abnormal_report = validator.generate_validation_report(abnormal_result)
    
    # ä¿å­˜æŠ¥å‘Š
    with open("validation_logs/normal_validation_report.md", "w", encoding="utf-8") as f:
        f.write(normal_report)
    
    with open("validation_logs/abnormal_validation_report.md", "w", encoding="utf-8") as f:
        f.write(abnormal_report)
    
    logger.info("âœ… éªŒè¯å™¨æ¼”ç¤ºå®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ° validation_logs/ ç›®å½•")

if __name__ == "__main__":
    main()
