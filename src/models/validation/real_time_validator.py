#!/usr/bin/env python3
"""
HydrAI-SWE å®æ—¶ç›‘æ§éªŒè¯å™¨
åœ¨çº¿ç›‘æ§é¢„æµ‹ç»“æœè´¨é‡ï¼Œå®æ—¶æ£€æµ‹å¼‚å¸¸å’Œæ€§èƒ½ä¸‹é™
"""

import pandas as pd
import numpy as np
import logging
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Callable
from dataclasses import dataclass
from collections import deque
import threading
import queue
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class RealTimeValidationResult:
    """å®æ—¶éªŒè¯ç»“æœæ•°æ®ç±»"""
    timestamp: datetime
    prediction_id: str
    is_valid: bool
    quality_score: float
    alerts: List[str]
    metrics: Dict[str, float]
    recommendations: List[str]

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, window_size: int = 100, alert_threshold: float = 0.8):
        self.window_size = window_size
        self.alert_threshold = alert_threshold
        self.performance_history = deque(maxlen=window_size)
        self.alert_history = deque(maxlen=100)
        
    def add_performance_metric(self, metric_name: str, value: float, timestamp: datetime):
        """æ·»åŠ æ€§èƒ½æŒ‡æ ‡"""
        self.performance_history.append({
            'timestamp': timestamp,
            'metric_name': metric_name,
            'value': value
        })
    
    def get_performance_trend(self, metric_name: str, window: int = None) -> Dict[str, Any]:
        """è·å–æ€§èƒ½è¶‹åŠ¿"""
        if window is None:
            window = self.window_size
        
        # è¿‡æ»¤æŒ‡å®šæŒ‡æ ‡
        metric_data = [
            item for item in self.performance_history 
            if item['metric_name'] == metric_name
        ][-window:]
        
        if len(metric_data) < 2:
            return {
                'trend': 'insufficient_data',
                'slope': 0.0,
                'mean': 0.0,
                'std': 0.0,
                'alert': False
            }
        
        values = [item['value'] for item in metric_data]
        timestamps = [item['timestamp'] for item in metric_data]
        
        # è®¡ç®—è¶‹åŠ¿
        x = np.array([(ts - timestamps[0]).total_seconds() for ts in timestamps])
        y = np.array(values)
        
        if len(x) > 1:
            slope = np.polyfit(x, y, 1)[0]
            mean = np.mean(values)
            std = np.std(values)
            
            # åˆ¤æ–­è¶‹åŠ¿
            if slope > 0.01:
                trend = 'improving'
            elif slope < -0.01:
                trend = 'declining'
            else:
                trend = 'stable'
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å‘Šè­¦
            alert = mean < self.alert_threshold or (trend == 'declining' and slope < -0.05)
            
            return {
                'trend': trend,
                'slope': slope,
                'mean': mean,
                'std': std,
                'alert': alert,
                'data_points': len(values)
            }
        
        return {
            'trend': 'insufficient_data',
            'slope': 0.0,
            'mean': 0.0,
            'std': 0.0,
            'alert': False
        }
    
    def add_alert(self, alert_type: str, message: str, severity: str = 'warning'):
        """æ·»åŠ å‘Šè­¦"""
        alert = {
            'timestamp': datetime.now(),
            'type': alert_type,
            'message': message,
            'severity': severity
        }
        self.alert_history.append(alert)
        logger.warning(f"ğŸš¨ å‘Šè­¦ [{severity.upper()}]: {message}")

class DriftDetector:
    """æ•°æ®æ¼‚ç§»æ£€æµ‹å™¨"""
    
    def __init__(self, reference_window: int = 1000, detection_threshold: float = 0.1):
        self.reference_window = reference_window
        self.detection_threshold = detection_threshold
        self.reference_distribution = None
        self.is_initialized = False
        
    def initialize_reference(self, reference_data: pd.DataFrame):
        """åˆå§‹åŒ–å‚è€ƒåˆ†å¸ƒ"""
        logger.info("ğŸ”§ åˆå§‹åŒ–æ•°æ®æ¼‚ç§»æ£€æµ‹å‚è€ƒåˆ†å¸ƒ...")
        
        # è®¡ç®—å‚è€ƒåˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹å¾
        self.reference_distribution = {
            'mean': reference_data.mean().to_dict(),
            'std': reference_data.std().to_dict(),
            'quantiles': reference_data.quantile([0.25, 0.5, 0.75]).to_dict()
        }
        self.is_initialized = True
        
        logger.info("âœ… å‚è€ƒåˆ†å¸ƒåˆå§‹åŒ–å®Œæˆ")
    
    def detect_drift(self, current_data: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æµ‹æ•°æ®æ¼‚ç§»"""
        if not self.is_initialized:
            raise RuntimeError("æ•°æ®æ¼‚ç§»æ£€æµ‹å™¨å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize_reference()")
        
        logger.info("ğŸ” å¼€å§‹æ•°æ®æ¼‚ç§»æ£€æµ‹...")
        
        drift_results = {}
        total_features = len(current_data.columns)
        drifted_features = 0
        
        for column in current_data.columns:
            if column in self.reference_distribution['mean']:
                ref_mean = self.reference_distribution['mean'][column]
                ref_std = self.reference_distribution['std'][column]
                
                current_mean = current_data[column].mean()
                current_std = current_data[column].std()
                
                # è®¡ç®—åˆ†å¸ƒå·®å¼‚
                mean_diff = abs(current_mean - ref_mean) / (abs(ref_mean) + 1e-8)
                std_diff = abs(current_std - ref_std) / (abs(ref_std) + 1e-8)
                
                # åˆ¤æ–­æ˜¯å¦æ¼‚ç§»
                is_drifted = mean_diff > self.detection_threshold or std_diff > self.detection_threshold
                
                drift_results[column] = {
                    'is_drifted': is_drifted,
                    'mean_difference': mean_diff,
                    'std_difference': std_diff,
                    'reference_mean': ref_mean,
                    'current_mean': current_mean,
                    'reference_std': ref_std,
                    'current_std': current_std
                }
                
                if is_drifted:
                    drifted_features += 1
        
        # è®¡ç®—æ•´ä½“æ¼‚ç§»åˆ†æ•°
        drift_score = drifted_features / total_features if total_features > 0 else 0.0
        
        result = {
            'overall_drift_score': drift_score,
            'total_features': total_features,
            'drifted_features': drifted_features,
            'feature_details': drift_results,
            'is_drifted': drift_score > 0.2,  # è¶…è¿‡20%çš„ç‰¹å¾æ¼‚ç§»è®¤ä¸ºæ•´ä½“æ¼‚ç§»
            'timestamp': datetime.now()
        }
        
        logger.info(f"âœ… æ•°æ®æ¼‚ç§»æ£€æµ‹å®Œæˆ: æ¼‚ç§»åˆ†æ•° {drift_score:.2%}")
        return result

class RealTimeValidator:
    """å®æ—¶éªŒè¯å™¨ä¸»ç±»"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.performance_monitor = PerformanceMonitor(
            window_size=self.config['performance_window_size'],
            alert_threshold=self.config['alert_threshold']
        )
        self.drift_detector = DriftDetector(
            reference_window=self.config['reference_window_size'],
            detection_threshold=self.config['drift_threshold']
        )
        
        # éªŒè¯é˜Ÿåˆ—å’Œç»“æœå­˜å‚¨
        self.validation_queue = queue.Queue()
        self.validation_results = deque(maxlen=1000)
        self.alert_history = deque(maxlen=100)
        
        # åˆ›å»ºç›®å½•
        os.makedirs("real_time_validation", exist_ok=True)
        os.makedirs("real_time_validation/results", exist_ok=True)
        os.makedirs("real_time_validation/alerts", exist_ok=True)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info("ğŸš€ å®æ—¶éªŒè¯å™¨å¯åŠ¨å®Œæˆ")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'performance_window_size': 100,
            'alert_threshold': 0.8,
            'reference_window_size': 1000,
            'drift_threshold': 0.1,
            'validation_interval': 1.0,  # ç§’
            'max_queue_size': 1000,
            'save_interval': 60  # ç§’
        }
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        last_save_time = time.time()
        
        while self.monitoring_active:
            try:
                # å¤„ç†éªŒè¯é˜Ÿåˆ—
                while not self.validation_queue.empty():
                    validation_task = self.validation_queue.get_nowait()
                    self._process_validation_task(validation_task)
                
                # å®šæœŸä¿å­˜ç»“æœ
                current_time = time.time()
                if current_time - last_save_time > self.config['save_interval']:
                    self._save_monitoring_data()
                    last_save_time = current_time
                
                time.sleep(self.config['validation_interval'])
                
            except Exception as e:
                logger.error(f"âŒ ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(5)  # é”™è¯¯åç­‰å¾…5ç§’å†ç»§ç»­
    
    def _process_validation_task(self, task: Dict[str, Any]):
        """å¤„ç†éªŒè¯ä»»åŠ¡"""
        try:
            prediction_id = task.get('prediction_id', f"pred_{int(time.time())}")
            predictions = task['predictions']
            variable_type = task['variable_type']
            source_name = task.get('source_name', 'unknown')
            
            # æ‰§è¡ŒéªŒè¯
            validation_result = self._validate_single_prediction(
                predictions, variable_type, source_name, prediction_id
            )
            
            # å­˜å‚¨ç»“æœ
            self.validation_results.append(validation_result)
            
            # æ›´æ–°æ€§èƒ½ç›‘æ§
            self.performance_monitor.add_performance_metric(
                'quality_score', validation_result.quality_score, validation_result.timestamp
            )
            
            # æ£€æŸ¥å‘Šè­¦
            if validation_result.quality_score < self.config['alert_threshold']:
                alert_message = f"é¢„æµ‹è´¨é‡ä¸‹é™: {validation_result.quality_score:.2%}"
                self.performance_monitor.add_alert('quality_decline', alert_message, 'warning')
            
            logger.info(f"âœ… éªŒè¯ä»»åŠ¡å®Œæˆ: {prediction_id}, è´¨é‡åˆ†æ•°: {validation_result.quality_score:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†éªŒè¯ä»»åŠ¡å¤±è´¥: {e}")
    
    def _validate_single_prediction(self, predictions: pd.DataFrame, 
                                  variable_type: str, source_name: str,
                                  prediction_id: str) -> RealTimeValidationResult:
        """éªŒè¯å•ä¸ªé¢„æµ‹ç»“æœ"""
        timestamp = datetime.now()
        alerts = []
        recommendations = []
        
        # åŸºç¡€è´¨é‡æ£€æŸ¥
        quality_score = self._calculate_basic_quality(predictions, variable_type)
        
        # æ•°æ®æ¼‚ç§»æ£€æµ‹
        if self.drift_detector.is_initialized:
            try:
                drift_result = self.drift_detector.detect_drift(predictions)
                if drift_result['is_drifted']:
                    alerts.append(f"æ£€æµ‹åˆ°æ•°æ®æ¼‚ç§»: {drift_result['overall_drift_score']:.2%}")
                    recommendations.append("å»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–æ›´æ–°å‚è€ƒåˆ†å¸ƒ")
            except Exception as e:
                logger.warning(f"æ•°æ®æ¼‚ç§»æ£€æµ‹å¤±è´¥: {e}")
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        trend_result = self.performance_monitor.get_performance_trend('quality_score', 20)
        if trend_result['alert']:
            alerts.append(f"æ€§èƒ½è¶‹åŠ¿ä¸‹é™: {trend_result['trend']}")
            recommendations.append("å»ºè®®æ£€æŸ¥æ¨¡å‹çŠ¶æ€å’Œæ•°æ®è´¨é‡")
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        final_quality_score = quality_score * 0.7 + (1 - trend_result['slope']) * 0.3
        final_quality_score = max(0.0, min(1.0, final_quality_score))
        
        # åˆ¤æ–­æœ‰æ•ˆæ€§
        is_valid = final_quality_score > self.config['alert_threshold'] and len(alerts) < 3
        
        # ç”Ÿæˆå»ºè®®
        if final_quality_score < 0.7:
            recommendations.append("é¢„æµ‹è´¨é‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹")
        elif final_quality_score < 0.9:
            recommendations.append("é¢„æµ‹è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®ä¼˜åŒ–")
        else:
            recommendations.append("é¢„æµ‹è´¨é‡è‰¯å¥½")
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'quality_score': final_quality_score,
            'prediction_count': len(predictions),
            'alert_count': len(alerts),
            'trend_slope': trend_result.get('slope', 0.0)
        }
        
        return RealTimeValidationResult(
            timestamp=timestamp,
            prediction_id=prediction_id,
            is_valid=is_valid,
            quality_score=final_quality_score,
            alerts=alerts,
            metrics=metrics,
            recommendations=recommendations
        )
    
    def _calculate_basic_quality(self, predictions: pd.DataFrame, variable_type: str) -> float:
        """è®¡ç®—åŸºç¡€è´¨é‡åˆ†æ•°"""
        if predictions.empty:
            return 0.0
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_ratio = predictions.isnull().sum().sum() / (len(predictions) * len(predictions.columns))
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        if variable_type == 'soil_moisture':
            valid_range = (0.0, 1.0)
        elif variable_type == 'snow_water_equivalent':
            valid_range = (0.0, 2000.0)
        elif variable_type == 'runoff':
            valid_range = (0.0, 10000.0)
        else:
            valid_range = (float('-inf'), float('inf'))
        
        in_range_ratio = np.mean(
            (predictions >= valid_range[0]) & (predictions <= valid_range[1])
        )
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = (1 - missing_ratio) * 0.5 + in_range_ratio * 0.5
        return quality_score
    
    def add_validation_task(self, predictions: pd.DataFrame, variable_type: str, 
                           source_name: str = "unknown", prediction_id: str = None):
        """æ·»åŠ éªŒè¯ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        if prediction_id is None:
            prediction_id = f"pred_{int(time.time())}"
        
        task = {
            'prediction_id': prediction_id,
            'predictions': predictions,
            'variable_type': variable_type,
            'source_name': source_name,
            'timestamp': datetime.now()
        }
        
        try:
            self.validation_queue.put_nowait(task)
            logger.info(f"âœ… éªŒè¯ä»»åŠ¡å·²æ·»åŠ åˆ°é˜Ÿåˆ—: {prediction_id}")
        except queue.Full:
            logger.warning(f"âš ï¸ éªŒè¯é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒä»»åŠ¡: {prediction_id}")
    
    def initialize_reference_distribution(self, reference_data: pd.DataFrame):
        """åˆå§‹åŒ–å‚è€ƒåˆ†å¸ƒï¼ˆç”¨äºæ¼‚ç§»æ£€æµ‹ï¼‰"""
        self.drift_detector.initialize_reference(reference_data)
        logger.info("âœ… å‚è€ƒåˆ†å¸ƒåˆå§‹åŒ–å®Œæˆ")
    
    def get_validation_status(self) -> Dict[str, Any]:
        """è·å–éªŒè¯çŠ¶æ€"""
        return {
            'queue_size': self.validation_queue.qsize(),
            'total_validations': len(self.validation_results),
            'active_monitoring': self.monitoring_active,
            'last_validation_time': self.validation_results[-1].timestamp if self.validation_results else None,
            'performance_trend': self.performance_monitor.get_performance_trend('quality_score', 50)
        }
    
    def get_recent_results(self, count: int = 10) -> List[RealTimeValidationResult]:
        """è·å–æœ€è¿‘çš„éªŒè¯ç»“æœ"""
        return list(self.validation_results)[-count:]
    
    def _save_monitoring_data(self):
        """ä¿å­˜ç›‘æ§æ•°æ®"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜éªŒè¯ç»“æœ
            results_file = f"real_time_validation/results/validation_results_{timestamp}.json"
            recent_results = self.get_recent_results(100)
            
            results_data = []
            for result in recent_results:
                results_data.append({
                    'timestamp': result.timestamp.isoformat(),
                    'prediction_id': result.prediction_id,
                    'is_valid': result.is_valid,
                    'quality_score': result.quality_score,
                    'alerts': result.alerts,
                    'metrics': result.metrics,
                    'recommendations': result.recommendations
                })
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜å‘Šè­¦å†å²
            alerts_file = f"real_time_validation/alerts/alerts_{timestamp}.json"
            alerts_data = []
            for alert in self.performance_monitor.alert_history:
                alerts_data.append({
                    'timestamp': alert['timestamp'].isoformat(),
                    'type': alert['type'],
                    'message': alert['message'],
                    'severity': alert['severity']
                })
            
            with open(alerts_file, 'w', encoding='utf-8') as f:
                json.dump(alerts_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… ç›‘æ§æ•°æ®å·²ä¿å­˜: {results_file}, {alerts_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç›‘æ§æ•°æ®å¤±è´¥: {e}")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        if self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
        logger.info("ğŸ›‘ å®æ—¶ç›‘æ§å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®æ—¶éªŒè¯å™¨ä½¿ç”¨"""
    logger.info("ğŸš€ å¯åŠ¨å®æ—¶éªŒè¯å™¨æ¼”ç¤º")
    
    # åˆ›å»ºå®æ—¶éªŒè¯å™¨
    validator = RealTimeValidator()
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=100, freq='H')
    
    # åˆå§‹åŒ–å‚è€ƒåˆ†å¸ƒ
    reference_data = pd.DataFrame({
        'soil_moisture': np.random.uniform(0.1, 0.8, 1000)
    })
    validator.initialize_reference_distribution(reference_data)
    
    # æ¨¡æ‹Ÿå®æ—¶é¢„æµ‹éªŒè¯
    for i in range(10):
        # ç”Ÿæˆé¢„æµ‹æ•°æ®
        predictions = pd.DataFrame({
            'soil_moisture': np.random.uniform(0.1, 0.8, 10)
        }, index=dates[i*10:(i+1)*10])
        
        # æ·»åŠ éªŒè¯ä»»åŠ¡
        validator.add_validation_task(
            predictions, 'soil_moisture', 'demo_model', f"demo_pred_{i}"
        )
        
        # ç­‰å¾…å¤„ç†
        time.sleep(2)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆ
    time.sleep(5)
    
    # è·å–çŠ¶æ€å’Œç»“æœ
    status = validator.get_validation_status()
    recent_results = validator.get_recent_results(5)
    
    logger.info(f"éªŒè¯çŠ¶æ€: {status}")
    logger.info(f"æœ€è¿‘ç»“æœæ•°é‡: {len(recent_results)}")
    
    # åœæ­¢ç›‘æ§
    validator.stop_monitoring()
    
    logger.info("âœ… å®æ—¶éªŒè¯å™¨æ¼”ç¤ºå®Œæˆ")

if __name__ == "__main__":
    main()
