#!/usr/bin/env python3
"""
é˜²è¿‡æ‹Ÿåˆæ ¸å¿ƒç³»ç»Ÿ
ç²¾å‡†è§£å†³RÂ²ä¸ºè´Ÿå€¼é—®é¢˜ï¼Œå›å½’æ ¸å¿ƒç›®æ ‡
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from typing import Dict, List, Optional
import logging
from datetime import datetime
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AntiOverfittingCore:
    """é˜²è¿‡æ‹Ÿåˆæ ¸å¿ƒç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ"""
        self.overfitting_detected = False
        self.optimization_history = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('models/anti_overfitting', exist_ok=True)
        
        logger.info("âœ… é˜²è¿‡æ‹Ÿåˆæ ¸å¿ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def detect_overfitting(self, train_losses: List[float], val_losses: List[float]) -> Dict:
        """æ£€æµ‹è¿‡æ‹Ÿåˆ"""
        try:
            if len(train_losses) < 5 or len(val_losses) < 5:
                return {'status': 'insufficient_data', 'overfitting': False}
            
            # è®¡ç®—æœ€è¿‘5ä¸ªepochçš„è¶‹åŠ¿
            recent_train = train_losses[-5:]
            recent_val = val_losses[-5:]
            
            # è®­ç»ƒæŸå¤±ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡ = è¿‡æ‹Ÿåˆ
            train_trend = np.polyfit(range(5), recent_train, 1)[0]  # æ–œç‡
            val_trend = np.polyfit(range(5), recent_val, 1)[0]
            
            overfitting = (train_trend < -0.001 and val_trend > 0.001)
            
            # è®¡ç®—è¿‡æ‹Ÿåˆä¸¥é‡ç¨‹åº¦
            if overfitting:
                severity = abs(val_trend) / (abs(train_trend) + 1e-8)
                self.overfitting_detected = True
            else:
                severity = 0.0
            
            result = {
                'status': 'success',
                'overfitting': overfitting,
                'severity': severity,
                'train_trend': train_trend,
                'val_trend': val_trend,
                'recommendation': self._get_recommendation(overfitting, severity)
            }
            
            logger.info(f"è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ: {'æ˜¯' if overfitting else 'å¦'}, ä¸¥é‡ç¨‹åº¦: {severity:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è¿‡æ‹Ÿåˆæ£€æµ‹å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _get_recommendation(self, overfitting: bool, severity: float) -> str:
        """è·å–ä¿®å¤å»ºè®®"""
        if not overfitting:
            return "æ¨¡å‹è®­ç»ƒæ­£å¸¸ï¼Œæ— éœ€ä¿®å¤"
        
        if severity > 2.0:
            return "ä¸¥é‡è¿‡æ‹Ÿåˆï¼šç«‹å³åœæ­¢è®­ç»ƒï¼Œå¤§å¹…ç®€åŒ–æ¨¡å‹"
        elif severity > 1.0:
            return "ä¸­åº¦è¿‡æ‹Ÿåˆï¼šå¢åŠ æ­£åˆ™åŒ–ï¼Œå‡å°‘æ¨¡å‹å¤æ‚åº¦"
        else:
            return "è½»åº¦è¿‡æ‹Ÿåˆï¼šå¾®è°ƒå­¦ä¹ ç‡ï¼Œå¢åŠ æ—©åœ"
    
    def fix_overfitting(self, model: nn.Module, X_train: np.ndarray, y_train: np.ndarray,
                       X_val: np.ndarray, y_val: np.ndarray) -> Dict:
        """ä¿®å¤è¿‡æ‹Ÿåˆ"""
        try:
            if not self.overfitting_detected:
                return {'status': 'no_overfitting', 'message': 'æœªæ£€æµ‹åˆ°è¿‡æ‹Ÿåˆ'}
            
            logger.info("ğŸ”§ å¼€å§‹ä¿®å¤è¿‡æ‹Ÿåˆ...")
            
            # æ­¥éª¤1: ç®€åŒ–æ¨¡å‹æ¶æ„
            simplified_model = self._simplify_model(model, X_train.shape[1])
            
            # æ­¥éª¤2: å¢åŠ æ­£åˆ™åŒ–
            regularized_model = self._add_regularization(simplified_model)
            
            # æ­¥éª¤3: ä¼˜åŒ–è®­ç»ƒå‚æ•°
            optimized_params = self._optimize_training_params(X_train, y_train, X_val, y_val)
            
            # ä¿å­˜ä¿®å¤åçš„æ¨¡å‹
            self._save_fixed_model(regularized_model)
            
            result = {
                'status': 'success',
                'original_model': str(model),
                'simplified_model': str(regularized_model),
                'optimized_params': optimized_params,
                'fix_timestamp': datetime.now().isoformat()
            }
            
            self.optimization_history.append(result)
            logger.info("âœ… è¿‡æ‹Ÿåˆä¿®å¤å®Œæˆ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ä¿®å¤è¿‡æ‹Ÿåˆå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _simplify_model(self, model: nn.Module, input_size: int) -> nn.Module:
        """ç®€åŒ–æ¨¡å‹æ¶æ„"""
        try:
            # åˆ›å»ºæç®€LSTMæ¨¡å‹
            class SimpleLSTM(nn.Module):
                def __init__(self, input_size: int):
                    super(SimpleLSTM, self).__init__()
                    
                    # å¤§å¹…å‡å°‘å‚æ•°ï¼šéšè—å±‚å¤§å°å‡åŠï¼Œå±‚æ•°å‡ä¸º1
                    hidden_size = max(8, input_size // 8)  # æœ€å°8ï¼Œæœ€å¤§input_size/8
                    
                    self.lstm = nn.LSTM(
                        input_size=input_size,
                        hidden_size=hidden_size,
                        num_layers=1,  # åªä½¿ç”¨1å±‚
                        batch_first=True,
                        dropout=0.0  # ç§»é™¤dropoutï¼Œé¿å…è¿‡åº¦æ­£åˆ™åŒ–
                    )
                    
                    self.fc = nn.Linear(hidden_size, 1)
                    
                def forward(self, x):
                    lstm_out, _ = self.lstm(x)
                    last_output = lstm_out[:, -1, :]
                    return self.fc(last_output)
            
            simplified = SimpleLSTM(input_size)
            
            # è®¡ç®—å‚æ•°å‡å°‘é‡
            original_params = sum(p.numel() for p in model.parameters())
            simplified_params = sum(p.numel() for p in simplified.parameters())
            reduction = (original_params - simplified_params) / original_params
            
            logger.info(f"âœ… æ¨¡å‹ç®€åŒ–å®Œæˆ: å‚æ•°å‡å°‘ {reduction:.1%} ({original_params} -> {simplified_params})")
            
            return simplified
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ç®€åŒ–å¤±è´¥: {e}")
            raise
    
    def _add_regularization(self, model: nn.Module) -> nn.Module:
        """æ·»åŠ é€‚åº¦çš„æ­£åˆ™åŒ–"""
        try:
            # ä¸ºLSTMå±‚æ·»åŠ æƒé‡è¡°å‡
            for name, param in model.named_parameters():
                if 'weight' in name:
                    param.requires_grad = True
                    # é€‚åº¦çš„L2æ­£åˆ™åŒ–
                    if hasattr(param, 'weight_decay'):
                        param.weight_decay = 0.001  # é™ä½æ­£åˆ™åŒ–å¼ºåº¦
            
            logger.info("âœ… æ­£åˆ™åŒ–æ·»åŠ å®Œæˆ")
            return model
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ­£åˆ™åŒ–å¤±è´¥: {e}")
            return model
    
    def _optimize_training_params(self, X_train: np.ndarray, y_train: np.ndarray,
                                 X_val: np.ndarray, y_val: np.ndarray) -> Dict:
        """ä¼˜åŒ–è®­ç»ƒå‚æ•°"""
        try:
            # åŸºäºæ•°æ®å¤§å°ä¼˜åŒ–å‚æ•°
            data_size = len(X_train)
            
            if data_size < 100:
                batch_size = 8
                epochs = 20
                learning_rate = 0.01
                patience = 5
            elif data_size < 500:
                batch_size = 16
                epochs = 30
                learning_rate = 0.005
                patience = 8
            else:
                batch_size = 32
                epochs = 50
                learning_rate = 0.001
                patience = 10
            
            # è°ƒæ•´æ—©åœç­–ç•¥
            if self.overfitting_detected:
                patience = max(5, patience // 2)  # æ›´æ—©åœæ­¢
            
            optimized_params = {
                'batch_size': batch_size,
                'epochs': epochs,
                'learning_rate': learning_rate,
                'patience': patience,
                'early_stopping': True,
                'reduce_lr_on_plateau': True
            }
            
            logger.info(f"âœ… è®­ç»ƒå‚æ•°ä¼˜åŒ–å®Œæˆ: {optimized_params}")
            return optimized_params
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–è®­ç»ƒå‚æ•°å¤±è´¥: {e}")
            return {}
    
    def _save_fixed_model(self, model: nn.Module):
        """ä¿å­˜ä¿®å¤åçš„æ¨¡å‹"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_path = f"models/anti_overfitting/fixed_model_{timestamp}.pth"
            
            torch.save({
                'model_state_dict': model.state_dict(),
                'fix_timestamp': timestamp,
                'overfitting_fixed': True
            }, model_path)
            
            logger.info(f"âœ… ä¿®å¤åçš„æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¿®å¤åçš„æ¨¡å‹å¤±è´¥: {e}")
    
    def get_optimization_summary(self) -> Dict:
        """è·å–ä¼˜åŒ–æ‘˜è¦"""
        if not self.optimization_history:
            return {"message": "æš‚æ— ä¼˜åŒ–è®°å½•"}
        
        total_fixes = len(self.optimization_history)
        successful_fixes = sum(1 for r in self.optimization_history if r['status'] == 'success')
        
        return {
            "total_fixes": total_fixes,
            "successful_fixes": successful_fixes,
            "success_rate": f"{successful_fixes/total_fixes*100:.1f}%",
            "last_fix": self.optimization_history[-1].get('fix_timestamp', 'Unknown'),
            "overfitting_detected": self.overfitting_detected
        }
