#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„SWEé¢„æµ‹æœåŠ¡
ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒå¥½çš„æœ€ä½³è¶…å‚æ•°æ¨¡å‹ã€é›†æˆæ¨¡å‹å’Œæ•°æ®å¢å¼ºæŠ€æœ¯
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import os
import pickle
from typing import Dict, List, Any, Optional

class OptimizedGRUModel(nn.Module):
    """ä¼˜åŒ–çš„GRUæ¨¡å‹"""
    
    def __init__(self, input_size=6, hidden_size=64, num_layers=2, dropout=0.1):
        super(OptimizedGRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                          dropout=dropout if num_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out)
        output = self.fc(gru_out[:, -1, :])
        return output

class OptimizedSWEPredictor:
    """ä¼˜åŒ–çš„SWEé¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str = None):
        self.model = None
        self.scaler_X = None
        self.scaler_y = None
        self.sequence_length = 30
        self.is_loaded = False
        
        # æ–°å¢ï¼šç‰¹å¾åˆ—é…ç½®å’ŒéªŒè¯
        self.feature_config = {
            'snow_depth_mm': 0,
            'snow_fall_mm': 1, 
            'snow_water_equivalent_mm': 2,
            'day_of_year': 3,
            'month': 4,
            'year': 5
        }
        
        # æ–°å¢ï¼šæ•°æ®éªŒè¯æ ‡å¿—
        self._data_validation_enabled = True
        
        # è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹
        if model_path is None:
            self._auto_load_best_model()
        else:
            self.load_model(model_path)
    
    def validate_feature_data(self, snow_depth_mm: float, snow_fall_mm: float, 
                            snow_water_equivalent_mm: float, date: datetime) -> bool:
        """éªŒè¯è¾“å…¥ç‰¹å¾æ•°æ® - åŠ å¼ºï¼šæ‹’ç»ä¸åˆç†æ•°æ®"""
        if not self._data_validation_enabled:
            return True
        
        try:
            # éªŒè¯æ•°æ®ç±»å‹
            if not all(isinstance(x, (int, float)) for x in [snow_depth_mm, snow_fall_mm, snow_water_equivalent_mm]):
                raise ValueError("é›ªç›¸å…³ç‰¹å¾å¿…é¡»æ˜¯æ•°å€¼ç±»å‹")
            
            # åŠ å¼ºéªŒè¯ï¼šæ‹’ç»æ˜æ˜¾ä¸åˆç†çš„é›ªæ•°æ®
            # é›ªæ·±åº¦å’Œé›ªæ°´å½“é‡ä¸èƒ½ä¸ºè´Ÿå€¼
            if snow_depth_mm < 0:
                raise ValueError(f"é›ªæ·±åº¦ä¸èƒ½ä¸ºè´Ÿå€¼: {snow_depth_mm}")
            if snow_water_equivalent_mm < 0:
                raise ValueError(f"é›ªæ°´å½“é‡ä¸èƒ½ä¸ºè´Ÿå€¼: {snow_water_equivalent_mm}")
            
            # é›ªé™é‡å¯ä»¥ä¸ºè´Ÿå€¼ï¼ˆè¡¨ç¤ºèåŒ–ï¼‰ï¼Œä½†éœ€è¦æ£€æŸ¥åˆç†æ€§
            if snow_fall_mm < -100:  # èåŒ–é‡è¿‡å¤§
                raise ValueError(f"é›ªé™é‡ï¼ˆèåŒ–é‡ï¼‰è¿‡å¤§: {snow_fall_mm}")
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´åˆç†æ€§
            # é›ªæ·±åº¦é€šå¸¸ä¸ä¼šè¶…è¿‡10ç±³ï¼ˆ10000mmï¼‰
            if snow_depth_mm > 10000:
                raise ValueError(f"é›ªæ·±åº¦è¿‡å¤§ï¼Œå¯èƒ½ä¸åˆç†: {snow_depth_mm} mm")
            
            # é›ªæ°´å½“é‡é€šå¸¸ä¸ä¼šè¶…è¿‡é›ªæ·±åº¦çš„1/3
            if snow_water_equivalent_mm > snow_depth_mm * 0.4:
                print(f"âš ï¸ è­¦å‘Šï¼šé›ªæ°´å½“é‡ç›¸å¯¹äºé›ªæ·±åº¦å¯èƒ½è¿‡å¤§")
                print(f"   é›ªæ·±åº¦: {snow_depth_mm} mm, é›ªæ°´å½“é‡: {snow_water_equivalent_mm} mm")
                print(f"   æ¯”ä¾‹: {snow_water_equivalent_mm/snow_depth_mm:.2f}")
            
            # éªŒè¯æ—¥æœŸ
            if not isinstance(date, datetime):
                raise ValueError("æ—¥æœŸå¿…é¡»æ˜¯datetimeå¯¹è±¡")
            
            # éªŒè¯æ—¥æœŸåˆç†æ€§
            current_year = datetime.now().year
            if date.year < 1900 or date.year > current_year + 10:
                raise ValueError(f"æ—¥æœŸå¹´ä»½ä¸åˆç†: {date.year}")
            
            # éªŒè¯æ—¥æœŸæ˜¯å¦åœ¨æœªæ¥ï¼ˆå¦‚æœæ˜¯é¢„æµ‹ï¼‰
            if date > datetime.now() + timedelta(days=365):
                print(f"âš ï¸ è­¦å‘Šï¼šé¢„æµ‹æ—¥æœŸè¾ƒè¿œ: {date}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def validate_historical_data_quality(self) -> Dict[str, Any]:
        """éªŒè¯å†å²æ•°æ®è´¨é‡ - æ–°å¢æ–¹æ³•"""
        if not hasattr(self, '_historical_features') or len(self._historical_features) == 0:
            return {
                'status': 'no_data',
                'message': 'æ²¡æœ‰å†å²æ•°æ®',
                'quality_score': 0.0
            }
        
        try:
            features_array = np.array(self._historical_features)
            n_samples = len(features_array)
            
            # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
            continuity_score = 1.0
            if n_samples > 1:
                # æ£€æŸ¥ç›¸é‚»æ ·æœ¬ä¹‹é—´çš„å˜åŒ–æ˜¯å¦åˆç†
                diffs = np.diff(features_array, axis=0)
                
                # é›ªç›¸å…³ç‰¹å¾çš„å˜åŒ–åº”è¯¥ç›¸å¯¹å¹³æ»‘
                snow_diffs = diffs[:, :3]  # å‰3åˆ—æ˜¯é›ªç›¸å…³ç‰¹å¾
                max_snow_change = np.max(np.abs(snow_diffs))
                
                if max_snow_change > 1000:  # å˜åŒ–è¿‡å¤§
                    continuity_score = 0.5
                    print(f"âš ï¸ è­¦å‘Šï¼šé›ªç›¸å…³ç‰¹å¾å˜åŒ–è¿‡å¤§: {max_snow_change}")
                
                # æ—¥æœŸç‰¹å¾åº”è¯¥é€’å¢
                date_diffs = diffs[:, 3:]  # å3åˆ—æ˜¯æ—¥æœŸç‰¹å¾
                if not all(date_diffs[:, 1] >= 0):  # æœˆä»½åº”è¯¥é€’å¢
                    continuity_score = 0.3
                    print("âš ï¸ è­¦å‘Šï¼šæ—¥æœŸç‰¹å¾ä¸è¿ç»­")
            
            # æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§
            range_score = 1.0
            for i, feature_name in enumerate(['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm']):
                values = features_array[:, i]
                
                # æ£€æŸ¥è´Ÿå€¼
                if np.any(values < 0):
                    if i == 1:  # é›ªé™é‡å¯ä»¥ä¸ºè´Ÿ
                        if np.any(values < -100):
                            range_score *= 0.8
                            print(f"âš ï¸ è­¦å‘Šï¼š{feature_name} åŒ…å«è¿‡å¤§çš„è´Ÿå€¼")
                    else:  # é›ªæ·±åº¦å’Œé›ªæ°´å½“é‡ä¸èƒ½ä¸ºè´Ÿ
                        range_score *= 0.5
                        print(f"âŒ é”™è¯¯ï¼š{feature_name} åŒ…å«è´Ÿå€¼")
                
                # æ£€æŸ¥å¼‚å¸¸å¤§çš„å€¼
                if np.any(values > 10000):
                    range_score *= 0.7
                    print(f"âš ï¸ è­¦å‘Šï¼š{feature_name} åŒ…å«å¼‚å¸¸å¤§çš„å€¼")
            
            # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
            quality_score = (continuity_score + range_score) / 2
            
            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            quality_report = {
                'status': 'valid' if quality_score > 0.7 else 'warning' if quality_score > 0.4 else 'error',
                'quality_score': quality_score,
                'n_samples': n_samples,
                'continuity_score': continuity_score,
                'range_score': range_score,
                'recommendations': []
            }
            
            # æ·»åŠ å»ºè®®
            if quality_score < 0.7:
                quality_report['recommendations'].append("å»ºè®®æ£€æŸ¥æ•°æ®æºå’Œé¢„å¤„ç†æ­¥éª¤")
            if continuity_score < 0.7:
                quality_report['recommendations'].append("å»ºè®®æ£€æŸ¥æ•°æ®æ—¶é—´é¡ºåºå’Œç¼ºå¤±å€¼")
            if range_score < 0.7:
                quality_report['recommendations'].append("å»ºè®®æ£€æŸ¥æ•°æ®èŒƒå›´å’Œå¼‚å¸¸å€¼")
            
            return quality_report
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}',
                'quality_score': 0.0
            }
    
    def get_data_requirements(self) -> Dict[str, Any]:
        """è·å–æ•°æ®è¦æ±‚è¯´æ˜ - æ–°å¢æ–¹æ³•"""
        return {
            'minimum_historical_data': self.sequence_length,
            'feature_requirements': {
                'snow_depth_mm': {
                    'type': 'float',
                    'range': '[0, 10000]',
                    'unit': 'mm',
                    'description': 'é›ªæ·±åº¦ï¼Œä¸èƒ½ä¸ºè´Ÿå€¼'
                },
                'snow_fall_mm': {
                    'type': 'float',
                    'range': '[-100, 10000]',
                    'unit': 'mm',
                    'description': 'é›ªé™é‡ï¼Œè´Ÿå€¼è¡¨ç¤ºèåŒ–'
                },
                'snow_water_equivalent_mm': {
                    'type': 'float',
                    'range': '[0, 10000]',
                    'unit': 'mm',
                    'description': 'é›ªæ°´å½“é‡ï¼Œä¸èƒ½ä¸ºè´Ÿå€¼'
                },
                'day_of_year': {
                    'type': 'int',
                    'range': '[1, 366]',
                    'description': 'ä¸€å¹´ä¸­çš„ç¬¬å‡ å¤©'
                },
                'month': {
                    'type': 'int',
                    'range': '[1, 12]',
                    'description': 'æœˆä»½'
                },
                'year': {
                    'type': 'int',
                    'range': '[1900, 2030]',
                    'description': 'å¹´ä»½'
                }
            },
            'data_quality_requirements': {
                'continuity': 'æ•°æ®åº”è¯¥è¿ç»­ï¼Œç›¸é‚»æ ·æœ¬å˜åŒ–åˆç†',
                'range': 'æ•°å€¼åœ¨åˆç†èŒƒå›´å†…',
                'completeness': 'æ²¡æœ‰ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼'
            }
        }
    
    def _auto_load_best_model(self):
        """è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹"""
        # æŸ¥æ‰¾æœ€æ–°çš„ä¼˜åŒ–æ¨¡å‹
        models_dir = "models"
        if not os.path.exists(models_dir):
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return
        
        # æŸ¥æ‰¾ä¼˜åŒ–æ¨¡å‹
        optimized_files = [f for f in os.listdir(models_dir) if f.startswith("optimized_gru_model_")]
        if optimized_files:
            latest_model = max(optimized_files)
            model_path = os.path.join(models_dir, latest_model)
            print(f"ğŸ” æ‰¾åˆ°ä¼˜åŒ–æ¨¡å‹: {latest_model}")
            self.load_model(model_path)
            return
        
        # æŸ¥æ‰¾é›†æˆæ¨¡å‹
        ensemble_dirs = [d for d in os.listdir(models_dir) if d.startswith("ensemble_models_")]
        if ensemble_dirs:
            latest_ensemble = max(ensemble_dirs)
            ensemble_path = os.path.join(models_dir, latest_ensemble)
            print(f"ğŸ” æ‰¾åˆ°é›†æˆæ¨¡å‹: {latest_ensemble}")
            self.load_ensemble_model(ensemble_path)
            return
        
        print("âŒ æœªæ‰¾åˆ°ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        self._create_default_model()
    
    def _create_default_model(self):
        """åˆ›å»ºé»˜è®¤æ¨¡å‹"""
        self.model = OptimizedGRUModel(
            input_size=6,
            hidden_size=64,
            num_layers=2,
            dropout=0.1
        )
        self.is_loaded = True
    
    def load_model(self, model_path: str):
        """åŠ è½½å•ä¸ªä¼˜åŒ–æ¨¡å‹ - å¢å¼ºï¼šæ›´å¥½çš„é”™è¯¯å¤„ç†"""
        try:
            print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(model_path)
            if file_size < 1000:  # å°äº1KBå¯èƒ½æ˜¯æŸåæ–‡ä»¶
                raise ValueError(f"æ¨¡å‹æ–‡ä»¶å¯èƒ½æŸåï¼Œå¤§å°å¼‚å¸¸: {file_size} bytes")
            
            # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
            try:
                checkpoint = torch.load(model_path, map_location='cpu')
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            
            # éªŒè¯æ£€æŸ¥ç‚¹ç»“æ„
            required_keys = ['model_state_dict', 'scaler_X_mean', 'scaler_X_scale', 
                           'scaler_y_mean', 'scaler_y_scale']
            missing_keys = [key for key in required_keys if key not in checkpoint]
            if missing_keys:
                raise ValueError(f"æ¨¡å‹æ£€æŸ¥ç‚¹ç¼ºå°‘å¿…è¦é”®: {missing_keys}")
            
            # é‡å»ºæ ‡å‡†åŒ–å™¨
            try:
                self.scaler_X = StandardScaler()
                self.scaler_X.mean_ = checkpoint['scaler_X_mean']
                self.scaler_X.scale_ = checkpoint['scaler_X_scale']
                
                self.scaler_y = StandardScaler()
                self.scaler_y.mean_ = checkpoint['scaler_y_mean']
                self.scaler_y.scale_ = checkpoint['scaler_y_scale']
            except Exception as e:
                raise RuntimeError(f"æ ‡å‡†åŒ–å™¨é‡å»ºå¤±è´¥: {e}")
            
            # åˆ›å»ºæ¨¡å‹
            try:
                self.model = OptimizedGRUModel(
                    input_size=6,
                    hidden_size=64,
                    num_layers=2,
                    dropout=0.1
                )
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            try:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.model.eval()
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
            
            self.is_loaded = True
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°é»˜è®¤æ¨¡å‹")
            self._create_default_model()
    
    def load_ensemble_model(self, ensemble_dir: str):
        """åŠ è½½é›†æˆæ¨¡å‹ - å¢å¼ºï¼šæ›´å¥½çš„é”™è¯¯å¤„ç†"""
        try:
            print(f"ğŸ“¥ åŠ è½½é›†æˆæ¨¡å‹: {ensemble_dir}")
            
            # éªŒè¯ç›®å½•å­˜åœ¨
            if not os.path.exists(ensemble_dir):
                raise FileNotFoundError(f"é›†æˆæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {ensemble_dir}")
            
            # éªŒè¯ç›®å½•ç»“æ„
            if not os.path.isdir(ensemble_dir):
                raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {ensemble_dir}")
            
            # åŠ è½½é›†æˆé…ç½®
            config_path = os.path.join(ensemble_dir, "ensemble_config.json")
            if os.path.exists(config_path):
                try:
                    import json
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                    print(f"âœ… é›†æˆé…ç½®åŠ è½½æˆåŠŸ: {config['n_models']} ä¸ªæ¨¡å‹")
                except Exception as e:
                    print(f"âš ï¸ é›†æˆé…ç½®åŠ è½½å¤±è´¥: {e}")
                    config = {'n_models': 3}  # é»˜è®¤é…ç½®
            else:
                print("âš ï¸ æœªæ‰¾åˆ°é›†æˆé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                config = {'n_models': 3}
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨å‚æ•°
            standardization_path = "models/standardization_params.pkl"
            if os.path.exists(standardization_path):
                try:
                    with open(standardization_path, 'rb') as f:
                        params = pickle.load(f)
                    
                    self.scaler_X = StandardScaler()
                    self.scaler_X.mean_ = params['scaler_X_mean']
                    self.scaler_X.scale_ = params['scaler_X_scale']
                    
                    self.scaler_y = StandardScaler()
                    self.scaler_y.mean_ = params['scaler_y_mean']
                    self.scaler_y.scale_ = params['scaler_y_scale']
                    
                    print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
                    # ç»§ç»­å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¯èƒ½æ¨¡å‹æ–‡ä»¶ä¸­æœ‰æ ‡å‡†åŒ–å™¨ä¿¡æ¯
            
            # åˆ›å»ºé›†æˆæ¨¡å‹åˆ—è¡¨
            self.ensemble_models = []
            successful_loads = 0
            
            for i in range(1, config['n_models'] + 1):
                try:
                    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                    model_files = [f for f in os.listdir(ensemble_dir) if f.startswith(f"model_{i}_")]
                    if not model_files:
                        print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ {i} çš„æ–‡ä»¶")
                        continue
                    
                    model_path = os.path.join(ensemble_dir, model_files[0])
                    
                    # éªŒè¯æ¨¡å‹æ–‡ä»¶
                    if not os.path.exists(model_path):
                        print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                        continue
                    
                    # åˆ›å»ºæ¨¡å‹
                    model = OptimizedGRUModel()
                    
                    # åŠ è½½æ£€æŸ¥ç‚¹
                    try:
                        checkpoint = torch.load(model_path, map_location='cpu')
                        model.load_state_dict(checkpoint['model_state_dict'])
                        model.eval()
                        
                        self.ensemble_models.append(model)
                        successful_loads += 1
                        print(f"âœ… é›†æˆæ¨¡å‹ {i} åŠ è½½æˆåŠŸ")
                        
                    except Exception as e:
                        print(f"âš ï¸ æ¨¡å‹ {i} åŠ è½½å¤±è´¥: {e}")
                        continue
                        
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ¨¡å‹ {i} æ—¶å‡ºé”™: {e}")
                    continue
            
            if successful_loads > 0:
                self.is_loaded = True
                print(f"âœ… é›†æˆæ¨¡å‹åŠ è½½å®Œæˆ: {successful_loads}/{config['n_models']} ä¸ªæ¨¡å‹")
            else:
                raise Exception("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•é›†æˆæ¨¡å‹")
                
        except Exception as e:
            print(f"âŒ é›†æˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°é»˜è®¤æ¨¡å‹")
            self._create_default_model()
    
    def prepare_input_features(self, snow_depth_mm: float, snow_fall_mm: float, 
                             snow_water_equivalent_mm: float, date: datetime) -> np.ndarray:
        """å‡†å¤‡è¾“å…¥ç‰¹å¾ - å¢å¼ºï¼šæ·»åŠ æ•°æ®éªŒè¯"""
        # æ•°æ®éªŒè¯
        if not self.validate_feature_data(snow_depth_mm, snow_fall_mm, snow_water_equivalent_mm, date):
            raise ValueError("è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥")
        
        # è®¡ç®—æ—¥æœŸç‰¹å¾
        day_of_year = date.timetuple().tm_yday
        month = date.month
        year = date.year
        
        # åˆ›å»ºç‰¹å¾å‘é‡
        features = np.array([
            snow_depth_mm,
            snow_fall_mm, 
            snow_water_equivalent_mm,
            day_of_year,
            month,
            year
        ]).reshape(1, -1)
        
        # éªŒè¯ç‰¹å¾ç»´åº¦
        if features.shape[1] != len(self.feature_config):
            raise ValueError(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{len(self.feature_config)}, å®é™…{features.shape[1]}")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if self.scaler_X is not None:
            try:
                features = self.scaler_X.transform(features)
            except Exception as e:
                print(f"âŒ ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
                raise
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ ‡å‡†åŒ–å™¨æœªåŠ è½½ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
        
        return features
    
    def create_sequence(self, features_list: List[np.ndarray]) -> np.ndarray:
        """åˆ›å»ºåºåˆ—æ•°æ®"""
        if len(features_list) < self.sequence_length:
            # å¦‚æœæ•°æ®ä¸è¶³ï¼Œç”¨é›¶å¡«å……
            padding = [np.zeros_like(features_list[0]) for _ in range(self.sequence_length - len(features_list))]
            features_list = padding + features_list
        
        # å–æœ€åsequence_lengthä¸ªç‰¹å¾
        sequence = features_list[-self.sequence_length:]
        return np.array(sequence).reshape(1, self.sequence_length, -1)
    
    def predict_single(self, snow_depth_mm: float, snow_fall_mm: float, 
                      snow_water_equivalent_mm: float, date: datetime) -> float:
        """å•æ¬¡é¢„æµ‹ - å½»åº•ä¿®å¤ï¼šæ‹’ç»è™šå‡åºåˆ—"""
        if not self.is_loaded:
            raise Exception("æ¨¡å‹æœªåŠ è½½")
        
        # å‡†å¤‡ç‰¹å¾
        features = self.prepare_input_features(snow_depth_mm, snow_fall_mm, snow_water_equivalent_mm, date)
        
        # å½»åº•ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰çœŸå®çš„å†å²æ•°æ®
        if not hasattr(self, '_historical_features') or len(self._historical_features) < self.sequence_length:
            # æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œæ‹’ç»é¢„æµ‹
            raise ValueError(
                f"æ— æ³•è¿›è¡Œé¢„æµ‹ï¼šéœ€è¦è‡³å°‘ {self.sequence_length} ä¸ªå†å²æ•°æ®ç‚¹ï¼Œ"
                f"ä½†åªæœ‰ {len(self._historical_features) if hasattr(self, '_historical_features') else 0} ä¸ªã€‚"
                "è¯·å…ˆä½¿ç”¨ update_historical_features() æ–¹æ³•æ·»åŠ è¶³å¤Ÿçš„å†å²æ•°æ®ã€‚"
            )
        
        # ä½¿ç”¨çœŸå®çš„å†å²æ•°æ®åˆ›å»ºåºåˆ—
        sequence = self._historical_features[-self.sequence_length:]
        sequence = np.array(sequence).reshape(1, self.sequence_length, -1)
        
        # é¢„æµ‹
        with torch.no_grad():
            if hasattr(self, 'ensemble_models') and self.ensemble_models:
                # é›†æˆé¢„æµ‹
                predictions = []
                for model in self.ensemble_models:
                    pred = model(torch.FloatTensor(sequence))
                    predictions.append(pred.item())
                
                # å¹³å‡é›†æˆ
                prediction = np.mean(predictions)
            else:
                # å•ä¸ªæ¨¡å‹é¢„æµ‹
                prediction = self.model(torch.FloatTensor(sequence)).item()
        
        # åæ ‡å‡†åŒ–
        if self.scaler_y is not None:
            prediction = self.scaler_y.inverse_transform([[prediction]])[0][0]
        
        return max(0, prediction)  # ç¡®ä¿éè´Ÿ
    
    def predict_with_minimal_data(self, snow_depth_mm: float, snow_fall_mm: float, 
                                snow_water_equivalent_mm: float, date: datetime) -> float:
        """ä½¿ç”¨æœ€å°æ•°æ®è¦æ±‚è¿›è¡Œé¢„æµ‹ - æ–°å¢æ–¹æ³•"""
        if not self.is_loaded:
            raise Exception("æ¨¡å‹æœªåŠ è½½")
        
        # å‡†å¤‡ç‰¹å¾
        features = self.prepare_input_features(snow_depth_mm, snow_fall_mm, snow_water_equivalent_mm, date)
        
        # å¦‚æœå†å²æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•
        if hasattr(self, '_historical_features') and len(self._historical_features) > 0:
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„å†å²æ•°æ®
            available_features = self._historical_features.copy()
            available_features.append(features.flatten())
            
            # å¦‚æœæ•°æ®ä»ç„¶ä¸è¶³ï¼Œä½¿ç”¨é‡å¤å¡«å……ï¼ˆä½†ç»™å‡ºæ˜ç¡®è­¦å‘Šï¼‰
            if len(available_features) < self.sequence_length:
                print(f"âš ï¸ è­¦å‘Šï¼šå†å²æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é‡å¤å¡«å……ã€‚è¿™å¯èƒ½å¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ã€‚")
                print(f"   éœ€è¦ {self.sequence_length} ä¸ªæ•°æ®ç‚¹ï¼Œå®é™…åªæœ‰ {len(available_features)} ä¸ª")
                
                # é‡å¤æœ€åä¸€ä¸ªç‰¹å¾ç›´åˆ°è¾¾åˆ°åºåˆ—é•¿åº¦
                while len(available_features) < self.sequence_length:
                    available_features.append(available_features[-1])
            
            # å–æœ€åsequence_lengthä¸ªç‰¹å¾
            sequence = available_features[-self.sequence_length:]
        else:
            # å®Œå…¨æ²¡æœ‰å†å²æ•°æ®ï¼Œæ‹’ç»é¢„æµ‹
            raise ValueError(
                "æ— æ³•è¿›è¡Œé¢„æµ‹ï¼šå®Œå…¨æ²¡æœ‰å†å²æ•°æ®ã€‚"
                "è¯·å…ˆä½¿ç”¨ update_historical_features() æ–¹æ³•æ·»åŠ å†å²æ•°æ®ï¼Œ"
                "æˆ–ä½¿ç”¨ predict_with_minimal_data() æ–¹æ³•è¿›è¡Œæœ‰é™é¢„æµ‹ã€‚"
            )
        
        sequence = np.array(sequence).reshape(1, self.sequence_length, -1)
        
        # é¢„æµ‹
        with torch.no_grad():
            if hasattr(self, 'ensemble_models') and self.ensemble_models:
                predictions = []
                for model in self.ensemble_models:
                    pred = model(torch.FloatTensor(sequence))
                    predictions.append(pred.item())
                prediction = np.mean(predictions)
            else:
                prediction = self.model(torch.FloatTensor(sequence)).item()
        
        # åæ ‡å‡†åŒ–
        if self.scaler_y is not None:
            prediction = self.scaler_y.inverse_transform([[prediction]])[0][0]
        
        return max(0, prediction)
    
    def initialize_with_real_data(self, real_features: np.ndarray):
        """ä½¿ç”¨çœŸå®æ•°æ®åˆå§‹åŒ–å†å²ç‰¹å¾ - ç¦æ­¢åˆæˆæ•°æ®"""
        print("âœ… ä½¿ç”¨çœŸå®æ•°æ®åˆå§‹åŒ–å†å²ç‰¹å¾")
        
        if not hasattr(self, '_historical_features'):
            self._historical_features = []
        
        # åªæ¥å—çœŸå®è§‚æµ‹æ•°æ®
        if real_features is not None and len(real_features) > 0:
            self._historical_features.append(real_features.flatten())
            print(f"âœ… å·²æ·»åŠ çœŸå®å†å²ç‰¹å¾æ•°æ®")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æä¾›çœŸå®æ•°æ®ï¼Œå†å²ç‰¹å¾ä¿æŒä¸ºç©º")
            print("âš ï¸ æ³¨æ„ï¼šç³»ç»Ÿç¦æ­¢ä½¿ç”¨åˆæˆæ•°æ®ï¼Œè¯·æä¾›çœŸå®çš„è§‚æµ‹æ•°æ®")
    
    def update_historical_features(self, features: np.ndarray):
        """æ›´æ–°å†å²ç‰¹å¾æ•°æ® - æ–°å¢æ–¹æ³•"""
        if not hasattr(self, '_historical_features'):
            self._historical_features = []
        
        self._historical_features.append(features.flatten())
        
        # ä¿æŒæœ€è¿‘çš„å†å²æ•°æ®
        if len(self._historical_features) > self.sequence_length * 2:
            self._historical_features = self._historical_features[-self.sequence_length * 2:]
    
    def predict_series(self, start_date: str, end_date: str, 
                      snow_data: pd.DataFrame) -> List[Dict[str, Any]]:
        """é¢„æµ‹æ—¶é—´åºåˆ—"""
        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
            
            # ç”Ÿæˆæ—¥æœŸèŒƒå›´
            date_range = pd.date_range(start=start_dt, end=end_dt, freq='D')
            
            predictions = []
            for date in date_range:
                # æŸ¥æ‰¾å¯¹åº”çš„é›ªæ•°æ®
                date_str = date.strftime("%Y-%m-%d")
                if date_str in snow_data.index:
                    row = snow_data.loc[date_str]
                    snow_depth = row.get('snow_depth_mm', 0)
                    snow_fall = row.get('snow_fall_mm', 0)
                    snow_we = row.get('snow_water_equivalent_mm', 0)
                else:
                    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    snow_depth = 0
                    snow_fall = 0
                    snow_we = 0
                
                # é¢„æµ‹
                prediction = self.predict_single(snow_depth, snow_fall, snow_we, date)
                
                predictions.append({
                    'date': date_str,
                    'snow_depth_mm': snow_depth,
                    'snow_fall_mm': snow_fall,
                    'snow_water_equivalent_mm': snow_we,
                    'predicted_swe_mm': round(prediction, 2),
                    'confidence': 'high' if self.is_loaded else 'low'
                })
            
            return predictions
            
        except Exception as e:
            print(f"âŒ åºåˆ—é¢„æµ‹å¤±è´¥: {e}")
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            'model_type': 'ensemble' if hasattr(self, 'ensemble_models') else 'single',
            'is_loaded': self.is_loaded,
            'sequence_length': self.sequence_length,
            'features': ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 'day_of_year', 'month', 'year']
        }
        
        if hasattr(self, 'ensemble_models'):
            info['n_models'] = len(self.ensemble_models)
            info['ensemble_method'] = 'simple_average'
        
        if self.scaler_X is not None:
            info['scaling'] = 'standardized'
        
        return info
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        return {
            'status': 'healthy' if self.is_loaded else 'unhealthy',
            'model_loaded': self.is_loaded,
            'timestamp': datetime.now().isoformat(),
            'model_info': self.get_model_info()
        }

# å…¨å±€é¢„æµ‹å™¨å®ä¾‹
_global_predictor = None

def get_predictor() -> OptimizedSWEPredictor:
    """è·å–å…¨å±€é¢„æµ‹å™¨å®ä¾‹"""
    global _global_predictor
    if _global_predictor is None:
        _global_predictor = OptimizedSWEPredictor()
    return _global_predictor
