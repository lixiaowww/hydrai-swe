#!/usr/bin/env python3
"""
ä½¿ç”¨PyTorchç›´æ¥è®­ç»ƒLSTMæ¨¡å‹
é¿å…NeuralHydrologyçš„å¤æ‚æ€§ï¼Œç›´æ¥å®ç°è®­ç»ƒæµç¨‹
"""

import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

class SnowRunoffDataset(Dataset):
    """ç§¯é›ª-å¾„æµæ•°æ®é›†"""
    
    def __init__(self, data, sequence_length=30):
        self.data = data
        self.sequence_length = sequence_length
        
    def __len__(self):
        return len(self.data) - self.sequence_length
        
    def __getitem__(self, idx):
        # è·å–è¾“å…¥åºåˆ—
        x = self.data[idx:idx + self.sequence_length, :-1]  # é™¤äº†æœ€åä¸€åˆ—ï¼ˆç›®æ ‡å˜é‡ï¼‰
        y = self.data[idx + self.sequence_length, -1]  # ç›®æ ‡å˜é‡
        
        return torch.FloatTensor(x), torch.FloatTensor([y])

class LSTMRegressor(nn.Module):
    """LSTMå›å½’æ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):
        super(LSTMRegressor, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        batch_size = x.size(0)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = out[:, -1, :]
        
        # å…¨è¿æ¥å±‚
        out = self.dropout(out)
        out = self.fc(out)
        
        return out

def create_training_data():
    """åˆ›å»ºè®­ç»ƒæ•°æ®"""
    print("Creating training data...")
    
    # åˆ›å»ºæ—¶é—´åºåˆ—
    dates = pd.date_range('1979-01-01', '1998-12-31', freq='D')
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
    
    # ç§¯é›ªæ·±åº¦ï¼šå­£èŠ‚æ€§å˜åŒ– + éšæœºå™ªå£°
    seasonal_snow = 100 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25) + 50
    snow_depth = np.maximum(0, seasonal_snow + np.random.normal(0, 20, len(dates)))
    
    # é™é›ªé‡ï¼šå†¬å­£è¾ƒé«˜
    snow_fall = np.where(dates.month.isin([12, 1, 2, 3]), 
                         np.random.exponential(10, len(dates)), 
                         np.random.exponential(2, len(dates)))
    
    # é›ªæ°´å½“é‡ï¼šç§¯é›ªæ·±åº¦çš„30%
    snow_water_equivalent = snow_depth * 0.3
    
    # å¾„æµï¼šåŸºäºç§¯é›ªèåŒ–çš„ç®€åŒ–æ¨¡å‹
    streamflow = 1000 + snow_depth * 0.1 + np.random.normal(0, 50, len(dates))
    streamflow = np.maximum(500, streamflow)  # æœ€å°å¾„æµ
    
    # åˆ›å»ºDataFrame
    data = pd.DataFrame({
        'date': dates,
        'snow_depth_mm': snow_depth,
        'snow_fall_mm': snow_fall,
        'snow_water_equivalent_mm': snow_water_equivalent,
        'day_of_year': dates.dayofyear,
        'month': dates.month,
        'year': dates.year,
        'streamflow_m3s': streamflow
    })
    
    print(f"Created {len(data)} records")
    print(f"Date range: {data['date'].min()} to {data['date'].max()}")
    
    return data

def prepare_data_for_training(data, sequence_length=30):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("Preparing data for training...")
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_columns = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                      'day_of_year', 'month', 'year', 'streamflow_m3s']
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    features = data[feature_columns].values
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(features_scaled))
    train_data = features_scaled[:train_size]
    val_data = features_scaled[train_size:]
    
    print(f"Training set: {len(train_data)} samples")
    print(f"Validation set: {len(val_data)} samples")
    
    return train_data, val_data, scaler

def train_model(train_data, val_data, model_params):
    """è®­ç»ƒæ¨¡å‹"""
    print("Starting model training...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = SnowRunoffDataset(train_data, model_params['sequence_length'])
    val_dataset = SnowRunoffDataset(val_data, model_params['sequence_length'])
    
    train_loader = DataLoader(train_dataset, batch_size=model_params['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=model_params['batch_size'], shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    model = LSTMRegressor(
        input_size=model_params['input_size'],
        hidden_size=model_params['hidden_size'],
        num_layers=model_params['num_layers'],
        dropout=model_params['dropout']
    ).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=model_params['learning_rate'])
    
    # è®­ç»ƒå¾ªç¯
    train_losses = []
    val_losses = []
    
    for epoch in range(model_params['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                val_predictions.extend(outputs.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        
        # è®¡ç®—RÂ²åˆ†æ•°
        val_r2 = r2_score(val_targets, val_predictions)
        
        print(f"Epoch {epoch+1}/{model_params['epochs']}: "
              f"Train Loss: {train_loss:.4f}, "
              f"Val Loss: {val_loss:.4f}, "
              f"Val RÂ²: {val_r2:.4f}")
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»å¤–éƒ¨ä¼ å…¥scalerï¼Œå› ä¸ºå®ƒåœ¨prepare_data_for_trainingå‡½æ•°ä¸­å®šä¹‰
    return model, train_losses, val_losses

def save_model_and_results(model, train_losses, val_losses, scaler, model_params):
    """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
    print("Saving model and results...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("models/pytorch_lstm")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / "snow_runoff_lstm.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_params': model_params,
        'scaler': scaler
    }, model_path)
    print(f"Model saved to: {model_path}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'model_params': model_params
    }
    
    history_path = output_dir / "training_history.json"
    import json
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2, default=str)
    print(f"Training history saved to: {history_path}")
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    
    plot_path = output_dir / "training_loss.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Training plot saved to: {plot_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç§¯é›ª-å¾„æµLSTMæ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # æ¨¡å‹å‚æ•°
    model_params = {
        'input_size': 6,  # ç‰¹å¾æ•°é‡
        'hidden_size': 64,
        'num_layers': 2,
        'dropout': 0.2,
        'sequence_length': 30,
        'batch_size': 32,
        'epochs': 50,
        'learning_rate': 0.001
    }
    
    print("Model parameters:")
    for key, value in model_params.items():
        print(f"  {key}: {value}")
    
    try:
        # åˆ›å»ºæ•°æ®
        data = create_training_data()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_data, val_data, scaler = prepare_data_for_training(data, model_params['sequence_length'])
        
        # è®­ç»ƒæ¨¡å‹
        model, train_losses, val_losses = train_model(train_data, val_data, model_params)
        
        # ä¿å­˜ç»“æœ
        save_model_and_results(model, train_losses, val_losses, scaler, model_params)
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ° models/pytorch_lstm/ ç›®å½•")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
