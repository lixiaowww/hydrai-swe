#!/usr/bin/env python3
"""
åŸºäºGitHubå‘ç°çš„SWEåˆ†æç³»ç»Ÿ
æ•´åˆå­£èŠ‚æ€§åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€ç›¸å…³æ€§åˆ†æç­‰åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from scipy import stats
from scipy.signal import periodogram
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class SWEAnalysisSystem:
    """SWEç»¼åˆåˆ†æç³»ç»Ÿ"""
    
    def __init__(self, data_path=None):
        """
        åˆå§‹åŒ–SWEåˆ†æç³»ç»Ÿ
        
        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data = None
        self.analysis_results = {}
        
        if data_path:
            self.load_data(data_path)
    
    def load_data(self, data_path):
        """åŠ è½½SWEæ•°æ®"""
        print(f"ğŸ“Š åŠ è½½SWEæ•°æ®: {data_path}")
        
        try:
            # é¦–å…ˆå°è¯•åŠ è½½æ•°æ®
            if not os.path.exists(data_path):
                # å°è¯•å¤‡ç”¨æ•°æ®è·¯å¾„
                backup_paths = [
                    "data/processed/eccc_manitoba_snow_processed.csv",
                    "data/raw/eccc_recent/eccc_recent_combined.csv"
                ]
                
                data_loaded = False
                for backup_path in backup_paths:
                    if os.path.exists(backup_path):
                        print(f"ğŸ“‚ ä½¿ç”¨å¤‡ç”¨æ•°æ®: {backup_path}")
                        data_path = backup_path
                        data_loaded = True
                        break
                
                if not data_loaded:
                    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            
            self.data = pd.read_csv(data_path)
            
            # å¤„ç†æ—¥æœŸåˆ—
            date_col = None
            for col_name in ['date', 'Date/Time', 'Date']:
                if col_name in self.data.columns:
                    date_col = col_name
                    break
            
            if date_col is None:
                raise ValueError("æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
            
            self.data[date_col] = pd.to_datetime(self.data[date_col], errors='coerce')
            self.data.set_index(date_col, inplace=True)
            
            # åˆ›å»ºæˆ–æ‰¾åˆ°snow_water_equivalent_mmåˆ—
            if 'snow_water_equivalent_mm' not in self.data.columns:
                # å°è¯•ä¸åŒçš„åˆ—åæ˜ å°„
                swe_candidates = [
                    'Snow on Grnd (mm)',  # å·²ç»æ˜¯mm
                    'Snow on Grnd (cm)',  # éœ€è¦è½¬æ¢cm->mm  
                    'Total Snow (mm)',
                    'Total Snow (cm)'
                ]
                
                swe_created = False
                for candidate in swe_candidates:
                    if candidate in self.data.columns:
                        if 'cm' in candidate:
                            # è½¬æ¢cmåˆ°mm
                            self.data['snow_water_equivalent_mm'] = self.data[candidate] * 10.0
                        else:
                            # å·²ç»æ˜¯mm
                            self.data['snow_water_equivalent_mm'] = self.data[candidate]
                        swe_created = True
                        print(f"âœ… ä» {candidate} åˆ›å»º snow_water_equivalent_mm åˆ—")
                        break
                
                if not swe_created:
                    # ä¸¥æ ¼éµå¾ªè§„åˆ™ï¼šä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                    print("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„é›ªæ°´å½“é‡æ•°æ®ï¼Œè®¾ç½®ä¸ºN/A")
                    self.data['snow_water_equivalent_mm'] = 'N/A'
            
            # æ•°æ®æ¸…ç†å’ŒéªŒè¯
            self.data['snow_water_equivalent_mm'] = pd.to_numeric(
                self.data['snow_water_equivalent_mm'], errors='coerce'
            )
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            original_len = len(self.data)
            self.data = self.data.dropna(subset=['snow_water_equivalent_mm'])
            
            if len(self.data) == 0:
                raise ValueError("å¤„ç†åæ•°æ®ä¸ºç©º")
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data)} æ¡è®°å½• (åŸå§‹: {original_len} æ¡)")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.data.index.min()} åˆ° {self.data.index.max()}")
            print(f"ğŸ“Š SWEæ•°æ®èŒƒå›´: {self.data['snow_water_equivalent_mm'].min():.1f} - {self.data['snow_water_equivalent_mm'].max():.1f} mm")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            self.data = None
    
    def seasonal_analysis(self, column='snow_water_equivalent_mm'):
        """
        å­£èŠ‚æ€§åˆ†æ - åŸºäºCIROH-Snowå’ŒSeasonal-Snowfall-Climatologyæ¨¡å—
        
        Args:
            column (str): è¦åˆ†æçš„åˆ—å
            
        Returns:
            dict: å­£èŠ‚æ€§åˆ†æç»“æœ
        """
        print(f"\nğŸŒ æ‰§è¡Œå­£èŠ‚æ€§åˆ†æ: {column}")
        print("=" * 50)
        
        if self.data is None:
            print("âŒ æ•°æ®æœªåŠ è½½")
            return {}
        
        if column not in self.data.columns:
            print(f"âŒ åˆ— {column} ä¸å­˜åœ¨")
            return {}
        
        series = self.data[column].dropna()
        
        if len(series) == 0:
            print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return {}
        
        try:
            # 1. å¹´åº¦å‘¨æœŸåˆ†æ
            annual_cycle = self._analyze_annual_cycle(series)
            
            # 2. æœˆåº¦ç»Ÿè®¡
            monthly_stats = self._analyze_monthly_patterns(series)
            
            # 3. å­£èŠ‚æ€§åˆ†è§£
            seasonal_decomposition = self._perform_seasonal_decomposition(series)
            
            # 4. é¢‘è°±åˆ†æ
            frequency_analysis = self._analyze_frequency_domain(series)
            
            results = {
                'annual_cycle': annual_cycle,
                'monthly_patterns': monthly_stats,
                'seasonal_decomposition': seasonal_decomposition,
                'frequency_analysis': frequency_analysis
            }
            
            self.analysis_results['seasonal_analysis'] = results
            print("âœ… å­£èŠ‚æ€§åˆ†æå®Œæˆ")
            return results
            
        except Exception as e:
            print(f"âŒ å­£èŠ‚æ€§åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _analyze_annual_cycle(self, series):
        """åˆ†æå¹´åº¦å‘¨æœŸ"""
        print("ğŸ“… åˆ†æå¹´åº¦å‘¨æœŸ...")
        
        # æŒ‰å¹´åˆ†ç»„
        annual_data = series.groupby(series.index.year).agg([
            'mean', 'std', 'min', 'max', 'count'
        ]).dropna()
        
        # è®¡ç®—å¹´é™…å˜åŒ–
        years = np.array(annual_data.index)
        means = annual_data['mean'].values
        
        if len(means) > 1:
            # çº¿æ€§è¶‹åŠ¿
            slope, intercept, r_value, p_value, std_err = stats.linregress(years, means)
            
            annual_trend = {
                'slope': slope,
                'intercept': intercept,
                'r_squared': r_value**2,
                'p_value': p_value,
                'trend_per_decade': slope * 10
            }
        else:
            annual_trend = {}
        
        return {
            'annual_statistics': annual_data,
            'trend_analysis': annual_trend
        }
    
    def _analyze_monthly_patterns(self, series):
        """åˆ†ææœˆåº¦æ¨¡å¼"""
        print("ğŸ“Š åˆ†ææœˆåº¦æ¨¡å¼...")
        
        # æŒ‰æœˆä»½åˆ†ç»„
        monthly_data = series.groupby(series.index.month).agg([
            'mean', 'std', 'min', 'max', 'count'
        ])
        
        # è®¡ç®—å­£èŠ‚æ€§æŒ‡æ•°
        overall_mean = series.mean()
        seasonal_indices = monthly_data['mean'] / overall_mean
        
        return {
            'monthly_statistics': monthly_data,
            'seasonal_indices': seasonal_indices,
            'overall_mean': overall_mean
        }
    
    def _perform_seasonal_decomposition(self, series):
        """æ‰§è¡Œå­£èŠ‚æ€§åˆ†è§£"""
        print("ğŸ” æ‰§è¡Œå­£èŠ‚æ€§åˆ†è§£...")
        
        # ç¡®ä¿æ•°æ®æ˜¯ç­‰é—´éš”çš„
        series_resampled = series.resample('D').mean().fillna(method='ffill')
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window = 365  # ä¸€å¹´çª—å£
        trend = series_resampled.rolling(window=window, center=True).mean()
        
        # è®¡ç®—å­£èŠ‚æ€§æˆåˆ†
        seasonal = series_resampled - trend
        
        # è®¡ç®—æ®‹å·®
        residual = series_resampled - trend - seasonal
        
        return {
            'original': series_resampled,
            'trend': trend,
            'seasonal': seasonal,
            'residual': residual
        }
    
    def _analyze_frequency_domain(self, series):
        """é¢‘åŸŸåˆ†æ"""
        print("ğŸ“¡ æ‰§è¡Œé¢‘åŸŸåˆ†æ...")
        
        # ç¡®ä¿æ•°æ®æ˜¯ç­‰é—´éš”çš„
        series_resampled = series.resample('D').mean().fillna(method='ffill')
        
        # è®¡ç®—åŠŸç‡è°±å¯†åº¦
        frequencies, power = periodogram(series_resampled.dropna(), fs=1.0)
        
        # æ‰¾åˆ°ä¸»è¦é¢‘ç‡
        main_freq_idx = np.argmax(power)
        main_frequency = frequencies[main_freq_idx]
        main_period = 1.0 / main_frequency if main_frequency > 0 else np.inf
        
        return {
            'frequencies': frequencies,
            'power': power,
            'main_frequency': main_frequency,
            'main_period': main_period
        }
    
    def anomaly_detection(self, column='snow_water_equivalent_mm'):
        """
        å¼‚å¸¸æ£€æµ‹ - åŸºäºKathiravanNatarajan/SnowDepth_AnomalyDetectionæ¨¡å—
        
        Args:
            column (str): è¦æ£€æµ‹çš„åˆ—å
            
        Returns:
            dict: å¼‚å¸¸æ£€æµ‹ç»“æœ
        """
        print(f"\nğŸš¨ æ‰§è¡Œå¼‚å¸¸æ£€æµ‹: {column}")
        print("=" * 50)
        
        if column not in self.data.columns:
            print(f"âŒ åˆ— {column} ä¸å­˜åœ¨")
            return {}
        
        series = self.data[column].dropna()
        
        # 1. ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹
        statistical_anomalies = self._statistical_anomaly_detection(series)
        
        # 2. æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
        ml_anomalies = self._machine_learning_anomaly_detection(series)
        
        # 3. æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
        timeseries_anomalies = self._timeseries_anomaly_detection(series)
        
        # 4. ç»¼åˆå¼‚å¸¸è¯„åˆ†
        combined_anomalies = self._combine_anomaly_scores(
            statistical_anomalies, ml_anomalies, timeseries_anomalies
        )
        
        results = {
            'statistical': statistical_anomalies,
            'machine_learning': ml_anomalies,
            'timeseries': timeseries_anomalies,
            'combined': combined_anomalies
        }
        
        self.analysis_results['anomaly_detection'] = results
        return results
    
    def _statistical_anomaly_detection(self, series):
        """ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹"""
        print("ğŸ“Š ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹...")
        
        # Z-scoreæ–¹æ³•
        z_scores = np.abs(stats.zscore(series))
        z_anomalies = z_scores > 3
        
        # IQRæ–¹æ³•
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        iqr_anomalies = (series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))
        
        # ç§»åŠ¨çª—å£æ–¹æ³•
        rolling_mean = series.rolling(window=30, center=True).mean()
        rolling_std = series.rolling(window=30, center=True).std()
        rolling_anomalies = np.abs(series - rolling_mean) > 3 * rolling_std
        
        return {
            'z_score_anomalies': z_anomalies,
            'iqr_anomalies': iqr_anomalies,
            'rolling_anomalies': rolling_anomalies,
            'z_scores': z_scores
        }
    
    def _machine_learning_anomaly_detection(self, series):
        """æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹"""
        print("ğŸ¤– æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹...")
        
        # å‡†å¤‡æ•°æ®
        X = series.values.reshape(-1, 1)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1)
        iso_predictions = iso_forest.fit_predict(X_scaled)
        iso_anomalies = iso_predictions == -1
        
        return {
            'isolation_forest_anomalies': iso_anomalies,
            'isolation_forest_scores': iso_forest.decision_function(X_scaled)
        }
    
    def _timeseries_anomaly_detection(self, series):
        """æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹"""
        print("â° æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹...")
        
        # åŸºäºè¶‹åŠ¿çš„å¼‚å¸¸æ£€æµ‹
        rolling_mean = series.rolling(window=30, center=True).mean()
        trend_anomalies = np.abs(series - rolling_mean) > 2 * series.std()
        
        # åŸºäºå­£èŠ‚æ€§çš„å¼‚å¸¸æ£€æµ‹
        monthly_means = series.groupby(series.index.month).mean()
        seasonal_anomalies = np.abs(series - monthly_means[series.index.month].values) > 2 * series.std()
        
        return {
            'trend_anomalies': trend_anomalies,
            'seasonal_anomalies': seasonal_anomalies
        }
    
    def _combine_anomaly_scores(self, statistical, ml, timeseries):
        """ç»¼åˆå¼‚å¸¸è¯„åˆ†"""
        print("ğŸ”— ç»¼åˆå¼‚å¸¸è¯„åˆ†...")
        
        # è®¡ç®—ç»¼åˆå¼‚å¸¸æ¦‚ç‡
        combined_score = np.zeros(len(self.data))
        
        # ç»Ÿè®¡æ–¹æ³•æƒé‡
        if 'z_score_anomalies' in statistical:
            combined_score += statistical['z_score_anomalies'].astype(int) * 0.3
        if 'iqr_anomalies' in statistical:
            combined_score += statistical['iqr_anomalies'].astype(int) * 0.3
        
        # æœºå™¨å­¦ä¹ æƒé‡
        if 'isolation_forest_anomalies' in ml:
            combined_score += ml['isolation_forest_anomalies'].astype(int) * 0.4
        
        # å½’ä¸€åŒ–åˆ°0-1
        combined_score = combined_score / combined_score.max() if combined_score.max() > 0 else combined_score
        
        # ç¡®å®šå¼‚å¸¸é˜ˆå€¼
        threshold = 0.5
        combined_anomalies = combined_score > threshold
        
        return {
            'combined_score': combined_score,
            'combined_anomalies': combined_anomalies,
            'threshold': threshold
        }
    
    def correlation_analysis(self, target_column='snow_water_equivalent_mm'):
        """
        ç›¸å…³æ€§åˆ†æ - åŸºäºBike-sharing-system-Analysisæ¨¡å—
        
        Args:
            target_column (str): ç›®æ ‡å˜é‡åˆ—å
            
        Returns:
            dict: ç›¸å…³æ€§åˆ†æç»“æœ
        """
        print(f"\nğŸ”— æ‰§è¡Œç›¸å…³æ€§åˆ†æ: {target_column}")
        print("=" * 50)
        
        if target_column not in self.data.columns:
            print(f"âŒ åˆ— {target_column} ä¸å­˜åœ¨")
            return {}
        
        # é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œç›¸å…³æ€§åˆ†æ
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()
        
        if target_column not in numeric_columns:
            numeric_columns.append(target_column)
        
        correlation_data = self.data[numeric_columns].dropna()
        
        # 1. çš®å°”é€Šç›¸å…³ç³»æ•°
        pearson_corr = correlation_data.corr(method='pearson')
        
        # 2. æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°
        spearman_corr = correlation_data.corr(method='spearman')
        
        # 3. ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
        target_correlations = {}
        for col in numeric_columns:
            if col != target_column:
                # çš®å°”é€Šç›¸å…³
                pearson_r, pearson_p = stats.pearsonr(
                    correlation_data[target_column], correlation_data[col]
                )
                
                # æ–¯çš®å°”æ›¼ç›¸å…³
                spearman_r, spearman_p = stats.spearmanr(
                    correlation_data[target_column], correlation_data[col]
                )
                
                target_correlations[col] = {
                    'pearson_r': pearson_r,
                    'pearson_p': pearson_p,
                    'spearman_r': spearman_r,
                    'spearman_p': spearman_p
                }
        
        # 4. æ»šåŠ¨ç›¸å…³æ€§åˆ†æ
        rolling_corr = self._calculate_rolling_correlations(correlation_data, target_column)
        
        results = {
            'pearson_correlation': pearson_corr,
            'spearman_correlation': spearman_corr,
            'target_correlations': target_correlations,
            'rolling_correlations': rolling_corr
        }
        
        self.analysis_results['correlation_analysis'] = results
        return results
    
    def _calculate_rolling_correlations(self, data, target_column, window=365):
        """è®¡ç®—æ»šåŠ¨ç›¸å…³æ€§"""
        print("ğŸ“ˆ è®¡ç®—æ»šåŠ¨ç›¸å…³æ€§...")
        
        rolling_corrs = {}
        for col in data.columns:
            if col != target_column:
                rolling_corr = data[target_column].rolling(window=window).corr(data[col])
                rolling_corrs[col] = rolling_corr
        
        return rolling_corrs
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
            return
        
        # å­£èŠ‚æ€§åˆ†ææ€»ç»“
        if 'seasonal_analysis' in self.analysis_results:
            seasonal = self.analysis_results['seasonal_analysis']
            print("\nğŸŒ å­£èŠ‚æ€§åˆ†ææ€»ç»“:")
            
            if 'annual_cycle' in seasonal:
                trend = seasonal['annual_cycle'].get('trend_analysis', {})
                if trend:
                    print(f"  å¹´é™…è¶‹åŠ¿: {trend.get('trend_per_decade', 0):.4f} æ¯10å¹´")
                    print(f"  è¶‹åŠ¿æ˜¾è‘—æ€§: {'æ˜¯' if trend.get('p_value', 1) < 0.05 else 'å¦'}")
            
            if 'monthly_patterns' in seasonal:
                monthly = seasonal['monthly_patterns']
                print(f"  å­£èŠ‚æ€§æŒ‡æ•°èŒƒå›´: {monthly.get('seasonal_indices', pd.Series()).min():.2f} - {monthly.get('seasonal_indices', pd.Series()).max():.2f}")
        
        # å¼‚å¸¸æ£€æµ‹æ€»ç»“
        if 'anomaly_detection' in self.analysis_results:
            anomaly = self.analysis_results['anomaly_detection']
            print("\nğŸš¨ å¼‚å¸¸æ£€æµ‹æ€»ç»“:")
            
            if 'combined' in anomaly:
                combined = anomaly['combined']
                anomaly_count = combined.get('combined_anomalies', pd.Series()).sum()
                total_count = len(combined.get('combined_anomalies', pd.Series()))
                anomaly_rate = anomaly_count / total_count if total_count > 0 else 0
                print(f"  æ£€æµ‹åˆ°å¼‚å¸¸: {anomaly_count}/{total_count} ({anomaly_rate:.2%})")
        
        # ç›¸å…³æ€§åˆ†ææ€»ç»“
        if 'correlation_analysis' in self.analysis_results:
            correlation = self.analysis_results['correlation_analysis']
            print("\nğŸ”— ç›¸å…³æ€§åˆ†ææ€»ç»“:")
            
            if 'target_correlations' in correlation:
                target_corr = correlation['target_correlations']
                strong_correlations = []
                for col, corr_data in target_corr.items():
                    if abs(corr_data.get('pearson_r', 0)) > 0.7:
                        strong_correlations.append((col, corr_data['pearson_r']))
                
                if strong_correlations:
                    print(f"  å¼ºç›¸å…³å˜é‡: {len(strong_correlations)} ä¸ª")
                    for col, r in strong_correlations[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                        print(f"    {col}: r = {r:.3f}")
                else:
                    print("  æ— å¼ºç›¸å…³å˜é‡")
        
        print("\n" + "=" * 60)
    
    def plot_analysis_results(self, save_path=None):
        """ç»˜åˆ¶åˆ†æç»“æœå›¾è¡¨"""
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
            return
        
        print("ğŸ“Š ç»˜åˆ¶åˆ†æç»“æœå›¾è¡¨...")
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('SWEç»¼åˆåˆ†æç»“æœ', fontsize=16)
        
        # 1. å­£èŠ‚æ€§åˆ†æ
        if 'seasonal_analysis' in self.analysis_results:
            seasonal = self.analysis_results['seasonal_analysis']
            
            if 'monthly_patterns' in seasonal:
                monthly = seasonal['monthly_patterns']
                seasonal_indices = monthly.get('seasonal_indices', pd.Series())
                if not seasonal_indices.empty:
                    axes[0, 0].plot(seasonal_indices.index, seasonal_indices.values, 'o-')
                    axes[0, 0].axhline(y=1, color='r', linestyle='--', alpha=0.5)
                    axes[0, 0].set_title('æœˆåº¦å­£èŠ‚æ€§æŒ‡æ•°')
                    axes[0, 0].set_xlabel('æœˆä»½')
                    axes[0, 0].set_ylabel('å­£èŠ‚æ€§æŒ‡æ•°')
                    axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å¼‚å¸¸æ£€æµ‹
        if 'anomaly_detection' in self.analysis_results:
            anomaly = self.analysis_results['anomaly_detection']
            
            if 'combined' in anomaly:
                combined = anomaly['combined']
                combined_score = combined.get('combined_score', [])
                if len(combined_score) > 0:
                    axes[0, 1].plot(combined_score, alpha=0.7)
                    axes[0, 1].axhline(y=combined.get('threshold', 0.5), color='r', linestyle='--')
                    axes[0, 1].set_title('ç»¼åˆå¼‚å¸¸è¯„åˆ†')
                    axes[0, 1].set_ylabel('å¼‚å¸¸è¯„åˆ†')
                    axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç›¸å…³æ€§çƒ­å›¾
        if 'correlation_analysis' in self.analysis_results:
            correlation = self.analysis_results['correlation_analysis']
            
            if 'pearson_correlation' in correlation:
                corr_matrix = correlation['pearson_correlation']
                if not corr_matrix.empty:
                    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                               ax=axes[1, 0], cbar_kws={'label': 'ç›¸å…³ç³»æ•°'})
                    axes[1, 0].set_title('çš®å°”é€Šç›¸å…³æ€§çŸ©é˜µ')
        
        # 4. æ—¶é—´åºåˆ—åˆ†è§£
        if 'seasonal_analysis' in self.analysis_results:
            seasonal = self.analysis_results['seasonal_analysis']
            
            if 'seasonal_decomposition' in seasonal:
                decomp = seasonal['seasonal_decomposition']
                if 'trend' in decomp and 'seasonal' in decomp:
                    # é€‰æ‹©æœ€è¿‘çš„æ•°æ®ç‚¹è¿›è¡Œå¯è§†åŒ–
                    n_points = min(1000, len(decomp['trend']))
                    x = range(n_points)
                    
                    axes[1, 1].plot(x, decomp['trend'].iloc[-n_points:], label='è¶‹åŠ¿', alpha=0.7)
                    axes[1, 1].plot(x, decomp['seasonal'].iloc[-n_points:], label='å­£èŠ‚æ€§', alpha=0.7)
                    axes[1, 1].set_title('æ—¶é—´åºåˆ—åˆ†è§£')
                    axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
                    axes[1, 1].set_ylabel('å€¼')
                    axes[1, 1].legend()
                    axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨ä¿å­˜åˆ°: {save_path}")
        
        plt.show()

def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸš€ SWEç»¼åˆåˆ†æç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†æç³»ç»Ÿ
    analyzer = SWEAnalysisSystem()
    
    # åŠ è½½æ•°æ®
    data_path = "src/neuralhydrology/data/red_river_basin/timeseries.csv"
    analyzer.load_data(data_path)
    
    if analyzer.data is not None:
        # æ‰§è¡Œç»¼åˆåˆ†æ
        print("\nğŸ” å¼€å§‹ç»¼åˆåˆ†æ...")
        
        # 1. å­£èŠ‚æ€§åˆ†æ
        seasonal_results = analyzer.seasonal_analysis('snow_water_equivalent_mm')
        
        # 2. å¼‚å¸¸æ£€æµ‹
        anomaly_results = analyzer.anomaly_detection('snow_water_equivalent_mm')
        
        # 3. ç›¸å…³æ€§åˆ†æ
        correlation_results = analyzer.correlation_analysis('snow_water_equivalent_mm')
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        analyzer.generate_comprehensive_report()
        
        # 5. ç»˜åˆ¶ç»“æœ
        analyzer.plot_analysis_results()
        
        print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
