#!/usr/bin/env python3
"""
ERA5ç®€åŒ–åœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹
ä¸“é—¨é’ˆå¯¹å°æ•°æ®é›†è®¾è®¡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import logging
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleSoilMoistureLSTM(nn.Module):
    """ç®€åŒ–çš„åœŸå£¤æ¹¿åº¦LSTMæ¨¡å‹ - é˜²è¿‡æ‹Ÿåˆè®¾è®¡"""
    
    def __init__(self, input_size: int, hidden_size: int = 16, num_layers: int = 1, dropout: float = 0.3):
        super(SimpleSoilMoistureLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # è¾“å…¥å±‚æ ‡å‡†åŒ–
        self.input_norm = nn.BatchNorm1d(input_size)
        
        # ç®€åŒ–çš„LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0  # å•å±‚LSTMä¸ä½¿ç”¨å†…ç½®dropout
        )
        
        # ç®€åŒ–çš„å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_size, 8)
        self.fc2 = nn.Linear(8, 1)
        
        # æ›´å¼ºçš„Dropout
        self.dropout = nn.Dropout(dropout)
        
        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
        logger.info(f"âœ… ç®€åŒ–LSTMæ¨¡å‹åˆ›å»º: input_size={input_size}, hidden_size={hidden_size}, dropout={dropout}")
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for name, param in self.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
    
    def forward(self, x):
        batch_size, seq_len, features = x.shape
        
        # è¾“å…¥æ ‡å‡†åŒ– (å¯¹æ¯ä¸ªæ—¶é—´æ­¥)
        x_reshaped = x.view(-1, features)
        x_norm = self.input_norm(x_reshaped)
        x = x_norm.view(batch_size, seq_len, features)
        
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        
        # æ·»åŠ å¼ºdropout
        last_output = self.dropout(last_output)
        
        # ç®€åŒ–çš„å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(last_output))
        x = self.dropout(x)
        x = self.fc2(x)
        
        # ç¡®ä¿è¾“å‡ºåœ¨0-1ä¹‹é—´
        x = self.sigmoid(x)
        
        return x

class ERA5SimplePredictor:
    """ERA5ç®€åŒ–é¢„æµ‹å™¨ - é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬"""
    
    def __init__(self, model_dir: str = "models/era5_simple"):
        self.model_dir = model_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ç®€åŒ–çš„æ¨¡å‹é…ç½®
        self.config = {
            'input_size': None,
            'hidden_size': 16,  # å¤§å¹…å‡å°‘
            'num_layers': 1,    # å‡å°‘å±‚æ•°
            'dropout': 0.4,     # å¢åŠ dropout
            'learning_rate': 0.01,  # å¢åŠ å­¦ä¹ ç‡
            'batch_size': 8,    # å‡å°‘æ‰¹æ¬¡å¤§å°
            'epochs': 50,       # å‡å°‘è®­ç»ƒè½®æ•°
            'sequence_length': 5,  # å‡å°‘åºåˆ—é•¿åº¦
            'patience': 8,      # å‡å°‘è€å¿ƒå€¼
            'min_delta': 0.001, # å¢åŠ æœ€å°æ”¹è¿›
            'weight_decay': 0.01,  # L2æ­£åˆ™åŒ–
            'feature_selection': True,  # å¯ç”¨ç‰¹å¾é€‰æ‹©
            'k_best_features': 10  # åªé€‰æ‹©æœ€é‡è¦çš„10ä¸ªç‰¹å¾
        }
        
        # ç‰¹å¾é€‰æ‹©å™¨å’Œæ ‡å‡†åŒ–å™¨
        self.feature_selector = None
        self.scaler = None
        
        # æ¨¡å‹ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.criterion = None
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': [],
            'learning_rates': []
        }
        
        logger.info(f"âœ… ERA5ç®€åŒ–é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def select_features(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """ç‰¹å¾é€‰æ‹©"""
        try:
            logger.info("ğŸ” æ‰§è¡Œç‰¹å¾é€‰æ‹©...")
            
            if not self.config['feature_selection']:
                return X
            
            # å°†åºåˆ—æ•°æ®é‡å¡‘ä¸º2D
            n_samples, seq_len, n_features = X.shape
            X_2d = X.reshape(-1, n_features)
            y_expanded = np.repeat(y, seq_len)
            
            # ç‰¹å¾é€‰æ‹©
            if self.feature_selector is None:
                self.feature_selector = SelectKBest(
                    score_func=f_regression,
                    k=self.config['k_best_features']
                )
                X_selected_2d = self.feature_selector.fit_transform(X_2d, y_expanded)
            else:
                X_selected_2d = self.feature_selector.transform(X_2d)
            
            # é‡å¡‘å›åºåˆ—å½¢çŠ¶
            X_selected = X_selected_2d.reshape(n_samples, seq_len, -1)
            
            # æ›´æ–°è¾“å…¥å¤§å°
            self.config['input_size'] = X_selected.shape[2]
            
            # è·å–é€‰ä¸­çš„ç‰¹å¾åç§°
            selected_features = self.feature_selector.get_support(indices=True)
            logger.info(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(selected_features)} ä¸ªç‰¹å¾è¢«é€‰ä¸­")
            logger.info(f"ğŸ“Š é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•: {selected_features[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
            
            return X_selected
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return X
    
    def scale_features(self, X: np.ndarray, fit: bool = True) -> np.ndarray:
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        try:
            logger.info("ğŸ“ æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
            
            # å°†åºåˆ—æ•°æ®é‡å¡‘ä¸º2D
            n_samples, seq_len, n_features = X.shape
            X_2d = X.reshape(-1, n_features)
            
            # æ ‡å‡†åŒ–
            if fit:
                if self.scaler is None:
                    self.scaler = StandardScaler()
                X_scaled_2d = self.scaler.fit_transform(X_2d)
            else:
                X_scaled_2d = self.scaler.transform(X_2d)
            
            # é‡å¡‘å›åºåˆ—å½¢çŠ¶
            X_scaled = X_scaled_2d.reshape(n_samples, seq_len, n_features)
            
            logger.info("âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
            return X_scaled
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return X
    
    def build_model(self, input_size: int) -> None:
        """æ„å»ºç®€åŒ–æ¨¡å‹"""
        try:
            logger.info(f"ğŸ”§ æ„å»ºç®€åŒ–æ¨¡å‹ï¼Œè¾“å…¥ç‰¹å¾æ•°: {input_size}")
            
            self.config['input_size'] = input_size
            
            # åˆ›å»ºç®€åŒ–æ¨¡å‹
            self.model = SimpleSoilMoistureLSTM(
                input_size=input_size,
                hidden_size=self.config['hidden_size'],
                num_layers=self.config['num_layers'],
                dropout=self.config['dropout']
            ).to(self.device)
            
            # åˆ›å»ºä¼˜åŒ–å™¨ (æ·»åŠ L2æ­£åˆ™åŒ–)
            self.optimizer = optim.Adam(
                self.model.parameters(), 
                lr=self.config['learning_rate'],
                weight_decay=self.config['weight_decay']
            )
            
            # åˆ›å»ºæŸå¤±å‡½æ•°
            self.criterion = nn.MSELoss()
            
            # å­¦ä¹ ç‡è°ƒåº¦å™¨ (æ›´æ¿€è¿›çš„è¡°å‡)
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, 
                mode='min', 
                factor=0.3,  # æ›´å¤§çš„è¡°å‡å› å­
                patience=5   # æ›´å°çš„è€å¿ƒå€¼
            )
            
            logger.info("âœ… ç®€åŒ–æ¨¡å‹æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            raise
    
    def load_and_preprocess_data(self, data_dir: str = "data/processed/era5") -> Dict:
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
        try:
            logger.info(f"ğŸ“¥ åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®: {data_dir}")
            
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶
            required_files = ['X_train.npy', 'y_train.npy', 'X_val.npy', 'y_val.npy', 'X_test.npy', 'y_test.npy']
            for file in required_files:
                file_path = os.path.join(data_dir, file)
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # åŠ è½½åŸå§‹æ•°æ®
            data = {}
            for split in ['train', 'val', 'test']:
                X_file = os.path.join(data_dir, f'X_{split}.npy')
                y_file = os.path.join(data_dir, f'y_{split}.npy')
                
                data[f'X_{split}_raw'] = np.load(X_file)
                data[f'y_{split}'] = np.load(y_file)
            
            # ç‰¹å¾é€‰æ‹© (åªåœ¨è®­ç»ƒé›†ä¸Šfit)
            X_train_selected = self.select_features(data['X_train_raw'], data['y_train'])
            X_val_selected = self.select_features(data['X_val_raw'], data['y_val']) if self.feature_selector else data['X_val_raw']
            X_test_selected = self.select_features(data['X_test_raw'], data['y_test']) if self.feature_selector else data['X_test_raw']
            
            # ç‰¹å¾æ ‡å‡†åŒ–
            data['X_train'] = self.scale_features(X_train_selected, fit=True)
            data['X_val'] = self.scale_features(X_val_selected, fit=False)
            data['X_test'] = self.scale_features(X_test_selected, fit=False)
            
            # è®°å½•æ•°æ®ä¿¡æ¯
            for split in ['train', 'val', 'test']:
                logger.info(f"  ğŸ“Š {split}: X={data[f'X_{split}'].shape}, y={data[f'y_{split}'].shape}")
            
            logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def create_data_loaders(self, data: Dict) -> Dict:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        try:
            logger.info("ğŸ”§ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
            
            loaders = {}
            batch_size = self.config['batch_size']
            
            for split in ['train', 'val', 'test']:
                X = torch.FloatTensor(data[f'X_{split}']).to(self.device)
                y = torch.FloatTensor(data[f'y_{split}']).to(self.device)
                
                dataset = TensorDataset(X, y)
                loader = DataLoader(
                    dataset, 
                    batch_size=batch_size, 
                    shuffle=(split == 'train'),
                    drop_last=False  # ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
                )
                
                loaders[split] = loader
                logger.info(f"  ğŸ“Š {split}: {len(loader)} æ‰¹æ¬¡")
            
            logger.info("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
            return loaders
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
            raise
    
    def train_with_regularization(self, data_loaders: Dict) -> Dict:
        """å¸¦æ­£åˆ™åŒ–çš„è®­ç»ƒ"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ­£åˆ™åŒ–è®­ç»ƒ...")
            
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªæ„å»º")
            
            # è®­ç»ƒå‚æ•°
            epochs = self.config['epochs']
            patience = self.config['patience']
            min_delta = self.config['min_delta']
            
            # æ—©åœå˜é‡
            best_val_loss = float('inf')
            patience_counter = 0
            
            # è®­ç»ƒå¾ªç¯
            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                self.model.train()
                train_loss = 0.0
                train_mae = 0.0
                train_batches = 0
                
                for batch_X, batch_y in data_loaders['train']:
                    self.optimizer.zero_grad()
                    
                    # å‰å‘ä¼ æ’­
                    outputs = self.model(batch_X)
                    loss = self.criterion(outputs.squeeze(), batch_y)
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    self.optimizer.step()
                    
                    # ç»Ÿè®¡
                    train_loss += loss.item()
                    train_mae += mean_absolute_error(
                        batch_y.cpu().numpy(), 
                        outputs.detach().cpu().numpy().squeeze()
                    )
                    train_batches += 1
                
                # éªŒè¯é˜¶æ®µ
                self.model.eval()
                val_loss = 0.0
                val_mae = 0.0
                val_batches = 0
                
                with torch.no_grad():
                    for batch_X, batch_y in data_loaders['val']:
                        outputs = self.model(batch_X)
                        loss = self.criterion(outputs.squeeze(), batch_y)
                        
                        val_loss += loss.item()
                        val_mae += mean_absolute_error(
                            batch_y.cpu().numpy(), 
                            outputs.cpu().numpy().squeeze()
                        )
                        val_batches += 1
                
                # è®¡ç®—å¹³å‡æŸå¤±
                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0
                avg_val_loss = val_loss / val_batches if val_batches > 0 else 0
                avg_train_mae = train_mae / train_batches if train_batches > 0 else 0
                avg_val_mae = val_mae / val_batches if val_batches > 0 else 0
                
                # æ›´æ–°å­¦ä¹ ç‡
                old_lr = self.optimizer.param_groups[0]['lr']
                self.scheduler.step(avg_val_loss)
                new_lr = self.optimizer.param_groups[0]['lr']
                
                # è®°å½•å†å²
                self.training_history['train_loss'].append(avg_train_loss)
                self.training_history['val_loss'].append(avg_val_loss)
                self.training_history['train_mae'].append(avg_train_mae)
                self.training_history['val_mae'].append(avg_val_mae)
                self.training_history['learning_rates'].append(new_lr)
                
                # æ‰“å°è¿›åº¦
                if (epoch + 1) % 5 == 0:
                    lr_change = " (LRé™ä½)" if new_lr < old_lr else ""
                    logger.info(f"Epoch {epoch+1}/{epochs}: "
                              f"Train Loss: {avg_train_loss:.6f}, "
                              f"Val Loss: {avg_val_loss:.6f}, "
                              f"Train MAE: {avg_train_mae:.6f}, "
                              f"Val MAE: {avg_val_mae:.6f}, "
                              f"LR: {new_lr:.6f}{lr_change}")
                
                # æ—©åœæ£€æŸ¥
                if avg_val_loss < best_val_loss - min_delta:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    self.save_model('best_simple_model.pth')
                else:
                    patience_counter += 1
                
                # æ—©åœ
                if patience_counter >= patience:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                    break
            
            logger.info("âœ… æ­£åˆ™åŒ–è®­ç»ƒå®Œæˆ")
            
            return {
                'status': 'success',
                'epochs_trained': epoch + 1,
                'best_val_loss': best_val_loss,
                'final_train_loss': avg_train_loss,
                'final_val_loss': avg_val_loss,
                'final_lr': new_lr
            }
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def evaluate_model(self, data_loaders: Dict) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
            
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªæ„å»º")
            
            self.model.eval()
            results = {}
            
            with torch.no_grad():
                for split in ['train', 'val', 'test']:
                    all_predictions = []
                    all_targets = []
                    
                    for batch_X, batch_y in data_loaders[split]:
                        outputs = self.model(batch_X)
                        predictions = outputs.cpu().numpy().squeeze()
                        targets = batch_y.cpu().numpy()
                        
                        # å¤„ç†å•ä¸ªæ ·æœ¬çš„æƒ…å†µ
                        if predictions.ndim == 0:
                            predictions = np.array([predictions])
                        if targets.ndim == 0:
                            targets = np.array([targets])
                        
                        all_predictions.extend(predictions)
                        all_targets.extend(targets)
                    
                    # è®¡ç®—æŒ‡æ ‡
                    if len(all_predictions) > 0 and len(all_targets) > 0:
                        mse = mean_squared_error(all_targets, all_predictions)
                        mae = mean_absolute_error(all_targets, all_predictions)
                        r2 = r2_score(all_targets, all_predictions)
                        
                        results[split] = {
                            'mse': mse,
                            'mae': mae,
                            'r2': r2,
                            'rmse': np.sqrt(mse),
                            'predictions': all_predictions,
                            'targets': all_targets,
                            'n_samples': len(all_predictions)
                        }
                        
                        logger.info(f"  ğŸ“Š {split}: MSE={mse:.6f}, MAE={mae:.6f}, RÂ²={r2:.6f}, n={len(all_predictions)}")
                    else:
                        logger.warning(f"  âš ï¸ {split}: æ— æœ‰æ•ˆé¢„æµ‹ç»“æœ")
            
            logger.info("âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def save_model(self, filename: str) -> None:
        """ä¿å­˜æ¨¡å‹"""
        try:
            os.makedirs(self.model_dir, exist_ok=True)
            model_path = os.path.join(self.model_dir, filename)
            
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config,
                'training_history': self.training_history,
                'feature_selector': self.feature_selector,
                'scaler': self.scaler
            }, model_path)
            
            logger.info(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆ: {model_path}")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def plot_training_analysis(self, save_path: str = None) -> None:
        """ç»˜åˆ¶è®­ç»ƒåˆ†æå›¾"""
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            
            # æŸå¤±æ›²çº¿
            ax1.plot(self.training_history['train_loss'], label='Train Loss', linewidth=2)
            ax1.plot(self.training_history['val_loss'], label='Validation Loss', linewidth=2)
            ax1.set_title('Training and Validation Loss')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # MAEæ›²çº¿
            ax2.plot(self.training_history['train_mae'], label='Train MAE', linewidth=2)
            ax2.plot(self.training_history['val_mae'], label='Validation MAE', linewidth=2)
            ax2.set_title('Training and Validation MAE')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('MAE')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # å­¦ä¹ ç‡å˜åŒ–
            ax3.plot(self.training_history['learning_rates'], label='Learning Rate', linewidth=2, color='orange')
            ax3.set_title('Learning Rate Schedule')
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('Learning Rate')
            ax3.set_yscale('log')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # è®­ç»ƒ/éªŒè¯æŸå¤±æ¯”ç‡
            if len(self.training_history['train_loss']) > 0 and len(self.training_history['val_loss']) > 0:
                ratios = [v/t if t > 0 else 1 for t, v in zip(self.training_history['train_loss'], self.training_history['val_loss'])]
                ax4.plot(ratios, label='Val Loss / Train Loss', linewidth=2, color='red')
                ax4.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Perfect Ratio')
                ax4.set_title('Overfitting Indicator')
                ax4.set_xlabel('Epoch')
                ax4.set_ylabel('Loss Ratio')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f"âœ… è®­ç»ƒåˆ†æå›¾ä¿å­˜: {save_path}")
            
            plt.show()
            
        except Exception as e:
            logger.error(f"âŒ ç»˜åˆ¶è®­ç»ƒåˆ†æå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ ERA5ç®€åŒ–åœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹ - é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬")
    print("=" * 60)
    
    try:
        # åˆ›å»ºç®€åŒ–é¢„æµ‹å™¨
        predictor = ERA5SimplePredictor()
        
        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        print("\nğŸ“¥ åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®...")
        data = predictor.load_and_preprocess_data()
        
        # æ„å»ºæ¨¡å‹
        print("\nğŸ”§ æ„å»ºç®€åŒ–æ¨¡å‹...")
        input_size = data['X_train'].shape[2]
        predictor.build_model(input_size)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\nğŸ”§ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        data_loaders = predictor.create_data_loaders(data)
        
        # æ­£åˆ™åŒ–è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹æ­£åˆ™åŒ–è®­ç»ƒ...")
        training_result = predictor.train_with_regularization(data_loaders)
        
        if training_result['status'] == 'success':
            print(f"âœ… è®­ç»ƒå®Œæˆ!")
            print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {training_result['epochs_trained']}")
            print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {training_result['best_val_loss']:.6f}")
            print(f"ğŸ“Š æœ€ç»ˆå­¦ä¹ ç‡: {training_result['final_lr']:.6f}")
            
            # è¯„ä¼°æ¨¡å‹
            print("\nğŸ“Š è¯„ä¼°æ¨¡å‹...")
            evaluation_results = predictor.evaluate_model(data_loaders)
            
            # ä¿å­˜æ¨¡å‹
            print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
            predictor.save_model('era5_simple_final.pth')
            
            # ç»˜åˆ¶è®­ç»ƒåˆ†æ
            print("\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒåˆ†æ...")
            plot_path = os.path.join(predictor.model_dir, 'training_analysis.png')
            predictor.plot_training_analysis(plot_path)
            
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {training_result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
