#!/usr/bin/env python3
"""
æ°”å€™å˜åŒ–å½±å“SWEåˆ†ææ¨¡å—
å®ç°Mann-Kendallè¶‹åŠ¿æ£€éªŒã€Theil-Senæ–œç‡ä¼°è®¡ã€30å¹´åŸºå‡†æœŸå¼‚å¸¸è®¡ç®—ç­‰åŠŸèƒ½
"""

import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import linregress
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class ClimateChangeAnalyzer:
    """æ°”å€™å˜åŒ–å½±å“SWEåˆ†æå™¨"""
    
    def __init__(self, data_path=None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data = None
        self.baseline_period = (1991, 2020)  # 30å¹´åŸºå‡†æœŸ
        self.analysis_results = {}
        
        if data_path:
            self.load_data(data_path)
    
    def load_data(self, data_path):
        """åŠ è½½æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {data_path}")
        
        try:
            self.data = pd.read_csv(data_path, parse_dates=['date'])
            self.data.set_index('date', inplace=True)
            
            # ç¡®ä¿æœ‰SWEåˆ—
            if 'snow_water_equivalent_mm' not in self.data.columns:
                if 'Snow on Grnd (mm)' in self.data.columns:
                    self.data['snow_water_equivalent_mm'] = self.data['Snow on Grnd (mm)']
                else:
                    print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°SWEæ•°æ®åˆ—")
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.data)} æ¡è®°å½•")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.data.index.min()} åˆ° {self.data.index.max()}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def check_data_homogeneity(self, column='snow_water_equivalent_mm'):
        """
        æ£€æŸ¥æ•°æ®åŒè´¨åŒ–
        æ£€æµ‹è§‚æµ‹æ–¹æ³•å˜åŒ–ã€ç«™ç‚¹è¿ç§»ç­‰å¯¼è‡´çš„éå‡è´¨æ€§
        
        Args:
            column (str): è¦æ£€æŸ¥çš„åˆ—å
            
        Returns:
            dict: åŒè´¨åŒ–æ£€æµ‹ç»“æœ
        """
        print(f"ğŸ” æ£€æŸ¥æ•°æ®åŒè´¨åŒ–: {column}")
        
        if column not in self.data.columns:
            print(f"âŒ åˆ— {column} ä¸å­˜åœ¨")
            return {}
        
        series = self.data[column].dropna()
        
        # 1. åŒç´¯ç§¯æ›²çº¿åˆ†æ
        cumulative = series.cumsum()
        time_index = np.arange(len(cumulative))
        
        # åˆ†æ®µçº¿æ€§å›å½’æ£€æµ‹æ–­ç‚¹
        breakpoints = self._detect_breakpoints(cumulative, time_index)
        
        # 2. æ ‡å‡†å·®å˜åŒ–æ£€æµ‹
        rolling_std = series.rolling(window=365, min_periods=30).std()
        std_changes = self._detect_std_changes(rolling_std)
        
        # 3. å‡å€¼åç§»æ£€æµ‹
        mean_shifts = self._detect_mean_shifts(series)
        
        results = {
            'breakpoints': breakpoints,
            'std_changes': std_changes,
            'mean_shifts': mean_shifts,
            'homogeneity_score': self._calculate_homogeneity_score(breakpoints, std_changes, mean_shifts)
        }
        
        print(f"ğŸ“Š åŒè´¨åŒ–æ£€æµ‹ç»“æœ:")
        print(f"  æ–­ç‚¹æ•°é‡: {len(breakpoints)}")
        print(f"  æ ‡å‡†å·®å˜åŒ–: {len(std_changes)}")
        print(f"  å‡å€¼åç§»: {len(mean_shifts)}")
        print(f"  åŒè´¨åŒ–è¯„åˆ†: {results['homogeneity_score']:.2f}/100")
        
        return results
    
    def _detect_breakpoints(self, cumulative, time_index, min_segment=30):
        """æ£€æµ‹æ–­ç‚¹"""
        breakpoints = []
        
        if len(cumulative) < min_segment * 2:
            return breakpoints
        
        # ä½¿ç”¨åˆ†æ®µçº¿æ€§å›å½’æ£€æµ‹æ–­ç‚¹
        for i in range(min_segment, len(cumulative) - min_segment):
            # åˆ†æ®µ1
            x1 = time_index[:i]
            y1 = cumulative[:i]
            slope1, _, r1, _, _ = linregress(x1, y1)
            
            # åˆ†æ®µ2
            x2 = time_index[i:]
            y2 = cumulative[i:]
            slope2, _, r2, _, _ = linregress(x2, y2)
            
            # å¦‚æœæ–œç‡å·®å¼‚æ˜¾è‘—ï¼Œè®¤ä¸ºæ˜¯æ–­ç‚¹
            if abs(slope1 - slope2) > 0.1 and r1 > 0.8 and r2 > 0.8:
                breakpoints.append(i)
        
        return breakpoints
    
    def _detect_std_changes(self, rolling_std, threshold=2.0):
        """æ£€æµ‹æ ‡å‡†å·®å˜åŒ–"""
        mean_std = rolling_std.mean()
        std_changes = []
        
        for i, std_val in enumerate(rolling_std):
            if pd.notna(std_val) and abs(std_val - mean_std) > threshold * rolling_std.std():
                std_changes.append(i)
        
        return std_changes
    
    def _detect_mean_shifts(self, series, window=365, threshold=2.0):
        """æ£€æµ‹å‡å€¼åç§»"""
        rolling_mean = series.rolling(window=window, min_periods=30).mean()
        overall_mean = series.mean()
        mean_shifts = []
        
        for i, mean_val in enumerate(rolling_mean):
            if pd.notna(mean_val) and abs(mean_val - overall_mean) > threshold * series.std():
                mean_shifts.append(i)
        
        return mean_shifts
    
    def _calculate_homogeneity_score(self, breakpoints, std_changes, mean_shifts):
        """è®¡ç®—åŒè´¨åŒ–è¯„åˆ† (0-100)"""
        # åŸºç¡€åˆ†100åˆ†ï¼Œæ¯æ£€æµ‹åˆ°ä¸€ä¸ªé—®é¢˜æ‰£åˆ†
        score = 100
        
        # æ–­ç‚¹æ‰£åˆ†
        score -= len(breakpoints) * 10
        
        # æ ‡å‡†å·®å˜åŒ–æ‰£åˆ†
        score -= len(std_changes) * 5
        
        # å‡å€¼åç§»æ‰£åˆ†
        score -= len(mean_shifts) * 5
        
        return max(0, score)
    
    def calculate_baseline_anomalies(self, column='snow_water_equivalent_mm'):
        """
        è®¡ç®—30å¹´åŸºå‡†æœŸå¼‚å¸¸
        
        Args:
            column (str): è¦åˆ†æçš„åˆ—å
            
        Returns:
            pd.Series: å¼‚å¸¸å€¼åºåˆ—
        """
        print(f"ğŸ“Š è®¡ç®—30å¹´åŸºå‡†æœŸå¼‚å¸¸: {column}")
        
        if column not in self.data.columns:
            print(f"âŒ åˆ— {column} ä¸å­˜åœ¨")
            return None
        
        # ç­›é€‰åŸºå‡†æœŸæ•°æ® (1991-2020)
        baseline_mask = (self.data.index.year >= self.baseline_period[0]) & \
                       (self.data.index.year <= self.baseline_period[1])
        
        baseline_data = self.data.loc[baseline_mask, column]
        
        if len(baseline_data) == 0:
            print(f"âš ï¸ åŸºå‡†æœŸ {self.baseline_period[0]}-{self.baseline_period[1]} æ— æ•°æ®")
            return None
        
        # è®¡ç®—åŸºå‡†æœŸç»Ÿè®¡é‡
        baseline_mean = baseline_data.mean()
        baseline_std = baseline_data.std()
        
        print(f"ğŸ“ˆ åŸºå‡†æœŸç»Ÿè®¡é‡:")
        print(f"  å‡å€¼: {baseline_mean:.2f}")
        print(f"  æ ‡å‡†å·®: {baseline_std:.2f}")
        print(f"  æ ·æœ¬æ•°: {len(baseline_data)}")
        
        # è®¡ç®—å¼‚å¸¸å€¼ (æ ‡å‡†åŒ–å¼‚å¸¸)
        anomalies = (self.data[column] - baseline_mean) / baseline_std
        
        # æ·»åŠ åŸºå‡†æœŸä¿¡æ¯
        self.analysis_results['baseline_stats'] = {
            'mean': baseline_mean,
            'std': baseline_std,
            'period': self.baseline_period,
            'sample_size': len(baseline_data)
        }
        
        return anomalies
    
    def mann_kendall_test(self, series, alpha=0.05):
        """
        Mann-Kendallè¶‹åŠ¿æ£€éªŒ
        
        Args:
            series (pd.Series): æ—¶é—´åºåˆ—æ•°æ®
            alpha (float): æ˜¾è‘—æ€§æ°´å¹³
            
        Returns:
            dict: æ£€éªŒç»“æœ
        """
        print("ğŸ” æ‰§è¡ŒMann-Kendallè¶‹åŠ¿æ£€éªŒ...")
        
        # ç§»é™¤ç¼ºå¤±å€¼
        clean_series = series.dropna()
        
        if len(clean_series) < 10:
            print("âš ï¸ æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿æ£€éªŒ")
            return {}
        
        # è®¡ç®—Mann-Kendallç»Ÿè®¡é‡
        n = len(clean_series)
        s = 0
        
        for i in range(n-1):
            for j in range(i+1, n):
                if clean_series.iloc[j] > clean_series.iloc[i]:
                    s += 1
                elif clean_series.iloc[j] < clean_series.iloc[i]:
                    s -= 1
        
        # è®¡ç®—æ–¹å·®
        var_s = n * (n - 1) * (2 * n + 5) / 18
        
        # è®¡ç®—Zç»Ÿè®¡é‡
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0
        
        # è®¡ç®—på€¼
        p_value = 2 * (1 - stats.norm.cdf(abs(z)))
        
        # åˆ¤æ–­è¶‹åŠ¿
        if p_value < alpha:
            if z > 0:
                trend = "ä¸Šå‡"
            else:
                trend = "ä¸‹é™"
            significant = True
        else:
            trend = "æ— æ˜¾è‘—è¶‹åŠ¿"
            significant = False
        
        results = {
            's_statistic': s,
            'z_statistic': z,
            'p_value': p_value,
            'significant': significant,
            'trend': trend,
            'alpha': alpha,
            'sample_size': n
        }
        
        print(f"ğŸ“Š Mann-Kendallæ£€éªŒç»“æœ:")
        print(f"  Sç»Ÿè®¡é‡: {s}")
        print(f"  Zç»Ÿè®¡é‡: {z:.4f}")
        print(f"  På€¼: {p_value:.4f}")
        print(f"  æ˜¾è‘—æ€§: {'æ˜¯' if significant else 'å¦'}")
        print(f"  è¶‹åŠ¿: {trend}")
        
        return results
    
    def theil_sen_slope(self, series):
        """
        Theil-Senç¨³å¥æ–œç‡ä¼°è®¡
        
        Args:
            series (pd.Series): æ—¶é—´åºåˆ—æ•°æ®
            
        Returns:
            dict: æ–œç‡ä¼°è®¡ç»“æœ
        """
        print("ğŸ“ˆ è®¡ç®—Theil-Senç¨³å¥æ–œç‡...")
        
        # ç§»é™¤ç¼ºå¤±å€¼
        clean_series = series.dropna()
        
        if len(clean_series) < 10:
            print("âš ï¸ æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è®¡ç®—æ–œç‡")
            return {}
        
        # åˆ›å»ºæ—¶é—´ç´¢å¼•
        time_index = np.arange(len(clean_series))
        
        # è®¡ç®—æ‰€æœ‰ç‚¹å¯¹çš„æ–œç‡
        slopes = []
        for i in range(len(clean_series)):
            for j in range(i+1, len(clean_series)):
                if time_index[j] != time_index[i]:
                    slope = (clean_series.iloc[j] - clean_series.iloc[i]) / (time_index[j] - time_index[i])
                    slopes.append(slope)
        
        if not slopes:
            print("âš ï¸ æ— æ³•è®¡ç®—æ–œç‡")
            return {}
        
        # è®¡ç®—ä¸­ä½æ•°æ–œç‡
        median_slope = np.median(slopes)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´ (ä½¿ç”¨ç™¾åˆ†ä½æ•°)
        slopes_sorted = np.sort(slopes)
        n = len(slopes_sorted)
        
        # 95%ç½®ä¿¡åŒºé—´
        lower_idx = int(0.025 * n)
        upper_idx = int(0.975 * n)
        
        ci_lower = slopes_sorted[lower_idx]
        ci_upper = slopes_sorted[upper_idx]
        
        results = {
            'median_slope': median_slope,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'confidence_level': 0.95,
            'sample_size': len(clean_series),
            'slope_pairs': len(slopes)
        }
        
        print(f"ğŸ“Š Theil-Senæ–œç‡ä¼°è®¡ç»“æœ:")
        print(f"  ä¸­ä½æ•°æ–œç‡: {median_slope:.6f}")
        print(f"  95%ç½®ä¿¡åŒºé—´: [{ci_lower:.6f}, {ci_upper:.6f}]")
        print(f"  æ ·æœ¬æ•°: {len(clean_series)}")
        
        return results
    
    def analyze_climate_change_impacts(self, column='snow_water_equivalent_mm'):
        """
        ç»¼åˆåˆ†ææ°”å€™å˜åŒ–å¯¹SWEçš„å½±å“
        
        Args:
            column (str): è¦åˆ†æçš„åˆ—å
            
        Returns:
            dict: ç»¼åˆåˆ†æç»“æœ
        """
        print(f"\nğŸŒ ç»¼åˆåˆ†ææ°”å€™å˜åŒ–å¯¹SWEçš„å½±å“: {column}")
        print("=" * 60)
        
        if column not in self.data.columns:
            print(f"âŒ åˆ— {column} ä¸å­˜åœ¨")
            return {}
        
        # 1. æ•°æ®åŒè´¨åŒ–æ£€æŸ¥
        homogeneity_results = self.check_data_homogeneity(column)
        
        # 2. è®¡ç®—åŸºå‡†æœŸå¼‚å¸¸
        anomalies = self.calculate_baseline_anomalies(column)
        
        # 3. Mann-Kendallè¶‹åŠ¿æ£€éªŒ
        mk_results = self.mann_kendall_test(self.data[column])
        
        # 4. Theil-Senæ–œç‡ä¼°è®¡
        ts_results = self.theil_sen_slope(self.data[column])
        
        # 5. å¹´é™…å˜åŒ–åˆ†æ
        annual_stats = self._analyze_annual_variations(column)
        
        # ç»¼åˆç»“æœ
        comprehensive_results = {
            'homogeneity': homogeneity_results,
            'baseline_anomalies': anomalies,
            'mann_kendall': mk_results,
            'theil_sen': ts_results,
            'annual_variations': annual_stats,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        self.analysis_results = comprehensive_results
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self._generate_analysis_report(comprehensive_results)
        
        return comprehensive_results
    
    def _analyze_annual_variations(self, column):
        """åˆ†æå¹´é™…å˜åŒ–"""
        print("ğŸ“Š åˆ†æå¹´é™…å˜åŒ–...")
        
        # æŒ‰å¹´åˆ†ç»„è®¡ç®—ç»Ÿè®¡é‡
        annual_data = self.data[column].groupby(self.data.index.year).agg([
            'mean', 'std', 'min', 'max', 'count'
        ]).dropna()
        
        # è®¡ç®—å¹´é™…å˜åŒ–ç‡
        annual_means = annual_data['mean']
        if len(annual_means) > 1:
            # çº¿æ€§è¶‹åŠ¿
            years = np.array(annual_means.index)
            means = annual_means.values
            
            slope, intercept, r_value, p_value, std_err = linregress(years, means)
            
            annual_trend = {
                'slope': slope,
                'intercept': intercept,
                'r_squared': r_value**2,
                'p_value': p_value,
                'trend_per_decade': slope * 10  # æ¯10å¹´çš„å˜åŒ–
            }
        else:
            annual_trend = {}
        
        results = {
            'annual_statistics': annual_data,
            'trend_analysis': annual_trend
        }
        
        return results
    
    def _generate_analysis_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ æ°”å€™å˜åŒ–å½±å“SWEåˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        # åŒè´¨åŒ–è¯„åˆ†
        homogeneity_score = results['homogeneity'].get('homogeneity_score', 0)
        print(f"ğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°:")
        print(f"  åŒè´¨åŒ–è¯„åˆ†: {homogeneity_score}/100")
        if homogeneity_score >= 80:
            print("  âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œé€‚åˆæ°”å€™å˜åŒ–åˆ†æ")
        elif homogeneity_score >= 60:
            print("  âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®è°¨æ…ä½¿ç”¨")
        else:
            print("  âŒ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œä¸å»ºè®®ç”¨äºæ°”å€™å˜åŒ–åˆ†æ")
        
        # è¶‹åŠ¿åˆ†æ
        mk_results = results['mann_kendall']
        if mk_results:
            print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æç»“æœ:")
            print(f"  Mann-Kendallè¶‹åŠ¿: {mk_results.get('trend', 'æœªçŸ¥')}")
            print(f"  æ˜¾è‘—æ€§: {'æ˜¯' if mk_results.get('significant', False) else 'å¦'}")
            print(f"  På€¼: {mk_results.get('p_value', 0):.4f}")
        
        # æ–œç‡ä¼°è®¡
        ts_results = results['theil_sen']
        if ts_results:
            print(f"\nğŸ“Š å˜åŒ–ç‡ä¼°è®¡:")
            print(f"  ä¸­ä½æ•°æ–œç‡: {ts_results.get('median_slope', 0):.6f}")
            print(f"  95%ç½®ä¿¡åŒºé—´: [{ts_results.get('ci_lower', 0):.6f}, {ts_results.get('ci_upper', 0):.6f}]")
        
        # å¹´é™…å˜åŒ–
        annual_trend = results['annual_variations'].get('trend_analysis', {})
        if annual_trend:
            trend_per_decade = annual_trend.get('trend_per_decade', 0)
            print(f"\nğŸŒ¡ï¸ å¹´é™…å˜åŒ–è¶‹åŠ¿:")
            print(f"  æ¯10å¹´å˜åŒ–: {trend_per_decade:.4f}")
            print(f"  å†³å®šç³»æ•°: {annual_trend.get('r_squared', 0):.4f}")
        
        print("\n" + "=" * 60)
    
    def plot_analysis_results(self, save_path=None):
        """ç»˜åˆ¶åˆ†æç»“æœå›¾è¡¨"""
        if not self.analysis_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
            return
        
        print("ğŸ“Š ç»˜åˆ¶åˆ†æç»“æœå›¾è¡¨...")
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ°”å€™å˜åŒ–å½±å“SWEåˆ†æç»“æœ', fontsize=16)
        
        # 1. åŸå§‹æ—¶é—´åºåˆ—
        if 'baseline_anomalies' in self.analysis_results:
            anomalies = self.analysis_results['baseline_anomalies']
            if anomalies is not None:
                axes[0, 0].plot(anomalies.index, anomalies.values, alpha=0.7)
                axes[0, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)
                axes[0, 0].set_title('SWEåŸºå‡†æœŸå¼‚å¸¸')
                axes[0, 0].set_ylabel('æ ‡å‡†åŒ–å¼‚å¸¸')
                axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å¹´é™…å˜åŒ–
        if 'annual_variations' in self.analysis_results:
            annual_stats = self.analysis_results['annual_variations'].get('annual_statistics', pd.DataFrame())
            if not annual_stats.empty:
                annual_means = annual_stats['mean']
                axes[0, 1].plot(annual_means.index, annual_means.values, 'o-')
                axes[0, 1].set_title('å¹´é™…SWEå‡å€¼å˜åŒ–')
                axes[0, 1].set_ylabel('SWE (mm)')
                axes[0, 1].grid(True, alpha=0.3)
        
        # 3. è¶‹åŠ¿çº¿
        if 'theil_sen' in self.analysis_results:
            ts_results = self.analysis_results['theil_sen']
            if ts_results:
                # è¿™é‡Œå¯ä»¥æ·»åŠ è¶‹åŠ¿çº¿ç»˜åˆ¶
                axes[1, 0].set_title('è¶‹åŠ¿åˆ†æ')
                axes[1, 0].text(0.1, 0.5, f"æ–œç‡: {ts_results.get('median_slope', 0):.6f}", 
                               transform=axes[1, 0].transAxes, fontsize=12)
        
        # 4. åŒè´¨åŒ–æ£€æµ‹
        if 'homogeneity' in self.analysis_results:
            homogeneity_score = self.analysis_results['homogeneity'].get('homogeneity_score', 0)
            axes[1, 1].bar(['åŒè´¨åŒ–è¯„åˆ†'], [homogeneity_score], color='skyblue')
            axes[1, 1].set_ylim(0, 100)
            axes[1, 1].set_title('æ•°æ®åŒè´¨åŒ–è¯„åˆ†')
            axes[1, 1].set_ylabel('è¯„åˆ†')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨ä¿å­˜åˆ°: {save_path}")
        
        plt.show()

def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸš€ æ°”å€™å˜åŒ–å½±å“SWEåˆ†ææ¨¡å—")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ClimateChangeAnalyzer()
    
    # åŠ è½½æ•°æ®
    data_path = "src/neuralhydrology/data/red_river_basin/timeseries.csv"
    analyzer.load_data(data_path)
    
    if analyzer.data is not None:
        # æ‰§è¡Œç»¼åˆåˆ†æ
        results = analyzer.analyze_climate_change_impacts('snow_water_equivalent_mm')
        
        # ç»˜åˆ¶ç»“æœ
        analyzer.plot_analysis_results()
        
        print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
