#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„PyTorch LSTMæ¨¡å‹è¿›è¡Œé¢„æµ‹
"""

import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from pathlib import Path
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.models.train_pytorch import LSTMRegressor

def load_trained_model(model_path):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"Loading model from: {model_path}")
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€å’Œå‚æ•°ï¼Œè®¾ç½®weights_only=Falseä»¥æ”¯æŒsklearnå¯¹è±¡
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    model_params = checkpoint['model_params']
    scaler = checkpoint['scaler']
    
    # åˆ›å»ºæ¨¡å‹
    model = LSTMRegressor(
        input_size=model_params['input_size'],
        hidden_size=model_params['hidden_size'],
        num_layers=model_params['num_layers'],
        dropout=model_params['dropout']
    )
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print("Model loaded successfully")
    print(f"Model parameters: {model_params}")
    
    return model, scaler, model_params

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    print("Creating test data...")
    
    # åˆ›å»ºæµ‹è¯•æ—¶é—´åºåˆ—ï¼ˆ1999å¹´çš„æ•°æ®ï¼‰
    dates = pd.date_range('1999-01-01', '1999-12-31', freq='D')
    
    # åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    np.random.seed(123)  # ä¸åŒçš„éšæœºç§å­
    
    # ç§¯é›ªæ·±åº¦ï¼šå­£èŠ‚æ€§å˜åŒ– + éšæœºå™ªå£°
    seasonal_snow = 100 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25) + 50
    snow_depth = np.maximum(0, seasonal_snow + np.random.normal(0, 20, len(dates)))
    
    # é™é›ªé‡ï¼šå†¬å­£è¾ƒé«˜
    snow_fall = np.where(dates.month.isin([12, 1, 2, 3]), 
                         np.random.exponential(10, len(dates)), 
                         np.random.exponential(2, len(dates)))
    
    # é›ªæ°´å½“é‡ï¼šç§¯é›ªæ·±åº¦çš„30%
    snow_water_equivalent = snow_depth * 0.3
    
    # åˆ›å»ºDataFrame
    test_data = pd.DataFrame({
        'date': dates,
        'snow_depth_mm': snow_depth,
        'snow_fall_mm': snow_fall,
        'snow_water_equivalent_mm': snow_water_equivalent,
        'day_of_year': dates.dayofyear,
        'month': dates.month,
        'year': dates.year
    })
    
    print(f"Created test data: {len(test_data)} records")
    print(f"Date range: {test_data['date'].min()} to {test_data['date'].max()}")
    
    return test_data

def prepare_input_data(test_data, scaler, sequence_length):
    """å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®"""
    print("Preparing input data...")
    
    # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆä¸åŒ…æ‹¬ç›®æ ‡å˜é‡ï¼‰
    feature_columns = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                      'day_of_year', 'month', 'year']
    
    # æå–ç‰¹å¾
    features = test_data[feature_columns].values
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    # æ³¨æ„ï¼šscaleræ˜¯åœ¨è®­ç»ƒæ—¶ç”¨7ä¸ªç‰¹å¾è®­ç»ƒçš„ï¼Œä½†é¢„æµ‹æ—¶æˆ‘ä»¬åªæœ‰6ä¸ªç‰¹å¾
    # æˆ‘ä»¬éœ€è¦åªä½¿ç”¨scalerçš„å‰6ä¸ªç‰¹å¾çš„æ ‡å‡†åŒ–å‚æ•°
    features_scaled = features.copy()
    for i in range(features.shape[1]):
        features_scaled[:, i] = (features[:, i] - scaler.mean_[i]) / scaler.scale_[i]
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    sequences = []
    for i in range(len(features_scaled) - sequence_length + 1):
        sequence = features_scaled[i:i + sequence_length]
        sequences.append(sequence)
    
    sequences = np.array(sequences)
    
    print(f"Created {len(sequences)} sequences")
    print(f"Sequence shape: {sequences.shape}")
    
    return sequences

def predict_streamflow(model, input_sequences, scaler):
    """ä½¿ç”¨æ¨¡å‹é¢„æµ‹å¾„æµ"""
    print("Making predictions...")
    
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for sequence in input_sequences:
            # è½¬æ¢ä¸ºtensor
            x = torch.FloatTensor(sequence).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            # é¢„æµ‹
            output = model(x)
            prediction = output.item()
            
            # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®scalerçš„å…·ä½“å®ç°æ¥è°ƒæ•´
            # å‡è®¾scaleræ˜¯StandardScalerï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åæ ‡å‡†åŒ–
            prediction_denorm = prediction * scaler.scale_[-1] + scaler.mean_[-1]
            
            predictions.append(prediction_denorm)
    
    return np.array(predictions)

def evaluate_predictions(predictions, test_data, sequence_length):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    print("Evaluating predictions...")
    
    # è·å–å¯¹åº”çš„å®é™…æ—¥æœŸ
    prediction_dates = test_data['date'].iloc[sequence_length-1:].values
    
    # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
    results_df = pd.DataFrame({
        'date': prediction_dates,
        'predicted_streamflow_m3s': predictions
    })
    
    # æ·»åŠ è¾“å…¥ç‰¹å¾
    for col in ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm']:
        results_df[col] = test_data[col].iloc[sequence_length-1:].values
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š Prediction Statistics:")
    print(f"  Number of predictions: {len(predictions)}")
    print(f"  Mean predicted streamflow: {predictions.mean():.2f} mÂ³/s")
    print(f"  Min predicted streamflow: {predictions.min():.2f} mÂ³/s")
    print(f"  Max predicted streamflow: {predictions.max():.2f} mÂ³/s")
    print(f"  Std predicted streamflow: {predictions.std():.2f} mÂ³/s")
    
    return results_df

def plot_results(results_df):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    print("Creating visualization...")
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # ç»˜åˆ¶ç§¯é›ªæ·±åº¦
    ax1.plot(results_df['date'], results_df['snow_depth_mm'], 'b-', alpha=0.7, label='Snow Depth')
    ax1.set_ylabel('Snow Depth (mm)')
    ax1.set_title('Snow Depth and Predicted Streamflow')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶é¢„æµ‹çš„å¾„æµ
    ax2.plot(results_df['date'], results_df['predicted_streamflow_m3s'], 'r-', alpha=0.7, label='Predicted Streamflow')
    ax2.set_ylabel('Streamflow (mÂ³/s)')
    ax2.set_xlabel('Date')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    output_dir = Path("models/pytorch_lstm")
    plot_path = output_dir / "prediction_results.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Plot saved to: {plot_path}")

def save_predictions(results_df):
    """ä¿å­˜é¢„æµ‹ç»“æœ"""
    print("Saving predictions...")
    
    output_dir = Path("models/pytorch_lstm")
    predictions_path = output_dir / "predictions_1999.csv"
    
    results_df.to_csv(predictions_path, index=False)
    print(f"Predictions saved to: {predictions_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä½¿ç”¨è®­ç»ƒå¥½çš„LSTMæ¨¡å‹è¿›è¡Œé¢„æµ‹")
    print("=" * 50)
    
    try:
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        model_path = "models/pytorch_lstm/snow_runoff_lstm.pth"
        if not os.path.exists(model_path):
            print(f"âŒ Model not found: {model_path}")
            print("Please train the model first using train_pytorch.py")
            return
        
        model, scaler, model_params = load_trained_model(model_path)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data()
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_sequences = prepare_input_data(test_data, scaler, model_params['sequence_length'])
        
        # è¿›è¡Œé¢„æµ‹
        predictions = predict_streamflow(model, input_sequences, scaler)
        
        # è¯„ä¼°ç»“æœ
        results_df = evaluate_predictions(predictions, test_data, model_params['sequence_length'])
        
        # ç»˜åˆ¶ç»“æœ
        plot_results(results_df)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        save_predictions(results_df)
        
        print("\nğŸ‰ é¢„æµ‹å®Œæˆï¼")
        print("ç»“æœå·²ä¿å­˜åˆ° models/pytorch_lstm/ ç›®å½•")
        
    except Exception as e:
        print(f"\nâŒ é¢„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
