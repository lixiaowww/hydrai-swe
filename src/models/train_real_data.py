#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒLSTMæ¨¡å‹
ä½¿ç”¨å·²ç»é¢„å¤„ç†å¥½çš„çœŸå®ç§¯é›ªå’Œå¾„æµæ•°æ®
"""

import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
from pathlib import Path
import joblib
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

class SnowRunoffDataset(Dataset):
    """ç§¯é›ª-å¾„æµæ•°æ®é›†"""
    
    def __init__(self, data, target_col, sequence_length=30):
        self.data = data.values if isinstance(data, pd.DataFrame) else data
        self.target_col = target_col
        self.sequence_length = sequence_length
        
    def __len__(self):
        return len(self.data) - self.sequence_length
        
    def __getitem__(self, idx):
        # è·å–è¾“å…¥åºåˆ—ï¼ˆä¸åŒ…æ‹¬ç›®æ ‡å˜é‡ï¼‰
        x = self.data[idx:idx + self.sequence_length, :-1]  
        # è·å–ç›®æ ‡å€¼
        y = self.data[idx + self.sequence_length, self.target_col]
        
        return torch.FloatTensor(x), torch.FloatTensor([y])

class LSTMRegressor(nn.Module):
    """LSTMå›å½’æ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):
        super(LSTMRegressor, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = out[:, -1, :]
        
        # å…¨è¿æ¥å±‚
        out = self.dropout(out)
        out = self.fc(out)
        
        return out

def load_real_data():
    """åŠ è½½çœŸå®æ•°æ®"""
    print("Loading real training data...")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_path = "data/processed/ready_for_training/train_data.csv"
    test_path = "data/processed/ready_for_training/test_data.csv"
    
    if not os.path.exists(train_path):
        print(f"âŒ Training data not found: {train_path}")
        return None, None
    
    train_data = pd.read_csv(train_path)
    
    # å¦‚æœæµ‹è¯•æ•°æ®å­˜åœ¨å°±åŠ è½½ï¼Œå¦åˆ™ä»è®­ç»ƒæ•°æ®ä¸­åˆ†å‰²
    if os.path.exists(test_path):
        test_data = pd.read_csv(test_path)
        print(f"âœ… Loaded train data: {len(train_data)} records")
        print(f"âœ… Loaded test data: {len(test_data)} records")
    else:
        # ä»è®­ç»ƒæ•°æ®ä¸­åˆ†å‰²å‡ºæµ‹è¯•é›†
        split_idx = int(0.8 * len(train_data))
        test_data = train_data.iloc[split_idx:].copy()
        train_data = train_data.iloc[:split_idx].copy()
        print(f"âœ… Split data - train: {len(train_data)}, test: {len(test_data)} records")
    
    # æ£€æŸ¥æ•°æ®èŒƒå›´
    print(f"\nğŸ“Š Data Summary:")
    print(f"  Training period: {train_data['date'].min()} to {train_data['date'].max()}")
    print(f"  Test period: {test_data['date'].min()} to {test_data['date'].max()}")
    
    # æ˜¾ç¤ºç‰¹å¾åˆ—
    feature_columns = [col for col in train_data.columns if col not in ['date']]
    print(f"  Features ({len(feature_columns)}): {feature_columns[:5]}...")
    
    return train_data, test_data

def prepare_features_and_target(data):
    """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
    print("Preparing features and target...")
    
    # æ’é™¤éç‰¹å¾åˆ—
    exclude_cols = ['date']
    feature_columns = [col for col in data.columns if col not in exclude_cols]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¾„æµç›¸å…³çš„ç›®æ ‡å˜é‡
    # å¯»æ‰¾å¯èƒ½çš„ç›®æ ‡å˜é‡
    target_candidates = ['streamflow_m3s', 'streamflow', 'flow', 'discharge']
    target_col = None
    target_col_idx = None
    
    for candidate in target_candidates:
        if candidate in data.columns:
            target_col = candidate
            target_col_idx = feature_columns.index(candidate)
            break
    
    if target_col is None:
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å¾„æµå˜é‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—ä½œä¸ºç›®æ ‡
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        available_cols = [col for col in numeric_cols if col not in exclude_cols]
        if available_cols:
            target_col = available_cols[0]
            target_col_idx = feature_columns.index(target_col)
            print(f"âš ï¸ No streamflow column found, using '{target_col}' as target")
        else:
            print("âŒ No suitable target column found")
            return None, None, None, None
    
    print(f"âœ… Using '{target_col}' as target variable (index: {target_col_idx})")
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    features = data[feature_columns].values
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    try:
        # é¦–å…ˆç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹
        features_df = pd.DataFrame(features, columns=feature_columns)
        features_numeric = features_df.select_dtypes(include=[np.number])
        
        if len(features_numeric.columns) < len(feature_columns):
            # æœ‰éæ•°å€¼åˆ—ï¼Œéœ€è¦è½¬æ¢æˆ–ç§»é™¤
            print("âš ï¸ Found non-numeric columns, converting to numeric...")
            for col in feature_columns:
                if col not in features_numeric.columns:
                    features_df[col] = pd.to_numeric(features_df[col], errors='coerce')
        
        features = features_df.values
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        if pd.isna(features).any():
            print("âš ï¸ Found NaN values, filling with column means...")
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='mean')
            features = imputer.fit_transform(features)
    except Exception as e:
        print(f"âš ï¸ Error in data preprocessing: {e}")
        # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        features = pd.DataFrame(features).apply(pd.to_numeric, errors='coerce').fillna(0).values
    
    print(f"  Feature matrix shape: {features.shape}")
    print(f"  Target column: {target_col}")
    print(f"  Target range: {features[:, target_col_idx].min():.2f} - {features[:, target_col_idx].max():.2f}")
    
    return features, target_col, target_col_idx, feature_columns

def train_model(train_data, val_data, target_col_idx, model_params):
    """è®­ç»ƒæ¨¡å‹"""
    print("Starting model training...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = SnowRunoffDataset(train_data, target_col_idx, model_params['sequence_length'])
    val_dataset = SnowRunoffDataset(val_data, target_col_idx, model_params['sequence_length'])
    
    train_loader = DataLoader(train_dataset, batch_size=model_params['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=model_params['batch_size'], shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    model = LSTMRegressor(
        input_size=model_params['input_size'],
        hidden_size=model_params['hidden_size'],
        num_layers=model_params['num_layers'],
        dropout=model_params['dropout']
    ).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=model_params['learning_rate'])
    
    # è®­ç»ƒå¾ªç¯
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    for epoch in range(model_params['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                val_predictions.extend(outputs.cpu().numpy())
                val_targets.extend(batch_y.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_losses.append(val_loss)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        val_predictions = np.array(val_predictions).flatten()
        val_targets = np.array(val_targets).flatten()
        val_r2 = r2_score(val_targets, val_predictions)
        val_mae = mean_absolute_error(val_targets, val_predictions)
        
        print(f"Epoch {epoch+1}/{model_params['epochs']}: "
              f"Train Loss: {train_loss:.4f}, "
              f"Val Loss: {val_loss:.4f}, "
              f"Val RÂ²: {val_r2:.4f}, "
              f"Val MAE: {val_mae:.4f}")
        
        # æ—©åœæœºåˆ¶
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_state)
    
    return model, train_losses, val_losses

def save_model_and_results(model, train_losses, val_losses, scaler, model_params, feature_columns, target_col):
    """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
    print("Saving model and results...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("models/real_data_lstm")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / "real_data_lstm.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_params': model_params,
        'scaler': scaler,
        'feature_columns': feature_columns,
        'target_column': target_col
    }, model_path, weights_only=False)
    print(f"Model saved to: {model_path}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'model_params': model_params,
        'feature_columns': feature_columns,
        'target_column': target_col,
        'final_train_loss': train_losses[-1],
        'final_val_loss': val_losses[-1],
        'best_val_loss': min(val_losses)
    }
    
    history_path = output_dir / "training_history.json"
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2, default=str)
    print(f"Training history saved to: {history_path}")
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Real Data Training: Loss vs Epochs')
    plt.legend()
    plt.grid(True)
    
    plot_path = output_dir / "training_loss.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Training plot saved to: {plot_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒLSTMæ¨¡å‹")
    print("=" * 50)
    
    try:
        # åŠ è½½çœŸå®æ•°æ®
        train_data, test_data = load_real_data()
        if train_data is None:
            return
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        train_features, target_col, target_col_idx, feature_columns = prepare_features_and_target(train_data)
        test_features, _, _, _ = prepare_features_and_target(test_data)
        
        if train_features is None:
            return
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        print("Standardizing features...")
        scaler = StandardScaler()
        train_features_scaled = scaler.fit_transform(train_features)
        test_features_scaled = scaler.transform(test_features)
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_split = 0.2
        val_size = int(val_split * len(train_features_scaled))
        val_data = train_features_scaled[-val_size:]
        train_data_final = train_features_scaled[:-val_size]
        
        print(f"Final data split - train: {len(train_data_final)}, val: {len(val_data)}, test: {len(test_features_scaled)}")
        
        # æ¨¡å‹å‚æ•° - ä½¿ç”¨å®é™…çš„ç‰¹å¾æ•°é‡
        actual_input_size = train_features_scaled.shape[1] - 1  # å‡å»ç›®æ ‡å˜é‡
        model_params = {
            'input_size': actual_input_size,
            'hidden_size': 128,  # å¢åŠ éšè—å•å…ƒæ•°é‡
            'num_layers': 3,     # å¢åŠ å±‚æ•°
            'dropout': 0.3,
            'sequence_length': 30,
            'batch_size': 64,    # å¢åŠ æ‰¹æ¬¡å¤§å°
            'epochs': 100,       # å¢åŠ è®­ç»ƒè½®æ•°
            'learning_rate': 0.001
        }
        
        print("\nModel parameters:")
        for key, value in model_params.items():
            print(f"  {key}: {value}")
        
        # è®­ç»ƒæ¨¡å‹
        model, train_losses, val_losses = train_model(train_data_final, val_data, target_col_idx, model_params)
        
        # ä¿å­˜ç»“æœ
        save_model_and_results(model, train_losses, val_losses, scaler, model_params, feature_columns, target_col)
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {min(val_losses):.4f}")
        print("æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ° models/real_data_lstm/ ç›®å½•")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
