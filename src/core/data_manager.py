#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨
æä¾›æ ‡å‡†åŒ–çš„æ•°æ®è®¿é—®æ¥å£ï¼Œè§£å†³è·¯å¾„ç¡¬ç¼–ç å’Œæ•°æ®æ ¼å¼ä¸ç»Ÿä¸€é—®é¢˜
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List, Union
from datetime import datetime, timedelta
import glob
import logging
from pathlib import Path
import hashlib
from functools import lru_cache

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class DataManager:
    """ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, base_path: str = "/home/sean/hydrai_swe"):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            base_path: é¡¹ç›®æ ¹è·¯å¾„
        """
        self.base_path = Path(base_path)
        self.data_path = self.base_path / "data"
        self.processed_path = self.data_path / "processed"
        
        # æ•°æ®æºé…ç½®
        self.data_sources = {
            "swe": {
                "path": self.processed_path / "swe",
                "sync_pattern": "swe_sync_*.csv",
                "static_file": "swe_analysis_optimized.csv"
            },
            "flood": {
                "path": self.processed_path / "flood_warning", 
                "sync_pattern": "flood_sync_*.csv",
                "static_file": "flood_warning_optimized.csv"
            },
            "hydrology": {
                "path": self.processed_path,
                "sync_pattern": "hydro_sync_*.csv", 
                "static_file": "hydat_streamflow_processed.csv"
            },
            "weather": {
                "path": self.processed_path / "weather",
                "sync_pattern": "weather_sync_*.csv",
                "static_file": "weather_data.csv"
            },
            "agriculture": {
                "path": self.processed_path / "agriculture",
                "sync_pattern": "agri_sync_*.csv",
                "static_file": "agriculture_data.csv"
            }
        }
        
        # ç¼“å­˜é…ç½®
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        logger.info(f"æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒåŸºç¡€è·¯å¾„: {self.base_path}")
    
    def _get_cache_key(self, source: str, file_path: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ï¼‰"""
        try:
            file_stat = os.stat(file_path)
            file_mtime = file_stat.st_mtime
            return f"{source}_{file_path}_{file_mtime}"
        except OSError:
            return f"{source}_{file_path}_{datetime.now().timestamp()}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self.cache:
            return False
        
        cache_time = self.cache[cache_key]["timestamp"]
        return (datetime.now() - cache_time).total_seconds() < self.cache_ttl
    
    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = datetime.now()
        expired_keys = []
        
        for key, cache_data in self.cache.items():
            if (current_time - cache_data["timestamp"]).total_seconds() > self.cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            logger.info(f"ğŸ§¹ æ¸…ç†äº†{len(expired_keys)}ä¸ªè¿‡æœŸç¼“å­˜")
    
    def get_latest_data(self, source: str, force_sync: bool = True, use_cache: bool = True) -> pd.DataFrame:
        """
        è·å–æœ€æ–°æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            source: æ•°æ®æºåç§° (swe, flood, hydrology, weather, agriculture)
            force_sync: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨åŒæ­¥æ•°æ®
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            DataFrame: æœ€æ–°æ•°æ®
            
        Raises:
            ValueError: æ•°æ®æºä¸å­˜åœ¨
            FileNotFoundError: æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶
        """
        if source not in self.data_sources:
            raise ValueError(f"æœªçŸ¥æ•°æ®æº: {source}")
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._clean_cache()
        
        config = self.data_sources[source]
        
        # 1. ä¼˜å…ˆä½¿ç”¨åŒæ­¥æ•°æ®
        if force_sync:
            sync_files = list(config["path"].glob(config["sync_pattern"]))
            if sync_files:
                latest_file = max(sync_files, key=os.path.getctime)
                file_path = str(latest_file)
                
                # æ£€æŸ¥ç¼“å­˜
                if use_cache:
                    cache_key = self._get_cache_key(source, file_path)
                    if self._is_cache_valid(cache_key):
                        logger.info(f"ğŸ“¦ ä½¿ç”¨{source}ç¼“å­˜æ•°æ®")
                        return self.cache[cache_key]["data"].copy()
                
                # è¯»å–æ•°æ®å¹¶ç¼“å­˜
                data = pd.read_csv(latest_file)
                logger.info(f"âœ… ä½¿ç”¨{source}åŒæ­¥æ•°æ®: {latest_file}")
                
                if use_cache:
                    cache_key = self._get_cache_key(source, file_path)
                    self.cache[cache_key] = {
                        "data": data.copy(),
                        "timestamp": datetime.now(),
                        "source": file_path
                    }
                
                return data
        
        # 2. å¤‡é€‰é™æ€æ–‡ä»¶
        static_file = config["path"] / config["static_file"]
        if static_file.exists():
            file_path = str(static_file)
            
            # æ£€æŸ¥ç¼“å­˜
            if use_cache:
                cache_key = self._get_cache_key(source, file_path)
                if self._is_cache_valid(cache_key):
                    logger.info(f"ğŸ“¦ ä½¿ç”¨{source}ç¼“å­˜æ•°æ®")
                    return self.cache[cache_key]["data"].copy()
            
            # è¯»å–æ•°æ®å¹¶ç¼“å­˜
            data = pd.read_csv(static_file)
            logger.warning(f"âš ï¸ ä½¿ç”¨{source}é™æ€æ•°æ®: {static_file} (æ•°æ®å¯èƒ½è¿‡æ—¶)")
            
            if use_cache:
                cache_key = self._get_cache_key(source, file_path)
                self.cache[cache_key] = {
                    "data": data.copy(),
                    "timestamp": datetime.now(),
                    "source": file_path
                }
            
            return data
        
        # 3. æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®
        raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°{source}æ•°æ®æ–‡ä»¶")
    
    def get_data_info(self, source: str) -> Dict[str, Any]:
        """
        è·å–æ•°æ®æºä¿¡æ¯
        
        Args:
            source: æ•°æ®æºåç§°
            
        Returns:
            Dict: æ•°æ®æºä¿¡æ¯
        """
        if source not in self.data_sources:
            raise ValueError(f"æœªçŸ¥æ•°æ®æº: {source}")
        
        config = self.data_sources[source]
        
        # æ£€æŸ¥åŒæ­¥æ•°æ®
        sync_files = list(config["path"].glob(config["sync_pattern"]))
        latest_sync = max(sync_files, key=os.path.getctime) if sync_files else None
        
        # æ£€æŸ¥é™æ€æ•°æ®
        static_file = config["path"] / config["static_file"]
        static_exists = static_file.exists()
        
        info = {
            "source": source,
            "path": str(config["path"]),
            "sync_files_count": len(sync_files),
            "latest_sync": str(latest_sync) if latest_sync else None,
            "static_file": str(static_file) if static_exists else None,
            "static_exists": static_exists,
            "data_available": len(sync_files) > 0 or static_exists
        }
        
        if latest_sync:
            info["sync_age_hours"] = (datetime.now() - datetime.fromtimestamp(latest_sync.stat().st_mtime)).total_seconds() / 3600
        
        return info
    
    def validate_data(self, data: pd.DataFrame, required_columns: List[str] = None) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®è´¨é‡
        
        Args:
            data: æ•°æ®DataFrame
            required_columns: å¿…éœ€çš„åˆ—å
            
        Returns:
            Dict: éªŒè¯ç»“æœ
        """
        validation = {
            "valid": True,
            "shape": data.shape,
            "columns": list(data.columns),
            "missing_columns": [],
            "empty_rows": data.isnull().all(axis=1).sum(),
            "duplicate_rows": data.duplicated().sum(),
            "data_types": data.dtypes.to_dict()
        }
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        if required_columns:
            missing = [col for col in required_columns if col not in data.columns]
            validation["missing_columns"] = missing
            validation["valid"] = len(missing) == 0
        
        return validation
    
    def get_all_data_sources_info(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰æ•°æ®æºä¿¡æ¯
        
        Returns:
            Dict: æ‰€æœ‰æ•°æ®æºä¿¡æ¯
        """
        return {source: self.get_data_info(source) for source in self.data_sources.keys()}

# å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹
data_manager = DataManager()

def get_data_manager() -> DataManager:
    """è·å–å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹"""
    return data_manager
