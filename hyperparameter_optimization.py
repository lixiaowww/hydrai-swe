#!/usr/bin/env python3
"""
è¶…å‚æ•°ä¼˜åŒ–è„šæœ¬
ä½¿ç”¨Optunaå¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œæ·±åº¦ä¼˜åŒ–
"""

import optuna
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import os
from datetime import datetime
import time

class OptimizedGRUModel(nn.Module):
    """å¯ä¼˜åŒ–çš„GRUæ¨¡å‹"""
    
    def __init__(self, input_size=6, hidden_size=64, num_layers=2, dropout=0.1, 
                 use_batch_norm=True, use_residual=True, activation='relu'):
        super(OptimizedGRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.use_batch_norm = use_batch_norm
        self.use_residual = use_residual
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_size, hidden_size)
        
        # GRUå±‚
        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, 
                          dropout=dropout if num_layers > 1 else 0, batch_first=True)
        
        # æ‰¹å½’ä¸€åŒ–
        if use_batch_norm:
            self.bn = nn.BatchNorm1d(hidden_size)
        
        # æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'swish':
            self.activation = nn.SiLU()
        elif activation == 'gelu':
            self.activation = nn.GELU()
        else:
            self.activation = nn.ReLU()
        
        # è¾“å‡ºå±‚
        self.output_layers = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            self.activation,
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_size // 2, 1)
        )
        
    def forward(self, x):
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # GRUå¤„ç†
        gru_out, _ = self.gru(x)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_output = gru_out[:, -1, :]
        
        # æ‰¹å½’ä¸€åŒ–
        if self.use_batch_norm:
            last_output = self.bn(last_output)
        
        # æ®‹å·®è¿æ¥
        if self.use_residual and self.num_layers > 1:
            residual = self.input_projection(x[:, -1, :])
            last_output = last_output + residual
        
        # è¾“å‡ºå±‚
        output = self.output_layers(last_output)
        return output

class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, n_trials=100):
        self.n_trials = n_trials
        self.best_trial = None
        self.optimization_history = []
        self.scaler_X = None
        self.scaler_y = None
        self.sequence_length = 30
        
    def load_data_and_scalers(self):
        """åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨"""
        print("ğŸ“Š åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨...")
        
        try:
            # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
            data_path = "data/processed/standardized_training_dataset.csv"
            data = pd.read_csv(data_path, index_col=0, parse_dates=True)
            print(f"âœ… åŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨å‚æ•°
            import pickle
            with open('models/standardization_params.pkl', 'rb') as f:
                params = pickle.load(f)
            
            # é‡å»ºæ ‡å‡†åŒ–å™¨
            self.scaler_X = StandardScaler()
            self.scaler_X.mean_ = params['scaler_X_mean']
            self.scaler_X.scale_ = params['scaler_X_scale']
            
            self.scaler_y = StandardScaler()
            self.scaler_y.mean_ = params['scaler_y_mean']
            self.scaler_y.scale_ = params['scaler_y_scale']
            
            print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            return data
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_sequences(self, data):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡åºåˆ—æ•°æ®...")
        
        feature_cols = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                       'day_of_year', 'month', 'year']
        target_col = 'snow_water_equivalent_mm'
        
        X = data[feature_cols].values
        y = data[target_col].values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.transform(X)
        y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºåºåˆ—
        X_seq, y_seq = [], []
        for i in range(len(X_scaled) - self.sequence_length):
            X_seq.append(X_scaled[i:(i + self.sequence_length)])
            y_seq.append(y_scaled[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"âœ… åºåˆ—æ•°æ®å‡†å¤‡å®Œæˆ: {X_seq.shape}, {y_seq.shape}")
        return X_seq, y_seq
    
    def split_data(self, X, y, train_ratio=0.7, val_ratio=0.15):
        """åˆ†å‰²æ•°æ®"""
        n = len(X)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        X_val = X[train_end:val_end]
        y_val = y[train_end:val_end]
        X_test = X[val_end:]
        y_test = y[val_end:]
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)
    
    def create_data_loaders(self, train_data, val_data, test_data, batch_size):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        X_train, y_train = train_data
        X_val, y_val = val_data
        X_test, y_test = test_data
        
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
        test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def objective(self, trial):
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        # è¶…å‚æ•°é‡‡æ ·
        params = {
            'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 128, 256]),
            'num_layers': trial.suggest_int('num_layers', 1, 4),
            'dropout': trial.suggest_float('dropout', 0.0, 0.5),
            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),
            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),
            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
            'use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),
            'use_residual': trial.suggest_categorical('use_residual', [True, False]),
            'activation': trial.suggest_categorical('activation', ['relu', 'swish', 'gelu']),
            'sequence_length': trial.suggest_categorical('sequence_length', [15, 30, 45, 60])
        }
        
        try:
            # å‡†å¤‡æ•°æ®
            data = self.load_data_and_scalers()
            if data is None:
                return float('inf')
            
            # ä½¿ç”¨æ–°çš„åºåˆ—é•¿åº¦
            self.sequence_length = params['sequence_length']
            X, y = self.prepare_sequences(data)
            
            # åˆ†å‰²æ•°æ®
            train_data, val_data, test_data = self.split_data(X, y)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader, val_loader, test_loader = self.create_data_loaders(
                train_data, val_data, test_data, params['batch_size']
            )
            
            # åˆ›å»ºæ¨¡å‹
            model = OptimizedGRUModel(
                input_size=6,
                hidden_size=params['hidden_size'],
                num_layers=params['num_layers'],
                dropout=params['dropout'],
                use_batch_norm=params['use_batch_norm'],
                use_residual=params['use_residual'],
                activation=params['activation']
            )
            
            # è®­ç»ƒæ¨¡å‹
            val_loss = self.train_and_evaluate(
                model, train_loader, val_loader, params
            )
            
            # è®°å½•ä¼˜åŒ–å†å²
            self.optimization_history.append({
                'trial': trial.number,
                'params': params,
                'val_loss': val_loss
            })
            
            return val_loss
            
        except Exception as e:
            print(f"âŒ è¯•éªŒ {trial.number} å¤±è´¥: {e}")
            return float('inf')
    
    def train_and_evaluate(self, model, train_loader, val_loader, params):
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=params['learning_rate'],
            weight_decay=params['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=10, T_mult=2
        )
        
        # è®­ç»ƒå‚æ•°
        epochs = 30
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 8
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        return best_val_loss
    
    def run_optimization(self):
        """è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        print("ğŸš€ å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
        
        # åˆ›å»ºç ”ç©¶
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner()
        )
        
        # è¿è¡Œä¼˜åŒ–
        study.optimize(self.objective, n_trials=self.n_trials, timeout=3600)
        
        # ä¿å­˜æœ€ä½³ç»“æœ
        self.best_trial = study.best_trial
        
        print(f"\nğŸ† ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_trial.value:.6f}")
        print(f"æœ€ä½³å‚æ•°:")
        for key, value in self.best_trial.params.items():
            print(f"  {key}: {value}")
        
        # ä¿å­˜ä¼˜åŒ–å†å²
        self.save_optimization_results(study)
        
        return study
    
    def save_optimization_results(self, study):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        best_params_path = f"logs/best_hyperparameters_{timestamp}.json"
        os.makedirs(os.path.dirname(best_params_path), exist_ok=True)
        
        import json
        with open(best_params_path, 'w', encoding='utf-8') as f:
            json.dump({
                'best_value': study.best_trial.value,
                'best_params': study.best_trial.params,
                'n_trials': len(study.trials),
                'optimization_time': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æœ€ä½³å‚æ•°å·²ä¿å­˜: {best_params_path}")
        
        # ä¿å­˜ä¼˜åŒ–å†å²
        history_path = f"logs/optimization_history_{timestamp}.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.optimization_history, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ä¼˜åŒ–å†å²å·²ä¿å­˜: {history_path}")
        
        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        self.generate_optimization_report(study, timestamp)
    
    def generate_optimization_report(self, study, timestamp):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        
        report_path = f"logs/hyperparameter_optimization_report_{timestamp}.md"
        
        report_content = f"""# è¶…å‚æ•°ä¼˜åŒ–æŠ¥å‘Š

## ä¼˜åŒ–æ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ä¼˜åŒ–é…ç½®
- **è¯•éªŒæ¬¡æ•°**: {self.n_trials}
- **ä¼˜åŒ–æ–¹å‘**: æœ€å°åŒ–éªŒè¯æŸå¤±
- **é‡‡æ ·å™¨**: TPE (Tree-structured Parzen Estimator)
- **å‰ªæå™¨**: Median Pruner

## æœ€ä½³ç»“æœ
ğŸ† **æœ€ä½³éªŒè¯æŸå¤±**: {study.best_trial.value:.6f}

### æœ€ä½³è¶…å‚æ•°
"""
        
        for key, value in study.best_trial.params.items():
            report_content += f"- **{key}**: {value}\n"
        
        report_content += f"""

## ä¼˜åŒ–ç»Ÿè®¡
- **æ€»è¯•éªŒæ•°**: {len(study.trials)}
- **æˆåŠŸè¯•éªŒæ•°**: {len([t for t in study.trials if t.value != float('inf')])}
- **å¤±è´¥è¯•éªŒæ•°**: {len([t for t in study.trials if t.value == float('inf')])}

## å‚æ•°é‡è¦æ€§åˆ†æ
"""
        
        # è®¡ç®—å‚æ•°é‡è¦æ€§
        try:
            importance = optuna.importance.get_param_importances(study)
            for param, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True):
                report_content += f"- **{param}**: {imp:.4f}\n"
        except:
            report_content += "- æ— æ³•è®¡ç®—å‚æ•°é‡è¦æ€§\n"
        
        report_content += f"""

## ä¼˜åŒ–å»ºè®®
1. **é‡ç‚¹å…³æ³¨**: æ ¹æ®å‚æ•°é‡è¦æ€§ï¼Œä¼˜å…ˆè°ƒæ•´é‡è¦å‚æ•°
2. **è¿›ä¸€æ­¥æ¢ç´¢**: åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œç²¾ç»†æœç´¢
3. **æ¨¡å‹é›†æˆ**: è€ƒè™‘é›†æˆå¤šä¸ªä¼˜ç§€è¯•éªŒçš„ç»“æœ
4. **æ•°æ®å¢å¼º**: ç»“åˆæœ€ä½³è¶…å‚æ•°ï¼Œå°è¯•æ•°æ®å¢å¼ºæŠ€æœ¯

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨
- ä½¿ç”¨æœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒå®Œæ•´æ¨¡å‹
- åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½
- è€ƒè™‘æ¨¡å‹é›†æˆå’Œéƒ¨ç½²ç­–ç•¥
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HydrAI-SWE è¶…å‚æ•°ä¼˜åŒ–")
    print("=" * 60)
    
    try:
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = HyperparameterOptimizer(n_trials=50)  # å‡å°‘è¯•éªŒæ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
        
        # è¿è¡Œä¼˜åŒ–
        study = optimizer.run_optimization()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"âœ… æœ€ä½³éªŒè¯æŸå¤±: {study.best_trial.value:.6f}")
        print("âœ… ä¼˜åŒ–ç»“æœå·²ä¿å­˜")
        print("âœ… ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ")
        
        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        print(f"  1. ä½¿ç”¨æœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹")
        print(f"  2. å°è¯•æ•°æ®å¢å¼ºæŠ€æœ¯")
        print(f"  3. è€ƒè™‘æ¨¡å‹é›†æˆ")
        print(f"  4. è¿›ä¸€æ­¥æ¢ç´¢å‚æ•°ç©ºé—´")
        
    except Exception as e:
        print(f"âŒ è¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
