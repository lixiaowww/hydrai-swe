#!/usr/bin/env python3
"""
NASA SMAPåœŸå£¤æ°´åˆ†æ•°æ®ä¸‹è½½è„šæœ¬
çœ‹é—¨ç‹—å®¡æ ¸é€šè¿‡ - ä½¿ç”¨çœŸå®å‡­æ®ä¸‹è½½æ•°æ®
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import json
import time
from typing import Dict, List, Optional
import logging
from dotenv import load_dotenv
import yaml

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('config/credentials.env')

# NASA Earthdataå‡­æ®
NASA_USERNAME = os.getenv('NASA_EARTHDATA_USERNAME')
NASA_PASSWORD = os.getenv('NASA_EARTHDATA_PASSWORD')

# åœ°ç†é…ç½®
def load_geographic_config():
    """åŠ è½½åœ°ç†é…ç½®"""
    try:
        with open('config/geographic_regions.yml', 'r') as file:
            config = yaml.safe_load(file)
        return config
    except Exception as e:
        logger.error(f"åŠ è½½åœ°ç†é…ç½®å¤±è´¥: {e}")
        return None

def search_smap_data(start_date: str, end_date: str, region_name: str = 'red_river_basin') -> Optional[Dict]:
    """
    æœç´¢NASA SMAPæ•°æ®
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        region_name: åŒºåŸŸåç§°
    
    Returns:
        æœç´¢ç»“æœå­—å…¸
    """
    try:
        # åŠ è½½åœ°ç†é…ç½®
        geo_config = load_geographic_config()
        if not geo_config or region_name not in geo_config:
            logger.error(f"æœªæ‰¾åˆ°åŒºåŸŸé…ç½®: {region_name}")
            return None
        
        region = geo_config[region_name]
        bounding_box = region['bounding_box']
        
        logger.info(f"æœç´¢åŒºåŸŸ: {region['name']}")
        logger.info(f"è¾¹ç•Œæ¡†: {bounding_box}")
        
        # NASA CMRæœç´¢API
        cmr_url = "https://cmr.earthdata.nasa.gov/search/granules.umm_json"
        
        # æœç´¢å‚æ•°
        params = {
            'collection_concept_id': 'C1940468260-POCLOUD',  # SMAP L3åœŸå£¤æ°´åˆ†
            'temporal': f"{start_date}T00:00:00Z,{end_date}T23:59:59Z",
            'bounding_box': f"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}",
            'page_size': 2000,
            'sort_key': 'start_date'
        }
        
        logger.info("ğŸ” æœç´¢SMAPæ•°æ®...")
        response = requests.get(cmr_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        # è°ƒè¯•APIå“åº”
        logger.info(f"APIå“åº”çŠ¶æ€: {response.status_code}")
        logger.info(f"å“åº”å†…å®¹ç±»å‹: {type(data)}")
        logger.info(f"å“åº”é”®: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
        
        # æ£€æŸ¥å“åº”ç»“æ„
        if isinstance(data, dict):
            # æ£€æŸ¥ä¸åŒçš„å“åº”æ ¼å¼
            if 'hits' in data and isinstance(data['hits'], dict):
                # æ ‡å‡†æ ¼å¼
                hits = data['hits']
                granules = hits.get('hits', [])
            elif 'items' in data and isinstance(data['items'], list):
                # æ›¿ä»£æ ¼å¼
                granules = data['items']
            elif 'hits' in data and isinstance(data['hits'], int):
                # hitsæ˜¯è®¡æ•°ï¼Œitemsæ˜¯æ•°æ®
                granules = data.get('items', [])
            else:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é”®
                granules = []
                for key in ['granules', 'results', 'data']:
                    if key in data and isinstance(data[key], list):
                        granules = data[key]
                        break
            
            logger.info(f"Granulesç±»å‹: {type(granules)}")
            logger.info(f"Granulesæ•°é‡: {len(granules) if isinstance(granules, list) else 'Not a list'}")
        else:
            granules = []
        
        logger.info(f"âœ… æ‰¾åˆ° {len(granules)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œå°è¯•ä¸åŒçš„æœç´¢å‚æ•°
        if len(granules) == 0:
            logger.info("ğŸ” å°è¯•ä¸åŒçš„æœç´¢å‚æ•°...")
            
            # å°è¯•ä¸åŒçš„é›†åˆID
            alternative_collections = [
                'C2776463717-NSIDC_ECS',  # SMAP Enhanced L1C Radiometer
                'C2938663435-NSIDC_CPRD',  # SMAP Enhanced L1C Radiometer
                'C3383993430-NSIDC_ECS',   # SMAP L4 Global 3-hourly
                'C1940468260-POCLOUD',     # SMAP L3 (åŸå§‹)
                'C1940468264-POCLOUD',     # SMAP L4
                'C1940468265-POCLOUD',     # SMAP Enhanced L3
            ]
            
            # å°è¯•ä¸åŒçš„æ—¶é—´èŒƒå›´
            time_ranges = [
                f"{start_date}T00:00:00Z,{end_date}T23:59:59Z",
                f"{start_date}T00:00:00Z,{end_date}T00:00:00Z",
                f"{start_date}T00:00:00Z,{end_date}T12:00:00Z"
            ]
            
            # å°è¯•ä¸åŒçš„è¾¹ç•Œæ¡†æ ¼å¼
            bounding_boxes = [
                f"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}",
                f"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}",
                f"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}"
            ]
            
            found_data = False
            for collection_id in alternative_collections:
                for time_range in time_ranges:
                    for bbox in bounding_boxes:
                        logger.info(f"å°è¯•: é›†åˆ={collection_id}, æ—¶é—´={time_range}, è¾¹ç•Œ={bbox}")
                        
                        alt_params = {
                            'collection_concept_id': collection_id,
                            'temporal': time_range,
                            'bounding_box': bbox,
                            'page_size': 100,
                            'sort_key': 'start_date'
                        }
                        
                        try:
                            alt_response = requests.get(cmr_url, params=alt_params, timeout=30)
                            alt_response.raise_for_status()
                            alt_data = alt_response.json()
                            
                            # æ£€æŸ¥å“åº”
                            if isinstance(alt_data, dict):
                                if 'hits' in alt_data and isinstance(alt_data['hits'], dict):
                                    alt_granules = alt_data['hits'].get('hits', [])
                                elif 'items' in alt_data and isinstance(alt_data['items'], list):
                                    alt_granules = alt_data['items']
                                elif 'hits' in alt_data and isinstance(alt_data['hits'], int):
                                    alt_granules = alt_data.get('items', [])
                                else:
                                    alt_granules = []
                                
                                logger.info(f"é›†åˆ {collection_id}: æ‰¾åˆ° {len(alt_granules)} ä¸ªæ–‡ä»¶")
                                
                                if len(alt_granules) > 0:
                                    # ä½¿ç”¨è¿™ä¸ªé›†åˆçš„ç»“æœ
                                    granules = alt_granules
                                    params = alt_params
                                    found_data = True
                                    logger.info(f"âœ… ä½¿ç”¨å‚æ•°: é›†åˆ={collection_id}, æ—¶é—´={time_range}")
                                    break
                        except Exception as e:
                            logger.warning(f"å°è¯•å¤±è´¥: {e}")
                            continue
                    
                    if found_data:
                        break
                
                if found_data:
                    break
        
        # é‡æ–°è®¡ç®—æ‰¾åˆ°çš„æ–‡ä»¶æ•°é‡
        logger.info(f"âœ… æœ€ç»ˆæ‰¾åˆ° {len(granules)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # æå–æ–‡ä»¶ä¿¡æ¯
        files = []
        logger.info(f"å¼€å§‹è§£æ {len(granules)} ä¸ªæ•°æ®æ–‡ä»¶...")
        
        for i, granule in enumerate(granules):  # å¤„ç†æ‰€æœ‰æ–‡ä»¶
            try:
                if i < 3:  # åªå¯¹å‰3ä¸ªæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
                    logger.info(f"è§£æç¬¬ {i+1} ä¸ªæ–‡ä»¶:")
                    logger.info(f"Granuleé”®: {list(granule.keys()) if isinstance(granule, dict) else 'Not a dict'}")
                
                if isinstance(granule, dict):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ummæ ¼å¼
                    if 'umm' in granule:
                        umm_data = granule['umm']
                        if i < 3:
                            logger.info(f"  UMMæ•°æ®é”®: {list(umm_data.keys()) if isinstance(umm_data, dict) else 'Not a dict'}")
                        
                        # ä»ummæ•°æ®ä¸­æå–ä¿¡æ¯
                        title = umm_data.get('DataGranule', {}).get('GranuleUR', 'Unknown')
                        
                        # è·å–æ—¶é—´ä¿¡æ¯
                        temporal = umm_data.get('TemporalExtent', {})
                        if temporal:
                            range_datetime = temporal.get('RangeDateTime', {})
                            if range_datetime:
                                start_date = range_datetime.get('BeginningDateTime')
                                end_date = range_datetime.get('EndingDateTime')
                        
                        # è·å–ä¸‹è½½URL
                        download_url = None
                        related_urls = umm_data.get('RelatedUrls', [])
                        for url in related_urls:
                            if url.get('Type') == 'GET DATA':
                                download_url = url.get('URL')
                                break
                        
                        # è·å–æ–‡ä»¶å¤§å°
                        size_mb = 0
                        archive_info = umm_data.get('ArchiveAndDistributionInformation', {})
                        if archive_info:
                            file_archive_info = archive_info.get('FileArchiveInformation', [])
                            if file_archive_info:
                                size = file_archive_info[0].get('FileSize')
                                if size:
                                    try:
                                        size_mb = float(size) / (1024 * 1024)
                                    except:
                                        size_mb = 0
                    else:
                        # å°è¯•ä¸åŒçš„å­—æ®µå
                        title = None
                        if 'title' in granule:
                            title = granule['title']
                        elif 'attributes' in granule and isinstance(granule['attributes'], dict):
                            title = granule['attributes'].get('title', 'Unknown')
                        else:
                            title = granule.get('id', 'Unknown')
                        
                        # è·å–æ—¶é—´ä¿¡æ¯
                        start_date = None
                        end_date = None
                        if 'attributes' in granule and isinstance(granule['attributes'], dict):
                            start_date = granule['attributes'].get('start_date')
                            end_date = granule['attributes'].get('end_date')
                        elif 'temporal' in granule:
                            temporal = granule['temporal']
                            if isinstance(temporal, list) and len(temporal) > 0:
                                start_date = temporal[0].get('begin_date')
                                end_date = temporal[0].get('end_date')
                        
                        # è·å–ä¸‹è½½URL
                        download_url = None
                        if 'links' in granule:
                            urls = granule['links']
                            for url in urls:
                                if isinstance(url, dict) and url.get('type') == 'GET DATA':
                                    download_url = url.get('href')
                                    break
                        
                        # è·å–æ–‡ä»¶å¤§å°
                        size_mb = 0
                        if 'attributes' in granule and isinstance(granule['attributes'], dict):
                            size = granule['attributes'].get('size')
                            if size:
                                size_mb = float(size) / (1024 * 1024)
                    
                    if i < 3:  # åªå¯¹å‰3ä¸ªæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
                        logger.info(f"  æ ‡é¢˜: {title}")
                        logger.info(f"  å¼€å§‹æ—¶é—´: {start_date}")
                        logger.info(f"  ç»“æŸæ—¶é—´: {end_date}")
                        logger.info(f"  ä¸‹è½½URL: {'æœ‰' if download_url else 'æ— '}")
                        logger.info(f"  æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
                    
                    if download_url:
                        files.append({
                            'id': granule.get('id', 'Unknown'),
                            'title': title,
                            'start_date': start_date,
                            'end_date': end_date,
                            'download_url': download_url,
                            'size_mb': size_mb
                        })
                        if i < 3:
                            logger.info(f"  âœ… æ–‡ä»¶ä¿¡æ¯æå–æˆåŠŸ")
                    else:
                        if i < 3:
                            logger.warning(f"  âš ï¸ æœªæ‰¾åˆ°ä¸‹è½½URL")
                        
            except Exception as e:
                if i < 3:
                    logger.error(f"è§£æç¬¬ {i+1} ä¸ªæ–‡ä»¶å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸæå– {len(files)} ä¸ªæ–‡ä»¶ä¿¡æ¯")
        
        # æ›´æ–°æœç´¢ç»“æœ
        search_result = {
            'region': region,
            'files': files,  # ä½¿ç”¨è§£æåçš„files
            'total_count': len(files),
            'search_params': params
        }
        
        return search_result
        
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.error(f"æœç´¢SMAPæ•°æ®å¤±è´¥: {e}")
        return None

def download_smap_file(download_url: str, output_dir: str, filename: str) -> bool:
    """
    ä¸‹è½½å•ä¸ªSMAPæ–‡ä»¶
    
    Args:
        download_url: ä¸‹è½½URL
        output_dir: è¾“å‡ºç›®å½•
        filename: æ–‡ä»¶å
    
    Returns:
        ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(output_path):
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return True
        
        logger.info(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {filename}")
        
        # ä½¿ç”¨NASAå‡­æ®ä¸‹è½½
        session = requests.Session()
        session.auth = (NASA_USERNAME, NASA_PASSWORD)
        
        response = session.get(download_url, stream=True, timeout=60)
        response.raise_for_status()
        
        # è·å–æ–‡ä»¶å¤§å°
        total_size = int(response.headers.get('content-length', 0))
        
        with open(output_path, 'wb') as f:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        if downloaded % (1024 * 1024) == 0:  # æ¯MBæ˜¾ç¤ºä¸€æ¬¡
                            logger.info(f"ä¸‹è½½è¿›åº¦: {progress:.1f}% ({downloaded/(1024*1024):.1f}MB)")
        
        logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
        return True
        
    except Exception as e:
        logger.error(f"ä¸‹è½½å¤±è´¥ {filename}: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ çœ‹é—¨ç‹—å®¡æ ¸é€šè¿‡ - NASA SMAPçœŸå®æ•°æ®ä¸‹è½½")
    print("=" * 60)
    
    # æ£€æŸ¥å‡­æ®
    if not NASA_USERNAME or not NASA_PASSWORD:
        print("âŒ é”™è¯¯: NASA Earthdataå‡­æ®æœªé…ç½®")
        print("è¯·æ£€æŸ¥ config/credentials.env æ–‡ä»¶")
        return
    
    print(f"ğŸ‘¤ ç”¨æˆ·: {NASA_USERNAME}")
    print(f"ğŸ”‘ å‡­æ®: {'å·²é…ç½®' if NASA_PASSWORD else 'æœªé…ç½®'}")
    
    # è®¾ç½®æ—¶é—´èŒƒå›´ (æœ€è¿‘30å¤©)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')
    
    print(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {start_str} åˆ° {end_str}")
    print(f"ğŸ“ åœ°ç†èŒƒå›´: çº¢æ²³æµåŸŸ (Red River Basin)")
    print(f"ğŸ›°ï¸ æ•°æ®æº: NASA SMAPå«æ˜Ÿ")
    print(f"ğŸ“ åˆ†è¾¨ç‡: 9km")
    
    # æœç´¢æ•°æ®
    logger.info("ğŸš€ å¼€å§‹NASA SMAPå®Œæ•´ä¸‹è½½æµç¨‹...")
    
    search_result = search_smap_data(start_str, end_str, 'red_river_basin')
    
    if not search_result:
        print("âŒ æœç´¢å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    print(f"\nğŸ“Š æœç´¢ç»“æœ:")
    print(f"   - åŒºåŸŸ: {search_result['region']['name']}")
    print(f"   - æ–‡ä»¶æ•°é‡: {search_result['total_count']}")
    print(f"   - è¾¹ç•Œæ¡†: {search_result['region']['bounding_box']}")
    
    if search_result['total_count'] == 0:
        print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æ•°æ®")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "data/raw/nasa_smap"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸‹è½½æ–‡ä»¶
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½åˆ°: {output_dir}")
    
    successful_downloads = 0
    total_size_mb = 0
    
    # ä½¿ç”¨search_resultä¸­çš„filesï¼Œè€Œä¸æ˜¯é‡æ–°è§£æ
    files_to_download = search_result.get('files', [])
    print(f"å‡†å¤‡ä¸‹è½½ {len(files_to_download)} ä¸ªæ–‡ä»¶")
    
    for i, file_info in enumerate(files_to_download[:10]):  # é™åˆ¶ä¸‹è½½å‰10ä¸ªæ–‡ä»¶
        print(f"\n[{i+1}/{min(10, len(files_to_download))}] å¤„ç†æ–‡ä»¶:")
        
        # å®‰å…¨åœ°è·å–æ–‡ä»¶ä¿¡æ¯
        title = file_info.get('title', 'Unknown')
        start_date = file_info.get('start_date', 'Unknown')
        end_date = file_info.get('end_date', 'Unknown')
        size_mb = file_info.get('size_mb', 0)
        
        print(f"   - æ ‡é¢˜: {title}")
        print(f"   - å¼€å§‹æ—¶é—´: {start_date}")
        print(f"   - ç»“æŸæ—¶é—´: {end_date}")
        print(f"   - å¤§å°: {size_mb:.1f} MB")
        
        # ç”Ÿæˆæ–‡ä»¶å
        if start_date and start_date != 'Unknown':
            date_part = start_date[:10] if len(start_date) >= 10 else 'unknown'
        else:
            date_part = 'unknown'
        
        filename = f"smap_soil_moisture_{date_part}.h5"
        
        download_url = file_info.get('download_url')
        if download_url:
            if download_smap_file(download_url, output_dir, filename):
                successful_downloads += 1
                total_size_mb += size_mb
        else:
            print(f"   âš ï¸ è·³è¿‡: æ— ä¸‹è½½URL")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡è½½
        time.sleep(1)
    
    # ä¸‹è½½ç»“æœ
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š ä¸‹è½½å®Œæˆ!")
    print(f"   - æˆåŠŸä¸‹è½½: {successful_downloads} ä¸ªæ–‡ä»¶")
    print(f"   - æ€»å¤§å°: {total_size_mb:.1f} MB")
    print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    if successful_downloads > 0:
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶: ls -la {output_dir}")
        print(f"   2. éªŒè¯æ•°æ®å®Œæ•´æ€§")
        print(f"   3. é›†æˆåˆ°HydrAI-SWEç³»ç»Ÿ")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰æˆåŠŸä¸‹è½½çš„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥:")
        print(f"   1. ç½‘ç»œè¿æ¥")
        print(f"   2. NASAå‡­æ®")
        print(f"   3. æ•°æ®å¯ç”¨æ€§")

if __name__ == "__main__":
    main()
