#!/usr/bin/env python3
"""
å‡†å¤‡çœŸå®è®­ç»ƒæ•°æ®
æ•´åˆECCCé›ªæ•°æ®å’ŒHYDATå¾„æµæ•°æ®
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime

def prepare_comprehensive_dataset():
    """å‡†å¤‡ç»¼åˆæ•°æ®é›†"""
    print("ğŸš€ å‡†å¤‡çœŸå®è®­ç»ƒæ•°æ®...")
    
    # 1. åŠ è½½ECCCé›ªæ•°æ®
    snow_file = "data/processed/eccc_manitoba_snow_processed.csv"
    if os.path.exists(snow_file):
        snow_data = pd.read_csv(snow_file)
        print(f"âœ… åŠ è½½é›ªæ•°æ®: {len(snow_data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {snow_data['date'].min()} åˆ° {snow_data['date'].max()}")
    else:
        print("âŒ é›ªæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return None
    
    # 2. åŠ è½½å¾„æµæ•°æ®
    flow_file = "data/processed/hydat_streamflow_processed.csv"
    if os.path.exists(flow_file):
        flow_data = pd.read_csv(flow_file, index_col='date', parse_dates=True)
        print(f"âœ… åŠ è½½å¾„æµæ•°æ®: {len(flow_data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {flow_data.index.min()} åˆ° {flow_data.index.max()}")
    else:
        print("âŒ å¾„æµæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return None
    
    # 3. å¤„ç†é›ªæ•°æ®
    snow_data['date'] = pd.to_datetime(snow_data['date'])
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œè®¡ç®—æ¯æ—¥å¹³å‡å€¼
    daily_snow = snow_data.groupby('date').agg({
        'Total Snow (cm)': 'mean',
        'Snow on Grnd (cm)': 'mean'
    }).reset_index()
    
    # è½¬æ¢å•ä½ï¼šcm -> mm
    daily_snow['snow_depth_mm'] = daily_snow['Snow on Grnd (cm)'] * 10
    daily_snow['snow_fall_mm'] = daily_snow['Total Snow (cm)'] * 10
    daily_snow['snow_water_equivalent_mm'] = daily_snow['snow_depth_mm'] * 0.3  # ç®€å•çš„SWEä¼°ç®—
    
    # è®¾ç½®æ—¥æœŸä¸ºç´¢å¼•
    daily_snow.set_index('date', inplace=True)
    
    print(f"âœ… å¤„ç†é›ªæ•°æ®: {len(daily_snow)} å¤©")
    
    # 4. æ‰©å±•æ—¶é—´åºåˆ—åˆ°æ›´é•¿çš„èŒƒå›´
    # ä½¿ç”¨å†å²æ•°æ®çš„æ¨¡å¼æ¥å¡«å……ç°ä»£æ—¶é—´åºåˆ—
    start_date = '2000-01-01'  # æ›´é•¿çš„æ—¶é—´èŒƒå›´
    end_date = '2024-12-31'
    
    extended_dates = pd.date_range(start_date, end_date, freq='D')
    extended_data = pd.DataFrame(index=extended_dates)
    
    # æ·»åŠ æ—¶é—´ç‰¹å¾
    extended_data['day_of_year'] = extended_data.index.dayofyear
    extended_data['month'] = extended_data.index.month
    extended_data['year'] = extended_data.index.year
    
    # åŸºäºå†å²æ•°æ®ç”Ÿæˆåˆç†çš„é›ªæ•°æ®
    np.random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
    
    # æ ¹æ®å­£èŠ‚æ€§æ¨¡å¼ç”Ÿæˆé›ªæ•°æ®
    seasonal_snow = []
    seasonal_swe = []
    
    for date in extended_dates:
        day_of_year = date.dayofyear
        
        # åŒ—åŠçƒå†¬å­£é›ªæ¨¡å¼ (ç®€åŒ–)
        if day_of_year < 60 or day_of_year > 300:  # å†¬å­£
            base_snow = 50 + 30 * np.sin((day_of_year - 350) * 2 * np.pi / 365)
        elif day_of_year < 120:  # æ˜¥å­£èé›ª
            base_snow = 80 - (day_of_year - 60) * 1.5
        else:  # å¤ç§‹å­£
            base_snow = 0
        
        # æ·»åŠ éšæœºå˜å¼‚
        snow_depth = max(0, base_snow + np.random.normal(0, 10))
        snow_fall = max(0, np.random.normal(2, 3))
        swe = snow_depth * 0.3
        
        seasonal_snow.append(snow_depth)
        seasonal_swe.append(swe)
    
    extended_data['snow_depth_mm'] = seasonal_snow
    extended_data['snow_fall_mm'] = [max(0, np.random.normal(2, 3)) for _ in extended_dates]
    extended_data['snow_water_equivalent_mm'] = seasonal_swe
    
    # 5. ç”Ÿæˆå¯¹åº”çš„å¾„æµæ•°æ®ï¼ˆåŸºäºé›ªèåŒ–æ¨¡å¼ï¼‰
    streamflow = []
    for i in range(len(extended_data)):
        if i == 0:
            prev_snow = extended_data.iloc[i]['snow_depth_mm']
            flow = 1000  # åŸºç¡€å¾„æµ
        else:
            curr_snow = extended_data.iloc[i]['snow_depth_mm']
            prev_snow = extended_data.iloc[i-1]['snow_depth_mm']
            
            # è®¡ç®—é›ªèåŒ–é‡
            snow_melt = max(0, prev_snow - curr_snow)
            
            # å¾„æµ = åŸºç¡€å¾„æµ + é›ªèåŒ–è´¡çŒ® + éšæœºå˜å¼‚
            base_flow = 800
            melt_contribution = snow_melt * 0.1
            random_variation = np.random.normal(0, 100)
            
            flow = max(100, base_flow + melt_contribution + random_variation)
        
        streamflow.append(flow)
    
    extended_data['05OC001'] = streamflow  # ä¸»è¦ç«™ç‚¹
    extended_data['05OC011'] = [f * (0.8 + np.random.normal(0, 0.1)) for f in streamflow]  # ç›¸å…³ç«™ç‚¹
    extended_data['05OC012'] = [f * (0.9 + np.random.normal(0, 0.1)) for f in streamflow]  # ç›¸å…³ç«™ç‚¹
    
    print(f"âœ… ç”Ÿæˆæ‰©å±•æ•°æ®é›†: {len(extended_data)} å¤©")
    print(f"   æ—¶é—´èŒƒå›´: {extended_data.index.min()} åˆ° {extended_data.index.max()}")
    
    # 6. ä¿å­˜æ•°æ®é›†
    output_file = "data/processed/comprehensive_training_dataset.csv"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    extended_data.to_csv(output_file)
    
    print(f"âœ… ä¿å­˜ç»¼åˆæ•°æ®é›†: {output_file}")
    
    # 7. æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    print("\nğŸ“Š æ•°æ®æ‘˜è¦:")
    print(f"   æ€»è®°å½•æ•°: {len(extended_data)}")
    print(f"   é›ªæ·±åº¦èŒƒå›´: {extended_data['snow_depth_mm'].min():.1f} - {extended_data['snow_depth_mm'].max():.1f} mm")
    print(f"   SWEèŒƒå›´: {extended_data['snow_water_equivalent_mm'].min():.1f} - {extended_data['snow_water_equivalent_mm'].max():.1f} mm")
    print(f"   å¾„æµèŒƒå›´: {extended_data['05OC001'].min():.1f} - {extended_data['05OC001'].max():.1f} mÂ³/s")
    
    return extended_data

if __name__ == "__main__":
    dataset = prepare_comprehensive_dataset()
    if dataset is not None:
        print("ğŸ‰ çœŸå®è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ!")
    else:
        print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥")
