#!/usr/bin/env python3
"""
ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒå®Œæ•´æ¨¡å‹
åŸºäºå¿«é€Ÿä¼˜åŒ–çš„æœ€ä½³å‚æ•°é…ç½®
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import os
from datetime import datetime
import time

class OptimizedGRUModel(nn.Module):
    """ä½¿ç”¨æœ€ä½³è¶…å‚æ•°çš„GRUæ¨¡å‹"""
    
    def __init__(self, input_size=6, hidden_size=64, num_layers=2, dropout=0.1):
        super(OptimizedGRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                          dropout=dropout if num_layers > 1 else 0, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out)
        output = self.fc(gru_out[:, -1, :])
        return output

class OptimizedModelTrainer:
    """ä½¿ç”¨æœ€ä½³è¶…å‚æ•°çš„æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.scaler_X = None
        self.scaler_y = None
        self.sequence_length = 30
        self.best_params = {
            'hidden_size': 64,
            'num_layers': 2,
            'dropout': 0.1,
            'learning_rate': 0.001,
            'batch_size': 16
        }
        
    def load_data_and_scalers(self):
        """åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨"""
        print("ğŸ“Š åŠ è½½æ•°æ®å’Œæ ‡å‡†åŒ–å™¨...")
        
        try:
            # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
            data_path = "data/processed/standardized_training_dataset.csv"
            data = pd.read_csv(data_path, index_col=0, parse_dates=True)
            print(f"âœ… åŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨å‚æ•°
            import pickle
            with open('models/standardization_params.pkl', 'rb') as f:
                params = pickle.load(f)
            
            # é‡å»ºæ ‡å‡†åŒ–å™¨
            self.scaler_X = StandardScaler()
            self.scaler_X.mean_ = params['scaler_X_mean']
            self.scaler_X.scale_ = params['scaler_X_scale']
            
            self.scaler_y = StandardScaler()
            self.scaler_y.mean_ = params['scaler_y_mean']
            self.scaler_y.scale_ = params['scaler_y_scale']
            
            print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            return data
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_sequences(self, data):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡åºåˆ—æ•°æ®...")
        
        feature_cols = ['snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm', 
                       'day_of_year', 'month', 'year']
        target_col = 'snow_water_equivalent_mm'
        
        X = data[feature_cols].values
        y = data[target_col].values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.transform(X)
        y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()
        
        # åˆ›å»ºåºåˆ—
        X_seq, y_seq = [], []
        for i in range(len(X_scaled) - self.sequence_length):
            X_seq.append(X_scaled[i:(i + self.sequence_length)])
            y_seq.append(y_scaled[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"âœ… åºåˆ—æ•°æ®å‡†å¤‡å®Œæˆ: {X_seq.shape}, {y_seq.shape}")
        return X_seq, y_seq
    
    def split_data(self, X, y, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
        """æ•°æ®åˆ†å‰²ï¼šè®­ç»ƒ70%ï¼ŒéªŒè¯15%ï¼Œæµ‹è¯•15%"""
        n = len(X)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        X_val = X[train_end:val_end]
        y_val = y[train_end:val_end]
        X_test = X[val_end:]
        y_test = y[val_end:]
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)
    
    def create_data_loaders(self, train_data, val_data, test_data):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        X_train, y_train = train_data
        X_val, y_val = val_data
        X_test, y_test = test_data
        
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
        test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
        
        train_loader = DataLoader(train_dataset, batch_size=self.best_params['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.best_params['batch_size'], shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=self.best_params['batch_size'], shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def train_full_model(self, train_loader, val_loader):
        """è®­ç»ƒå®Œæ•´æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒå®Œæ•´æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        model = OptimizedGRUModel(
            input_size=6,
            hidden_size=self.best_params['hidden_size'],
            num_layers=self.best_params['num_layers'],
            dropout=self.best_params['dropout']
        )
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=self.best_params['learning_rate'])
        
        # è®­ç»ƒå‚æ•°
        epochs = 100
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 15
        train_losses = []
        val_losses = []
        
        print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
        print(f"   - éšè—å¤§å°: {self.best_params['hidden_size']}")
        print(f"   - å±‚æ•°: {self.best_params['num_layers']}")
        print(f"   - Dropout: {self.best_params['dropout']}")
        print(f"   - å­¦ä¹ ç‡: {self.best_params['learning_rate']}")
        print(f"   - æ‰¹å¤§å°: {self.best_params['batch_size']}")
        print(f"   - æœ€å¤§è½®æ•°: {epochs}")
        print(f"   - æ—©åœè€å¿ƒ: {patience}")
        
        start_time = time.time()
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            train_losses.append(train_loss)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    val_loss += loss.item()
            
            val_loss /= len(val_loader)
            val_losses.append(val_loss)
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                break
        
        training_time = time.time() - start_time
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_model_state)
        
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   - æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"   - è®­ç»ƒè½®æ•°: {epoch+1}")
        print(f"   - æ€»è€—æ—¶: {training_time:.2f} ç§’")
        
        return model, train_losses, val_losses, training_time
    
    def evaluate_model(self, model, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        print("ğŸ” è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                outputs = model(batch_X)
                predictions.extend(outputs.squeeze().cpu().numpy())
                actuals.extend(batch_y.cpu().numpy())
        
        # åæ ‡å‡†åŒ–é¢„æµ‹å€¼å’Œå®é™…å€¼
        predictions = np.array(predictions).reshape(-1, 1)
        actuals = np.array(actuals).reshape(-1, 1)
        
        predictions_original = self.scaler_y.inverse_transform(predictions).flatten()
        actuals_original = self.scaler_y.inverse_transform(actuals).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(actuals_original, predictions_original)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(actuals_original, predictions_original)
        r2 = r2_score(actuals_original, predictions_original)
        
        print(f"âœ… æµ‹è¯•é›†æ€§èƒ½:")
        print(f"   - RMSE: {rmse:.4f}")
        print(f"   - MAE: {mae:.4f}")
        print(f"   - RÂ²: {r2:.4f}")
        
        return {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'predictions': predictions_original,
            'actuals': actuals_original
        }
    
    def save_model_and_results(self, model, train_losses, val_losses, test_results, training_time):
        """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/optimized_gru_model_{timestamp}.pth"
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        torch.save({
            'model_state_dict': model.state_dict(),
            'best_params': self.best_params,
            'sequence_length': self.sequence_length,
            'scaler_X_mean': self.scaler_X.mean_,
            'scaler_X_scale': self.scaler_X.scale_,
            'scaler_y_mean': self.scaler_y.mean_,
            'scaler_y_scale': self.scaler_y.scale_,
            'test_results': test_results,
            'training_time': training_time
        }, model_path)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = f"logs/optimized_training_history_{timestamp}.json"
        import json
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump({
                'best_params': self.best_params,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'test_results': {
                    'rmse': test_results['rmse'],
                    'mae': test_results['mae'],
                    'r2': test_results['r2']
                },
                'training_time': training_time,
                'training_date': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
        
        # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        self.generate_training_report(test_results, training_time, timestamp)
        
        return model_path
    
    def generate_training_report(self, test_results, training_time, timestamp):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        
        report_path = f"logs/optimized_training_report_{timestamp}.md"
        
        report_content = f"""# æœ€ä½³è¶…å‚æ•°æ¨¡å‹è®­ç»ƒæŠ¥å‘Š

## è®­ç»ƒæ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æœ€ä½³è¶…å‚æ•°é…ç½®
åŸºäºå¿«é€Ÿä¼˜åŒ–çš„æœ€ä½³å‚æ•°:
- **éšè—å¤§å°**: {self.best_params['hidden_size']}
- **å±‚æ•°**: {self.best_params['num_layers']}
- **Dropout**: {self.best_params['dropout']}
- **å­¦ä¹ ç‡**: {self.best_params['learning_rate']}
- **æ‰¹å¤§å°**: {self.best_params['batch_size']}

## è®­ç»ƒç»“æœ
- **è®­ç»ƒè½®æ•°**: æ ¹æ®æ—©åœæœºåˆ¶è‡ªåŠ¨ç¡®å®š
- **è®­ç»ƒè€—æ—¶**: {training_time:.2f} ç§’
- **æ—©åœè€å¿ƒ**: 15ä¸ªepoch

## æµ‹è¯•é›†æ€§èƒ½
ğŸ† **æœ€ç»ˆæ¨¡å‹æ€§èƒ½**:
- **RMSE**: {test_results['rmse']:.4f}
- **MAE**: {test_results['mae']:.4f}
- **RÂ²**: {test_results['r2']:.4f}

## å…³é”®æ”¹è¿›
1. **è¶…å‚æ•°ä¼˜åŒ–**: ä»å¿«é€Ÿä¼˜åŒ–ä¸­é€‰æ‹©äº†æœ€ä½³é…ç½®
2. **å®Œæ•´è®­ç»ƒ**: ä½¿ç”¨æœ€ä½³å‚æ•°è¿›è¡Œå®Œæ•´æ¨¡å‹è®­ç»ƒ
3. **æ—©åœæœºåˆ¶**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³è®­ç»ƒè½®æ•°
4. **æ€§èƒ½éªŒè¯**: åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸ŠéªŒè¯æœ€ç»ˆæ€§èƒ½

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. **ç²¾ç»†è°ƒä¼˜**: åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œæ›´ç²¾ç»†çš„æœç´¢
2. **æ¨¡å‹é›†æˆ**: è€ƒè™‘é›†æˆå‰3ä¸ªæœ€ä½³é…ç½®
3. **æ•°æ®å¢å¼º**: ç»“åˆæœ€ä½³è¶…å‚æ•°å°è¯•æ•°æ®å¢å¼º
4. **éƒ¨ç½²å‡†å¤‡**: å‡†å¤‡æ¨¡å‹éƒ¨ç½²å’Œç›‘æ§

## æ¨¡å‹æ–‡ä»¶
- **æ¨¡å‹æ–‡ä»¶**: `models/optimized_gru_model_{timestamp}.pth`
- **è®­ç»ƒå†å²**: `logs/optimized_training_history_{timestamp}.json`
- **æœ¬æŠ¥å‘Š**: `logs/optimized_training_report_{timestamp}.md`
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def plot_training_history(self, train_losses, val_losses):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
        plt.plot(val_losses, label='éªŒè¯æŸå¤±', color='red')
        plt.xlabel('Epoch')
        plt.ylabel('æŸå¤±')
        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue', alpha=0.7)
        plt.plot(val_losses, label='éªŒè¯æŸå¤±', color='red', alpha=0.7)
        plt.xlabel('Epoch')
        plt.ylabel('æŸå¤± (å¯¹æ•°å°ºåº¦)')
        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤± (å¯¹æ•°å°ºåº¦)')
        plt.yscale('log')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plot_path = f"logs/optimized_training_plots_{timestamp}.png"
        os.makedirs(os.path.dirname(plot_path), exist_ok=True)
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜: {plot_path}")
    
    def run_full_training(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸ¯ å¼€å§‹å®Œæ•´æ¨¡å‹è®­ç»ƒæµç¨‹...")
        
        # 1. åŠ è½½æ•°æ®
        data = self.load_data_and_scalers()
        if data is None:
            return
        
        # 2. å‡†å¤‡åºåˆ—æ•°æ®
        X, y = self.prepare_sequences(data)
        
        # 3. åˆ†å‰²æ•°æ®
        train_data, val_data, test_data = self.split_data(X, y)
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   - è®­ç»ƒé›†: {len(train_data[0])} æ ·æœ¬")
        print(f"   - éªŒè¯é›†: {len(val_data[0])} æ ·æœ¬")
        print(f"   - æµ‹è¯•é›†: {len(test_data[0])} æ ·æœ¬")
        
        # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = self.create_data_loaders(train_data, val_data, test_data)
        
        # 5. è®­ç»ƒæ¨¡å‹
        model, train_losses, val_losses, training_time = self.train_full_model(train_loader, val_loader)
        
        # 6. è¯„ä¼°æ¨¡å‹
        test_results = self.evaluate_model(model, test_loader)
        
        # 7. ä¿å­˜ç»“æœ
        model_path = self.save_model_and_results(model, train_losses, val_losses, test_results, training_time)
        
        # 8. ç»˜åˆ¶è®­ç»ƒå†å²
        self.plot_training_history(train_losses, val_losses)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å®Œæ•´æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"âœ… æœ€ä½³è¶…å‚æ•°æ¨¡å‹å·²ä¿å­˜: {model_path}")
        print(f"âœ… æµ‹è¯•é›†RÂ²: {test_results['r2']:.4f}")
        print(f"âœ… æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        print("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜")
        
        return model_path, test_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HydrAI-SWE æœ€ä½³è¶…å‚æ•°å®Œæ•´æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = OptimizedModelTrainer()
        
        # è¿è¡Œå®Œæ•´è®­ç»ƒ
        model_path, test_results = trainer.run_full_training()
        
        if model_path and test_results:
            print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
            print("  1. åœ¨æœ€ä½³å‚æ•°é™„è¿‘è¿›è¡Œç²¾ç»†æœç´¢")
            print("  2. è€ƒè™‘é›†æˆå‰3ä¸ªæœ€ä½³é…ç½®")
            print("  3. å°è¯•æ•°æ®å¢å¼ºæŠ€æœ¯")
            print("  4. å‡†å¤‡æ¨¡å‹éƒ¨ç½²")
        else:
            print("âŒ å®Œæ•´è®­ç»ƒå¤±è´¥")
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
