#!/usr/bin/env python3
"""
ä¿®å¤ Environment Canada æ•°æ®è´¨é‡é—®é¢˜
è§£å†³æ—¶é—´å‚æ•°é”™è¯¯å¯¼è‡´çš„é«˜ç¼ºå¤±ç‡é—®é¢˜
"""

import os
import sys
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
import requests
import time
from typing import Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnvironmentCanadaDataFixer:
    """ä¿®å¤ Environment Canada æ•°æ®è´¨é‡é—®é¢˜çš„å·¥å…·"""
    
    def __init__(self):
        # çº¢æ²³æµåŸŸç›¸å…³çš„Environment Canadaç«™ç‚¹
        self.stations = {
            'winnipeg_airport': {
                'station_id': '27174',
                'name': 'Winnipeg Richardson International Airport',
                'province': 'MB',
                'lat': 49.9100,
                'lon': -97.2394
            },
            'morris': {
                'station_id': '3025', 
                'name': 'Morris',
                'province': 'MB',
                'lat': 49.3558,
                'lon': -97.3642
            },
            'emerson': {
                'station_id': '3017',
                'name': 'Emerson',
                'province': 'MB', 
                'lat': 49.0042,
                'lon': -97.2189
            }
        }
        
        # Environment Canada æ•°æ®URLæ¨¡æ¿
        self.base_url = "https://climate.weather.gc.ca/climate_data/bulk_data_e.html"
        
        logger.info("âœ… Environment Canadaæ•°æ®ä¿®å¤å·¥å…·åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š å¯ç”¨ç«™ç‚¹: {len(self.stations)} ä¸ª")
    
    def download_corrected_data(self, output_dir: str = "data/real/environment_canada_fixed") -> Dict:
        """ä¸‹è½½ä¿®æ­£åçš„å†å²æ•°æ®"""
        try:
            logger.info("ğŸš€ å¼€å§‹ä¸‹è½½ä¿®æ­£åçš„Environment Canadaå†å²æ•°æ®")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # ä½¿ç”¨æ­£ç¡®çš„å†å²æ—¥æœŸèŒƒå›´
            # Environment Canadaé€šå¸¸æœ‰1-2ä¸ªæœˆçš„æ•°æ®å»¶è¿Ÿ
            end_date = datetime.now() - timedelta(days=90)  # 3ä¸ªæœˆå‰
            start_date = datetime(2023, 1, 1)  # ä»2023å¹´å¼€å§‹
            
            logger.info(f"ğŸ“… ä¸‹è½½èŒƒå›´: {start_date.strftime('%Y-%m')} åˆ° {end_date.strftime('%Y-%m')}")
            
            all_downloads = []
            successful_downloads = 0
            
            # éå†æ¯ä¸ªç«™ç‚¹
            for station_key, station_info in self.stations.items():
                logger.info(f"ğŸ“ å¤„ç†ç«™ç‚¹: {station_info['name']}")
                
                station_downloads = []
                current_date = start_date
                
                # æŒ‰æœˆä¸‹è½½æ•°æ®
                while current_date <= end_date:
                    year = current_date.year
                    month = current_date.month
                    
                    # ä¸‹è½½æœˆåº¦æ•°æ®
                    result = self._download_monthly_data(station_key, year, month)
                    
                    if result['status'] == 'success':
                        # éªŒè¯ä¸‹è½½çš„æ•°æ®è´¨é‡
                        if self._validate_data_quality(result['data']):
                            # ä¿å­˜æ•°æ®æ–‡ä»¶
                            filename = f"{station_key}_{year}_{month:02d}.csv"
                            filepath = os.path.join(output_dir, filename)
                            
                            with open(filepath, 'w', encoding='utf-8') as f:
                                f.write(result['data'])
                            
                            result['local_file'] = filepath
                            successful_downloads += 1
                            
                            logger.info(f"ğŸ’¾ ä¿å­˜: {filename}")
                        else:
                            logger.warning(f"âš ï¸ {station_key} {year}-{month:02d} æ•°æ®è´¨é‡éªŒè¯å¤±è´¥")
                            result['status'] = 'quality_failed'
                    elif result['status'] == 'no_data':
                        logger.info(f"â„¹ï¸ {station_key} {year}-{month:02d} æ— æ•°æ®")
                    else:
                        logger.warning(f"âš ï¸ {station_key} {year}-{month:02d} ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")
                    
                    station_downloads.append(result)
                    all_downloads.append(result)
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸ªæœˆ
                    if month == 12:
                        current_date = current_date.replace(year=year+1, month=1)
                    else:
                        current_date = current_date.replace(month=month+1)
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(2)
                
                logger.info(f"âœ… å®Œæˆç«™ç‚¹ {station_info['name']}: {len([d for d in station_downloads if d['status'] == 'success'])} ä¸ªæœˆçš„æ•°æ®")
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            merged_file = self._merge_fixed_data(output_dir)
            
            summary = {
                'status': 'success',
                'total_downloads': len(all_downloads),
                'successful_downloads': successful_downloads,
                'stations': len(self.stations),
                'output_dir': output_dir,
                'merged_file': merged_file,
                'downloads': all_downloads
            }
            
            logger.info(f"ğŸ‰ ä¿®æ­£åçš„Environment Canadaæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“Š æˆåŠŸä¸‹è½½: {successful_downloads}/{len(all_downloads)} ä¸ªæ–‡ä»¶")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ä¿®æ­£åçš„æ•°æ®å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _download_monthly_data(self, station_key: str, year: int, month: int) -> Dict:
        """ä¸‹è½½ç‰¹å®šç«™ç‚¹çš„æœˆåº¦æ•°æ®"""
        try:
            station = self.stations.get(station_key)
            if not station:
                raise ValueError(f"æœªçŸ¥ç«™ç‚¹: {station_key}")
            
            logger.info(f"ğŸ“¥ ä¸‹è½½ {station['name']} æ•°æ®: {year}-{month:02d}")
            
            # æ„å»ºä¸‹è½½URL
            params = {
                'format': 'csv',
                'stationID': station['station_id'],
                'Year': year,
                'Month': month,
                'Day': '1',
                'timeframe': 2,  # 2=daily, 1=hourly
                'submit': 'Download Data'
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(self.base_url, params=params, timeout=30)
            
            if response.status_code == 200:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„CSVæ•°æ®
                content = response.text
                if 'Date/Time' in content and len(content.strip().split('\n')) > 3:
                    logger.info(f"âœ… æˆåŠŸä¸‹è½½ {station['name']} {year}-{month:02d} æ•°æ®")
                    
                    return {
                        'status': 'success',
                        'station_key': station_key,
                        'station_info': station,
                        'year': year,
                        'month': month,
                        'data': content,
                        'url': response.url
                    }
                else:
                    logger.warning(f"âš ï¸ {station['name']} {year}-{month:02d} æ•°æ®å†…å®¹æ— æ•ˆ")
                    return {
                        'status': 'no_data',
                        'station_key': station_key,
                        'year': year,
                        'month': month,
                        'message': 'æ•°æ®å†…å®¹æ— æ•ˆ'
                    }
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                return {
                    'status': 'error',
                    'station_key': station_key,
                    'error': f'HTTP {response.status_code}'
                }
                
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ {station_key} æ•°æ®å¤±è´¥: {e}")
            return {
                'status': 'error',
                'station_key': station_key,
                'error': str(e)
            }
    
    def _validate_data_quality(self, data_content: str) -> bool:
        """éªŒè¯æ•°æ®è´¨é‡"""
        try:
            lines = data_content.strip().split('\n')
            if len(lines) < 4:  # è‡³å°‘éœ€è¦æ ‡é¢˜è¡Œå’Œ3è¡Œæ•°æ®
                return False
            
            # æ£€æŸ¥æ•°æ®è¡Œ
            data_lines = [line for line in lines[1:] if line.strip() and ',' in line]
            if len(data_lines) < 3:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°å€¼åˆ—
            first_data_line = data_lines[0]
            values = first_data_line.split(',')
            
            # è®¡ç®—æœ‰æ•ˆæ•°å€¼åˆ—
            numeric_values = 0
            for value in values:
                value = value.strip().strip('"')
                if value and value not in ['', 'M', 'E', 'NA', 'N/A', 'null']:
                    try:
                        float(value)
                        numeric_values += 1
                    except ValueError:
                        continue
            
            # è‡³å°‘éœ€è¦5ä¸ªæœ‰æ•ˆæ•°å€¼åˆ—
            is_valid = numeric_values >= 5
            
            if not is_valid:
                logger.debug(f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: åªæœ‰ {numeric_values} ä¸ªæœ‰æ•ˆæ•°å€¼åˆ—")
            
            return is_valid
            
        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def _merge_fixed_data(self, data_dir: str) -> Optional[str]:
        """åˆå¹¶ä¿®æ­£åçš„æ•°æ®"""
        try:
            logger.info("ğŸ”— åˆå¹¶ä¿®æ­£åçš„æ•°æ®...")
            
            all_dataframes = []
            csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
            
            for csv_file in csv_files:
                try:
                    filepath = os.path.join(data_dir, csv_file)
                    
                    # è¯»å–CSVæ–‡ä»¶
                    df = pd.read_csv(filepath, encoding='utf-8', low_memory=False)
                    
                    # æ·»åŠ ç«™ç‚¹ä¿¡æ¯
                    station_key = csv_file.split('_')[0]
                    if station_key in self.stations:
                        station_info = self.stations[station_key]
                        df['station_key'] = station_key
                        df['station_name'] = station_info['name']
                        df['station_lat'] = station_info['lat']
                        df['station_lon'] = station_info['lon']
                    
                    all_dataframes.append(df)
                    logger.info(f"ğŸ“„ å¤„ç†: {csv_file} ({len(df)} è¡Œ)")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†æ–‡ä»¶ {csv_file} å¤±è´¥: {e}")
                    continue
            
            if all_dataframes:
                # åˆå¹¶æ‰€æœ‰æ•°æ®
                merged_df = pd.concat(all_dataframes, ignore_index=True)
                
                # ä¿å­˜åˆå¹¶åçš„æ•°æ®
                merged_file = os.path.join(data_dir, 'environment_canada_fixed.csv')
                merged_df.to_csv(merged_file, index=False, encoding='utf-8')
                
                logger.info(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {merged_file}")
                logger.info(f"ğŸ“Š åˆå¹¶åæ•°æ®: {len(merged_df)} è¡Œ, {len(merged_df.columns)} åˆ—")
                
                return merged_file
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯åˆå¹¶")
                return None
                
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶æ•°æ®å¤±è´¥: {e}")
            return None
    
    def analyze_fixed_data(self, data_dir: str) -> Dict:
        """åˆ†æä¿®æ­£åçš„æ•°æ®è´¨é‡"""
        try:
            logger.info("ğŸ” åˆ†æä¿®æ­£åçš„æ•°æ®è´¨é‡...")
            
            merged_file = os.path.join(data_dir, 'environment_canada_fixed.csv')
            
            if not os.path.exists(merged_file):
                return {'status': 'no_merged_file', 'message': 'æœªæ‰¾åˆ°åˆå¹¶æ•°æ®æ–‡ä»¶'}
            
            # è¯»å–åˆå¹¶æ•°æ®
            df = pd.read_csv(merged_file, low_memory=False)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            validation_result = {
                'status': 'success',
                'total_records': len(df),
                'date_range': {
                    'start': df['Year'].min() if 'Year' in df.columns else 'Unknown',
                    'end': df['Year'].max() if 'Year' in df.columns else 'Unknown'
                },
                'stations': df['station_key'].nunique() if 'station_key' in df.columns else 0,
                'variables': list(df.columns),
                'missing_data': {},
                'data_quality': 'government_official_fixed'
            }
            
            # æ£€æŸ¥ç¼ºå¤±æ•°æ®
            for col in df.columns:
                missing_count = df[col].isna().sum()
                if missing_count > 0:
                    validation_result['missing_data'][col] = {
                        'count': int(missing_count),
                        'percentage': float(missing_count / len(df) * 100)
                    }
            
            # è®¡ç®—æ€»ä½“ç¼ºå¤±ç‡
            total_missing = sum([info['count'] for info in validation_result['missing_data'].values()])
            overall_missing_rate = total_missing / (len(df) * len(df.columns)) * 100
            validation_result['overall_missing_rate'] = overall_missing_rate
            
            # æ£€æŸ¥å…³é”®å˜é‡
            key_variables = ['Temp (Â°C)', 'Rel Hum (%)', 'Wind Spd (km/h)', 'Total Precip (mm)']
            available_key_vars = [var for var in key_variables if var in df.columns]
            
            validation_result['key_variables_available'] = available_key_vars
            validation_result['data_completeness'] = len(available_key_vars) / len(key_variables)
            
            logger.info(f"âœ… æ•°æ®è´¨é‡åˆ†æå®Œæˆ:")
            logger.info(f"  ğŸ“Š æ€»è®°å½•: {validation_result['total_records']}")
            logger.info(f"  ğŸ“ ç«™ç‚¹æ•°: {validation_result['stations']}")
            logger.info(f"  ğŸ“ˆ å…³é”®å˜é‡: {len(available_key_vars)}/{len(key_variables)}")
            logger.info(f"  ğŸ“‰ æ€»ä½“ç¼ºå¤±ç‡: {overall_missing_rate:.1f}%")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Environment Canada æ•°æ®è´¨é‡ä¿®å¤å·¥å…·")
    print("=" * 60)
    
    try:
        # åˆ›å»ºä¿®å¤å·¥å…·
        fixer = EnvironmentCanadaDataFixer()
        
        # ä¸‹è½½ä¿®æ­£åçš„æ•°æ®
        print("\nğŸš€ å¼€å§‹ä¸‹è½½ä¿®æ­£åçš„å†å²æ•°æ®...")
        result = fixer.download_corrected_data()
        
        if result['status'] == 'success':
            print(f"âœ… ä¸‹è½½æˆåŠŸ!")
            print(f"ğŸ“Š æˆåŠŸä¸‹è½½: {result['successful_downloads']}/{result['total_downloads']} ä¸ªæ–‡ä»¶")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {result['output_dir']}")
            
            if result['merged_file']:
                print(f"ğŸ”— åˆå¹¶æ–‡ä»¶: {result['merged_file']}")
                
                # åˆ†ææ•°æ®è´¨é‡
                print("\nğŸ” åˆ†æä¿®æ­£åçš„æ•°æ®è´¨é‡...")
                validation = fixer.analyze_fixed_data(result['output_dir'])
                
                if validation['status'] == 'success':
                    print(f"âœ… æ•°æ®è´¨é‡åˆ†æå®Œæˆ!")
                    print(f"ğŸ“Š æ€»è®°å½•: {validation['total_records']}")
                    print(f"ğŸ“ ç«™ç‚¹æ•°: {validation['stations']}")
                    print(f"ğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {validation['data_completeness']:.1%}")
                    print(f"ğŸ“‰ æ€»ä½“ç¼ºå¤±ç‡: {validation['overall_missing_rate']:.1f}%")
                    print(f"ğŸ·ï¸ æ•°æ®è´¨é‡: {validation['data_quality']}")
                    
                    # æ¯”è¾ƒä¿®å¤å‰åçš„è´¨é‡
                    print(f"\nğŸ“Š ä¿®å¤æ•ˆæœå¯¹æ¯”:")
                    print(f"  ä¿®å¤å‰ç¼ºå¤±ç‡: 64.7%")
                    print(f"  ä¿®å¤åç¼ºå¤±ç‡: {validation['overall_missing_rate']:.1f}%")
                    
                    if validation['overall_missing_rate'] < 64.7:
                        improvement = 64.7 - validation['overall_missing_rate']
                        print(f"  âœ… æ”¹å–„æ•ˆæœ: {improvement:.1f}%")
                    else:
                        print(f"  âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
                        
                else:
                    print(f"âŒ æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {validation.get('error', 'Unknown error')}")
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
