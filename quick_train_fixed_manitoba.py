#!/usr/bin/env python3
"""
å¿«é€Ÿè®­ç»ƒä¿®å¤åçš„æ›¼çœæ•°æ®
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def estimate_soil_moisture(data):
    """ä¼°ç®—åœŸå£¤æ¹¿åº¦"""
    base_moisture = 0.3
    temp_factor = 1 - (data['temperature'] + 20) / 60
    temp_factor = np.clip(temp_factor, 0, 1)
    precip_factor = np.log1p(data['precipitation']) / 20
    precip_factor = np.clip(precip_factor, 0, 0.3)
    
    seasonal_factor = np.where(
        data['month'].isin([12, 1, 2]), 0.1,
        np.where(
            data['month'].isin([3, 4, 5]), 0.2,
            np.where(
                data['month'].isin([6, 7, 8]), 0.0,
                0.1
            )
        )
    )
    
    crop_factor = data['crop_growth_status'] * 0.1
    
    estimated = (
        base_moisture * 0.4 +
        temp_factor * 0.3 +
        precip_factor * 0.2 +
        seasonal_factor * 0.1 +
        crop_factor * 0.1
    )
    
    return np.clip(estimated, 0.1, 0.9)

def main():
    logger.info("ğŸš€ å¿«é€Ÿè®­ç»ƒä¿®å¤åçš„æ›¼çœæ•°æ®...")
    
    # åŠ è½½ä¿®å¤åçš„æ•°æ®
    data = pd.read_csv("data/real/manitoba/fixed/manitoba_fixed_no_leakage_20250822_073708.csv")
    logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # åˆ›å»ºç›®æ ‡å˜é‡
    target = estimate_soil_moisture(data)
    
    # å‡†å¤‡ç‰¹å¾
    features = data.drop(['year', 'month', 'day'], axis=1)
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        features, target, test_size=0.3, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train_scaled)
    y_train_tensor = torch.FloatTensor(y_train.values)
    X_test_tensor = torch.FloatTensor(X_test_scaled)
    y_test_tensor = torch.FloatTensor(y_test.values)
    
    # æ„å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(X_train.shape[1], 32),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(32, 16),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(16, 1)
    )
    
    # è®­ç»ƒ
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    logger.info("ğŸ”§ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(50):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensor).squeeze()
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 10 == 0:
            logger.info(f"Epoch {epoch + 1}/50: Loss: {loss.item():.6f}")
    
    # è¯„ä¼°
    model.eval()
    with torch.no_grad():
        test_predictions = model(X_test_tensor).squeeze()
    
    r2 = r2_score(y_test, test_predictions)
    mae = mean_absolute_error(y_test, test_predictions)
    rmse = np.sqrt(mean_squared_error(y_test, test_predictions))
    
    logger.info("ğŸ“Š ä¿®å¤åæ¨¡å‹æ€§èƒ½è¯„ä¼°:")
    logger.info(f"  RÂ²: {r2:.4f}")
    logger.info(f"  MAE: {mae:.4f}")
    logger.info(f"  RMSE: {rmse:.4f}")
    logger.info(f"  çŠ¶æ€: {'è¿‡æ‹Ÿåˆ' if r2 < 0 else 'æ­£å¸¸'}")
    logger.info(f"  æ•°æ®æ³„éœ²: å·²ä¿®å¤")
    
    return r2, mae, rmse

if __name__ == "__main__":
    main()
