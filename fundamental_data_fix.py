#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡æ ¹æœ¬æ€§ä¿®å¤è„šæœ¬
è§£å†³æ•°æ®åˆ†å¸ƒã€ç›¸å…³æ€§å’Œæ ¹æœ¬è´¨é‡é—®é¢˜
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import json
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression
from sklearn.decomposition import PCA
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FundamentalDataFixer:
    """æ•°æ®è´¨é‡æ ¹æœ¬æ€§ä¿®å¤å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.scaler = StandardScaler()
        self.pca = None
        self.feature_importance = None
        
    def fundamental_fix(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """æ ¹æœ¬æ€§ä¿®å¤æ•°æ®è´¨é‡"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ•°æ®è´¨é‡æ ¹æœ¬æ€§ä¿®å¤...")
            logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: X={X.shape}, y={y.shape}")
            
            # æ­¥éª¤1: æ•°æ®åˆ†å¸ƒè¯Šæ–­
            logger.info("ğŸ” æ­¥éª¤1: æ•°æ®åˆ†å¸ƒè¯Šæ–­...")
            distribution_analysis = self._analyze_distributions(X, y)
            
            # æ­¥éª¤2: ç‰¹å¾ç›¸å…³æ€§åˆ†æ
            logger.info("ğŸ” æ­¥éª¤2: ç‰¹å¾ç›¸å…³æ€§åˆ†æ...")
            correlation_analysis = self._analyze_correlations(X, y)
            
            # æ­¥éª¤3: ç‰¹å¾é‡è¦æ€§åˆ†æ
            logger.info("ğŸ” æ­¥éª¤3: ç‰¹å¾é‡è¦æ€§åˆ†æ...")
            importance_analysis = self._analyze_feature_importance(X, y)
            
            # æ­¥éª¤4: æ•°æ®é‡æ„
            logger.info("ğŸ”§ æ­¥éª¤4: æ•°æ®é‡æ„...")
            X_reconstructed, y_reconstructed = self._reconstruct_data(X, y, importance_analysis)
            
            # æ­¥éª¤5: éªŒè¯ä¿®å¤æ•ˆæœ
            logger.info("ğŸ” æ­¥éª¤5: éªŒè¯ä¿®å¤æ•ˆæœ...")
            validation_result = self._validate_fix(X_reconstructed, y_reconstructed)
            
            result = {
                'status': 'success',
                'X_fixed': X_reconstructed,
                'y_fixed': y_reconstructed,
                'distribution_analysis': distribution_analysis,
                'correlation_analysis': correlation_analysis,
                'importance_analysis': importance_analysis,
                'validation_result': validation_result,
                'original_shape': X.shape,
                'fixed_shape': X_reconstructed.shape
            }
            
            logger.info("ğŸ‰ æ•°æ®è´¨é‡æ ¹æœ¬æ€§ä¿®å¤å®Œæˆï¼")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡æ ¹æœ¬æ€§ä¿®å¤å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _analyze_distributions(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        try:
            analysis = {}
            
            # ç›®æ ‡å˜é‡åˆ†å¸ƒ
            y_stats = {
                'mean': np.mean(y),
                'std': np.std(y),
                'min': np.min(y),
                'max': np.max(y),
                'skewness': stats.skew(y),
                'kurtosis': stats.kurtosis(y)
            }
            analysis['target_distribution'] = y_stats
            
            # ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
            feature_stats = []
            for i in range(X.shape[1]):
                feature_data = X[:, i]
                feature_stats.append({
                    'feature_id': i,
                    'mean': np.mean(feature_data),
                    'std': np.std(feature_data),
                    'min': np.min(feature_data),
                    'max': np.max(feature_data),
                    'skewness': stats.skew(feature_data),
                    'kurtosis': stats.kurtosis(feature_data),
                    'missing_ratio': np.sum(np.isnan(feature_data)) / len(feature_data)
                })
            
            analysis['feature_distributions'] = feature_stats
            
            # æ•°æ®è´¨é‡é—®é¢˜è¯†åˆ«
            issues = []
            
            # æ£€æŸ¥ç›®æ ‡å˜é‡æ–¹å·®
            if np.var(y) < 1e-6:
                issues.append({
                    'type': 'low_target_variance',
                    'severity': 'critical',
                    'description': f'ç›®æ ‡å˜é‡æ–¹å·®è¿‡ä½: {np.var(y):.6f}',
                    'recommendation': 'ç›®æ ‡å˜é‡ç¼ºä¹å˜åŒ–ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆé¢„æµ‹'
                })
            
            # æ£€æŸ¥ç‰¹å¾æ–¹å·®
            low_variance_features = [i for i, stats in enumerate(feature_stats) if stats['std'] < 1e-6]
            if low_variance_features:
                issues.append({
                    'type': 'low_feature_variance',
                    'severity': 'high',
                    'description': f'å‘ç° {len(low_variance_features)} ä¸ªä½æ–¹å·®ç‰¹å¾',
                    'features': low_variance_features,
                    'recommendation': 'ç§»é™¤ä½æ–¹å·®ç‰¹å¾'
                })
            
            analysis['issues'] = issues
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_correlations(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """åˆ†æç‰¹å¾ç›¸å…³æ€§"""
        try:
            analysis = {}
            
            # ç‰¹å¾é—´ç›¸å…³æ€§
            feature_corr = np.corrcoef(X.T)
            analysis['feature_correlation_matrix'] = feature_corr.tolist()
            
            # ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
            target_correlations = []
            for i in range(X.shape[1]):
                corr = np.corrcoef(X[:, i], y)[0, 1]
                if not np.isnan(corr):
                    target_correlations.append({
                        'feature_id': i,
                        'correlation': corr,
                        'abs_correlation': abs(corr)
                    })
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            target_correlations.sort(key=lambda x: x['abs_correlation'], reverse=True)
            analysis['target_correlations'] = target_correlations
            
            # é«˜ç›¸å…³ç‰¹å¾å¯¹
            high_corr_pairs = []
            for i in range(X.shape[1]):
                for j in range(i+1, X.shape[1]):
                    corr = feature_corr[i, j]
                    if abs(corr) > 0.8:
                        high_corr_pairs.append({
                            'feature1': i,
                            'feature2': j,
                            'correlation': corr
                        })
            
            analysis['high_correlation_pairs'] = high_corr_pairs
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_feature_importance(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        try:
            analysis = {}
            
            # ä½¿ç”¨äº’ä¿¡æ¯
            try:
                mi_scores = mutual_info_regression(X, y, random_state=42)
                mi_features = [(i, score) for i, score in enumerate(mi_scores)]
                mi_features.sort(key=lambda x: x[1], reverse=True)
                analysis['mutual_info_scores'] = mi_features
            except:
                analysis['mutual_info_scores'] = []
            
            # ä½¿ç”¨Fæ£€éªŒ
            try:
                f_scores, _ = f_regression(X, y)
                f_features = [(i, score) for i, score in enumerate(f_scores)]
                f_features.sort(key=lambda x: x[1], reverse=True)
                analysis['f_scores'] = f_features
            except:
                analysis['f_features'] = []
            
            # ç»¼åˆé‡è¦æ€§è¯„åˆ†
            if analysis['mutual_info_scores'] and analysis['f_scores']:
                # å½’ä¸€åŒ–åˆ†æ•°
                mi_max = max(score for _, score in analysis['mutual_info_scores'])
                f_max = max(score for _, score in analysis['f_scores'])
                
                combined_scores = []
                for i in range(X.shape[1]):
                    mi_score = next(score for feat_id, score in analysis['mutual_info_scores'] if feat_id == i)
                    f_score = next(score for feat_id, score in analysis['f_scores'] if feat_id == i)
                    
                    # å½’ä¸€åŒ–å¹¶ç»„åˆ
                    mi_norm = mi_score / mi_max if mi_max > 0 else 0
                    f_norm = f_score / f_max if f_max > 0 else 0
                    combined = (mi_norm + f_norm) / 2
                    
                    combined_scores.append((i, combined))
                
                combined_scores.sort(key=lambda x: x[1], reverse=True)
                analysis['combined_importance'] = combined_scores
            else:
                analysis['combined_importance'] = []
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _reconstruct_data(self, X: np.ndarray, y: np.ndarray, importance_analysis: Dict) -> Tuple[np.ndarray, np.ndarray]:
        """é‡æ„æ•°æ®"""
        try:
            # åŸºäºé‡è¦æ€§é€‰æ‹©ç‰¹å¾
            if importance_analysis.get('combined_importance'):
                # é€‰æ‹©å‰5ä¸ªæœ€é‡è¦çš„ç‰¹å¾
                top_features = [feat_id for feat_id, _ in importance_analysis['combined_importance'][:5]]
                X_selected = X[:, top_features]
                logger.info(f"âœ… åŸºäºé‡è¦æ€§é€‰æ‹©ç‰¹å¾: {len(top_features)} ä¸ª")
            else:
                # å¦‚æœæ— æ³•è®¡ç®—é‡è¦æ€§ï¼Œä½¿ç”¨ç›¸å…³æ€§
                target_corr = importance_analysis.get('target_correlations', [])
                if target_corr:
                    top_features = [feat['feature_id'] for feat in target_corr[:5]]
                    X_selected = X[:, top_features]
                    logger.info(f"âœ… åŸºäºç›¸å…³æ€§é€‰æ‹©ç‰¹å¾: {len(top_features)} ä¸ª")
                else:
                    # æœ€åæ‰‹æ®µï¼šéšæœºé€‰æ‹©
                    top_features = list(range(min(5, X.shape[1])))
                    X_selected = X[:, top_features]
                    logger.info(f"âš ï¸ éšæœºé€‰æ‹©ç‰¹å¾: {len(top_features)} ä¸ª")
            
            # æ•°æ®æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X_selected)
            
            # å¼‚å¸¸å€¼å¤„ç†
            X_clean, y_clean = self._remove_outliers(X_scaled, y)
            
            # æ•°æ®å¢å¼º
            X_augmented, y_augmented = self._augment_data(X_clean, y_clean, target_samples=100)
            
            return X_augmented, y_augmented
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é‡æ„å¤±è´¥: {e}")
            return X, y
    
    def _remove_outliers(self, X: np.ndarray, y: np.ndarray, threshold: float = 2.0) -> Tuple[np.ndarray, np.ndarray]:
        """ç§»é™¤å¼‚å¸¸å€¼"""
        try:
            # ä½¿ç”¨Z-scoreæ–¹æ³•
            z_scores = np.abs(stats.zscore(X))
            outlier_mask = np.any(z_scores > threshold, axis=1)
            
            X_clean = X[~outlier_mask]
            y_clean = y[~outlier_mask]
            
            logger.info(f"âœ… å¼‚å¸¸å€¼å¤„ç†: {len(y)} -> {len(y_clean)} æ ·æœ¬")
            return X_clean, y_clean
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸å€¼å¤„ç†å¤±è´¥: {e}")
            return X, y
    
    def _augment_data(self, X: np.ndarray, y: np.ndarray, target_samples: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """æ•°æ®å¢å¼º"""
        try:
            if len(X) >= target_samples:
                return X, y
            
            # é€šè¿‡æ’å€¼å’Œå™ªå£°å¢åŠ æ ·æœ¬
            additional_samples = target_samples - len(X)
            
            X_augmented = []
            y_augmented = []
            
            for i in range(additional_samples):
                # éšæœºé€‰æ‹©ä¸¤ä¸ªç°æœ‰æ ·æœ¬
                idx1, idx2 = np.random.choice(len(X), 2, replace=False)
                
                # æ’å€¼æƒé‡
                alpha = np.random.random()
                
                # æ’å€¼
                X_interp = alpha * X[idx1] + (1 - alpha) * X[idx2]
                y_interp = alpha * y[idx1] + (1 - alpha) * y[idx2]
                
                # æ·»åŠ å°‘é‡å™ªå£°
                noise_scale = 0.01
                X_noise = X_interp + np.random.normal(0, noise_scale, X_interp.shape)
                y_noise = y_interp + np.random.normal(0, noise_scale)
                
                X_augmented.append(X_noise)
                y_augmented.append(y_noise)
            
            # åˆå¹¶æ•°æ®
            X_final = np.vstack([X, np.array(X_augmented)])
            y_final = np.hstack([y, np.array(y_augmented)])
            
            logger.info(f"âœ… æ•°æ®å¢å¼º: {len(y)} -> {len(y_final)} æ ·æœ¬")
            return X_final, y_final
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¢å¼ºå¤±è´¥: {e}")
            return X, y
    
    def _validate_fix(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """éªŒè¯ä¿®å¤æ•ˆæœ"""
        try:
            validation = {}
            
            # æ•°æ®è´¨é‡æŒ‡æ ‡
            validation['feature_count'] = X.shape[1]
            validation['sample_count'] = len(y)
            validation['target_variance'] = np.var(y)
            validation['feature_variance'] = [np.var(X[:, i]) for i in range(X.shape[1])]
            
            # ç›¸å…³æ€§æ£€æŸ¥
            target_correlations = []
            for i in range(X.shape[1]):
                corr = np.corrcoef(X[:, i], y)[0, 1]
                if not np.isnan(corr):
                    target_correlations.append(abs(corr))
            
            validation['avg_target_correlation'] = np.mean(target_correlations) if target_correlations else 0
            validation['max_target_correlation'] = np.max(target_correlations) if target_correlations else 0
            
            # è´¨é‡è¯„åˆ†
            quality_score = 0
            if validation['target_variance'] > 1e-6:
                quality_score += 0.3
            if validation['avg_target_correlation'] > 0.1:
                quality_score += 0.3
            if validation['sample_count'] >= 50:
                quality_score += 0.2
            if validation['feature_count'] <= 10:
                quality_score += 0.2
            
            validation['quality_score'] = quality_score
            
            return validation
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e)}

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æ•°æ®è´¨é‡æ ¹æœ¬æ€§ä¿®å¤...")
        
        # åŠ è½½ERA5æ•°æ®
        from src.models.agriculture.era5_soil_moisture_predictor import ERA5SoilMoisturePredictor
        
        predictor = ERA5SoilMoisturePredictor()
        data = predictor.load_data()
        
        # å±•å¹³æ•°æ®
        X_train = data['X_train'].reshape(-1, data['X_train'].shape[-1])
        y_train = np.repeat(data['y_train'], data['X_train'].shape[1])
        
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: X={X_train.shape}, y={y_train.shape}")
        
        # æ ¹æœ¬æ€§ä¿®å¤
        fixer = FundamentalDataFixer()
        result = fixer.fundamental_fix(X_train, y_train)
        
        if result['status'] != 'success':
            logger.error(f"âŒ æ ¹æœ¬æ€§ä¿®å¤å¤±è´¥: {result}")
            return
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        output_dir = "data/processed/era5_fundamentally_fixed"
        os.makedirs(output_dir, exist_ok=True)
        
        np.save(os.path.join(output_dir, 'X_fundamentally_fixed.npy'), result['X_fixed'])
        np.save(os.path.join(output_dir, 'y_fundamentally_fixed.npy'), result['y_fixed'])
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_file = f"fundamental_fix_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"âœ… ä¿®å¤åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
        logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        validation = result['validation_result']
        logger.info(f"ğŸ“Š ä¿®å¤æ•ˆæœ:")
        logger.info(f"  ç‰¹å¾æ•°é‡: {result['original_shape'][1]} -> {result['fixed_shape'][1]}")
        logger.info(f"  æ ·æœ¬æ•°é‡: {result['original_shape'][0]} -> {result['fixed_shape'][0]}")
        logger.info(f"  ç›®æ ‡æ–¹å·®: {validation['target_variance']:.6f}")
        logger.info(f"  å¹³å‡ç›¸å…³æ€§: {validation['avg_target_correlation']:.4f}")
        logger.info(f"  è´¨é‡è¯„åˆ†: {validation['quality_score']:.2f}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e)}

if __name__ == "__main__":
    main()
