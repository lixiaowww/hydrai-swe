#!/usr/bin/env python3
"""
HydrAI-SWE æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨PyTorchç›´æ¥è®­ç»ƒLSTMæ¨¡å‹è¿›è¡ŒSWEé¢„æµ‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from datetime import datetime
import os

class SWELSTMModel(nn.Module):
    """SWEé¢„æµ‹LSTMæ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(SWELSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch_size, seq_length, input_size)
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        output = self.fc(self.dropout(last_output))
        return output

def load_and_prepare_data(data_path, sequence_length=30):
    """åŠ è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_path, parse_dates=['date'])
    df.set_index('date', inplace=True)
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_columns = [
        'snow_depth_mm', 'snow_fall_mm', 'snow_water_equivalent_mm',
        'day_of_year', 'month', 'year'
    ]
    target_column = 'streamflow_m3s'
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"ç‰¹å¾åˆ—: {feature_columns}")
    print(f"ç›®æ ‡åˆ—: {target_column}")
    
    # å¤„ç†ç¼ºå¤±å€¼
    df = df.fillna(method='ffill').fillna(0)
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    X, y = [], []
    for i in range(sequence_length, len(df)):
        X.append(df[feature_columns].iloc[i-sequence_length:i].values)
        y.append(df[target_column].iloc[i])
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"åºåˆ—æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    # é‡å¡‘Xè¿›è¡Œæ ‡å‡†åŒ–
    X_reshaped = X.reshape(-1, X.shape[-1])
    X_scaled = scaler_X.fit_transform(X_reshaped)
    X_scaled = X_scaled.reshape(X.shape)
    
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    
    return X_scaled, y_scaled, scaler_X, scaler_y

def train_model(X_train, y_train, X_val, y_val, input_size, hidden_size=64, 
                num_layers=2, epochs=100, learning_rate=0.001):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸ¤– å¼€å§‹è®­ç»ƒSWEé¢„æµ‹æ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = SWELSTMModel(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        output_size=1
    )
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        model.train()
        
        # å‰å‘ä¼ æ’­
        outputs = model(torch.FloatTensor(X_train))
        loss = criterion(outputs.squeeze(), torch.FloatTensor(y_train))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_outputs = model(torch.FloatTensor(X_val))
            val_loss = criterion(val_outputs.squeeze(), torch.FloatTensor(y_val))
        
        train_losses.append(loss.item())
        val_losses.append(val_loss.item())
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    return model, train_losses, val_losses

def evaluate_model(model, X_test, y_test, scaler_y):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    model.eval()
    with torch.no_grad():
        predictions = model(torch.FloatTensor(X_test))
        predictions = predictions.squeeze().numpy()
    
    # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
    predictions_original = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()
    y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
    
    # è®¡ç®—æŒ‡æ ‡
    mse = mean_squared_error(y_test_original, predictions_original)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test_original, predictions_original)
    r2 = r2_score(y_test_original, predictions_original)
    
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
    print(f"  MSE: {mse:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RÂ²: {r2:.4f}")
    
    return predictions_original, y_test_original, {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}

def plot_training_history(train_losses, val_losses, save_path=None):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.7)
    plt.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.7)
    plt.xlabel('è®­ç»ƒè½®æ•°')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('SWEæ¨¡å‹è®­ç»ƒå†å²')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š è®­ç»ƒå†å²å›¾ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

def plot_predictions(predictions, actual, save_path=None):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    plt.figure(figsize=(12, 6))
    
    # é€‰æ‹©å‰100ä¸ªç‚¹è¿›è¡Œå¯è§†åŒ–
    n_points = min(100, len(predictions))
    x = range(n_points)
    
    plt.plot(x, actual[:n_points], label='å®é™…å€¼', alpha=0.7, linewidth=2)
    plt.plot(x, predictions[:n_points], label='é¢„æµ‹å€¼', alpha=0.7, linewidth=2)
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('æµé‡ (mÂ³/s)')
    plt.title('SWEæ¨¡å‹é¢„æµ‹ç»“æœ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š é¢„æµ‹ç»“æœå›¾ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¼€å§‹HydrAI-SWEæ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # æ•°æ®è·¯å¾„
    data_path = "src/neuralhydrology/data/red_river_basin/timeseries.csv"
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬")
        return
    
    # è®­ç»ƒå‚æ•°
    sequence_length = 30
    hidden_size = 64
    num_layers = 2
    epochs = 100
    learning_rate = 0.001
    
    print(f"ğŸ”§ è®­ç»ƒå‚æ•°:")
    print(f"  åºåˆ—é•¿åº¦: {sequence_length}")
    print(f"  éšè—å±‚å¤§å°: {hidden_size}")
    print(f"  LSTMå±‚æ•°: {num_layers}")
    print(f"  è®­ç»ƒè½®æ•°: {epochs}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    
    # åŠ è½½å’Œå‡†å¤‡æ•°æ®
    X, y, scaler_X, scaler_y = load_and_prepare_data(data_path, sequence_length)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))
    
    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # è®­ç»ƒæ¨¡å‹
    input_size = X.shape[-1]
    model, train_losses, val_losses = train_model(
        X_train, y_train, X_val, y_val,
        input_size, hidden_size, num_layers, epochs, learning_rate
    )
    
    # è¯„ä¼°æ¨¡å‹
    predictions, actual, metrics = evaluate_model(model, X_test, y_test, scaler_y)
    
    # ä¿å­˜æ¨¡å‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_save_path = f"models/swe_lstm_model_{timestamp}.pth"
    os.makedirs("models", exist_ok=True)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        'sequence_length': sequence_length,
        'input_size': input_size,
        'hidden_size': hidden_size,
        'num_layers': num_layers,
        'metrics': metrics
    }, model_save_path)
    
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_save_path}")
    
    # ç»˜åˆ¶ç»“æœ
    plot_training_history(train_losses, val_losses, f"models/training_history_{timestamp}.png")
    plot_predictions(predictions, actual, f"models/predictions_{timestamp}.png")
    
    print("\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆæ€§èƒ½: RMSE={metrics['rmse']:.4f}, RÂ²={metrics['r2']:.4f}")

if __name__ == "__main__":
    main()
