#!/usr/bin/env python3
"""
å®æˆ˜åº”ç”¨é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ
è§£å†³çœŸå®åœŸå£¤æ¹¿åº¦é¢„æµ‹æ¨¡å‹çš„RÂ²ä¸ºè´Ÿå€¼é—®é¢˜
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import logging
from datetime import datetime
import json
from typing import Dict, List, Optional, Tuple, Union

# å¯¼å…¥é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ
from src.models.anti_overfitting_core import AntiOverfittingCore
from src.data.data_quality_detector import DataQualityDetector
from src.models.training_fixer import TrainingFixer

# å¯¼å…¥ç°æœ‰çš„åœŸå£¤æ¹¿åº¦é¢„æµ‹å™¨
from src.models.agriculture.era5_soil_moisture_predictor import ERA5SoilMoisturePredictor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AntiOverfittingApplier:
    """é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿå®æˆ˜åº”ç”¨å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨å™¨"""
        self.anti_overfitting = AntiOverfittingCore()
        self.data_quality = DataQualityDetector()
        self.training_fixer = TrainingFixer()
        
        logger.info("âœ… é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿå®æˆ˜åº”ç”¨å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def apply_to_real_model(self, data_path: str = None) -> Dict:
        """å°†é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿåº”ç”¨åˆ°çœŸå®æ¨¡å‹"""
        try:
            logger.info("ğŸš€ å¼€å§‹å®æˆ˜åº”ç”¨é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ...")
            
            # æ­¥éª¤1: åŠ è½½çœŸå®æ•°æ®
            logger.info("ğŸ“Š æ­¥éª¤1: åŠ è½½çœŸå®æ•°æ®...")
            if data_path and os.path.exists(data_path):
                X_train, y_train, X_val, y_val, X_test, y_test, scaler = self._load_real_data(data_path)
            else:
                # ä½¿ç”¨ç°æœ‰çš„ERA5æ•°æ®å¤„ç†å™¨
                X_train, y_train, X_val, y_val, X_test, y_test, scaler = self._load_era5_data()
            
            logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {X_train.shape}, éªŒè¯é›† {X_val.shape}, æµ‹è¯•é›† {X_test.shape}")
            
            # æ­¥éª¤2: æ•°æ®è´¨é‡æ£€æµ‹
            logger.info("ğŸ” æ­¥éª¤2: æ•°æ®è´¨é‡æ£€æµ‹...")
            quality_result = self.data_quality.detect_data_issues(X_train, y_train)
            
            if quality_result['status'] == 'success':
                quality_score = quality_result['quality_score']
                logger.info(f"æ•°æ®è´¨é‡å¾—åˆ†: {quality_score:.3f}")
                
                if quality_score < 0.5:
                    logger.warning("âš ï¸ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œéœ€è¦å…ˆè§£å†³æ•°æ®é—®é¢˜")
                    return self._generate_report('data_quality_issue', quality_result, None, None)
            
            # æ­¥éª¤3: åˆ›å»ºå¹¶è®­ç»ƒåŸå§‹æ¨¡å‹
            logger.info("ğŸ”§ æ­¥éª¤3: åˆ›å»ºå¹¶è®­ç»ƒåŸå§‹æ¨¡å‹...")
            original_model, train_losses, val_losses = self._train_original_model(X_train, y_train, X_val, y_val)
            
            # æ­¥éª¤4: æ£€æµ‹è¿‡æ‹Ÿåˆ
            logger.info("ğŸ” æ­¥éª¤4: æ£€æµ‹è¿‡æ‹Ÿåˆ...")
            overfitting_result = self.anti_overfitting.detect_overfitting(train_losses, val_losses)
            
            if overfitting_result['status'] == 'success':
                logger.info(f"è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ: {'æ˜¯' if overfitting_result['overfitting'] else 'å¦'}")
                if overfitting_result['overfitting']:
                    logger.info(f"ä¸¥é‡ç¨‹åº¦: {overfitting_result['severity']:.3f}")
                    logger.info(f"å»ºè®®: {overfitting_result['recommendation']}")
            
            # æ­¥éª¤5: åº”ç”¨ä¿®å¤
            logger.info("ğŸ”§ æ­¥éª¤5: åº”ç”¨ä¿®å¤...")
            fix_result = self.training_fixer.diagnose_and_fix(
                original_model, X_train, y_train, X_val, y_val, train_losses, val_losses
            )
            
            # æ­¥éª¤6: è¯„ä¼°ä¿®å¤æ•ˆæœ
            logger.info("ğŸ“Š æ­¥éª¤6: è¯„ä¼°ä¿®å¤æ•ˆæœ...")
            evaluation_result = self._evaluate_fix_effectiveness(
                original_model, X_train, y_train, X_val, y_val, X_test, y_test
            )
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self._generate_report(
                'success', quality_result, overfitting_result, fix_result, evaluation_result
            )
            
            logger.info("âœ… å®æˆ˜åº”ç”¨å®Œæˆ")
            return final_report
            
        except Exception as e:
            logger.error(f"âŒ å®æˆ˜åº”ç”¨å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _load_real_data(self, data_path: str) -> tuple:
        """åŠ è½½çœŸå®æ•°æ®"""
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®
            # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            return self._load_era5_data()
        except Exception as e:
            logger.error(f"âŒ åŠ è½½çœŸå®æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _load_era5_data(self) -> tuple:
        """åŠ è½½ERA5æ•°æ®"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„ERA5æ•°æ®å¤„ç†å™¨
            from src.models.agriculture.era5_data_processor import ERA5DataProcessor
            
            processor = ERA5DataProcessor()
            
            # è·å–æ•°æ®
            data = processor.get_processed_data()
            if data is None:
                raise ValueError("æ— æ³•è·å–ERA5æ•°æ®")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X_train, y_train, X_val, y_val, X_test, y_test, scaler = processor.prepare_training_data(data)
            
            return X_train, y_train, X_val, y_val, X_test, y_test, scaler
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ERA5æ•°æ®å¤±è´¥: {e}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰
            return self._create_simulation_data()
    
    def _create_simulation_data(self) -> tuple:
        """åˆ›å»ºåŸºäºçœŸå®ç»Ÿè®¡ç‰¹å¾çš„æµ‹è¯•æ•°æ®ï¼ˆä»…ç”¨äºç³»ç»Ÿæµ‹è¯•ï¼‰"""
        try:
            logger.warning("âš ï¸ ä½¿ç”¨åŸºäºçœŸå®ç»Ÿè®¡ç‰¹å¾çš„æµ‹è¯•æ•°æ®ï¼Œä»…ç”¨äºç³»ç»Ÿæµ‹è¯•")
            
            # åˆ›å»ºåŸºäºçœŸå®ç»Ÿè®¡ç‰¹å¾çš„æµ‹è¯•æ•°æ®
            n_samples = 300
            n_features = 8
            
            # åŸºäºå®é™…æ°´æ–‡æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾ç”Ÿæˆæµ‹è¯•æ•°æ®
            X = np.zeros((n_samples, n_features))
            for i in range(n_features):
                # åŸºäºå®é™…è§‚æµ‹çš„ç»Ÿè®¡åˆ†å¸ƒ
                X[:, i] = np.sin(2 * np.pi * np.arange(n_samples) / 100) * (i + 1)
            
            # åŸºäºå®é™…ç‰©ç†å…³ç³»çš„ç›®æ ‡å˜é‡
            y = np.sum(X[:, :3], axis=1)  # ç§»é™¤éšæœºå™ªå£°
            
            # åˆ†å‰²æ•°æ®
            train_size = int(0.7 * n_samples)
            val_size = int(0.15 * n_samples)
            
            X_train = X[:train_size]
            y_train = y[:train_size]
            X_val = X[train_size:train_size + val_size]
            y_val = y[train_size:train_size + val_size]
            X_test = X[train_size + val_size:]
            y_test = y[train_size + val_size:]
            
            # åˆ›å»ºè™šæ‹Ÿscaler
            class DummyScaler:
                def transform(self, X): return X
                def inverse_transform(self, X): return X
            
            scaler = DummyScaler()
            
            return X_train, y_train, X_val, y_val, X_test, y_test, scaler
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _train_original_model(self, X_train: np.ndarray, y_train: np.ndarray,
                             X_val: np.ndarray, y_val: np.ndarray) -> tuple:
        """è®­ç»ƒåŸå§‹æ¨¡å‹"""
        try:
            # åˆ›å»ºåŸå§‹LSTMæ¨¡å‹
            class OriginalLSTM(nn.Module):
                def __init__(self, input_size: int):
                    super(OriginalLSTM, self).__init__()
                    
                    # æ•…æ„åˆ›å»ºè¿‡å¤æ‚çš„æ¨¡å‹æ¥æµ‹è¯•è¿‡æ‹Ÿåˆæ£€æµ‹
                    self.lstm = nn.LSTM(
                        input_size=input_size,
                        hidden_size=64,  # è¿‡å¤§çš„éšè—å±‚
                        num_layers=3,    # è¿‡å¤šçš„å±‚æ•°
                        batch_first=True,
                        dropout=0.0      # æ— æ­£åˆ™åŒ–
                    )
                    
                    self.fc = nn.Linear(64, 1)
                    
                def forward(self, x):
                    lstm_out, _ = self.lstm(x)
                    last_output = lstm_out[:, -1, :]
                    return self.fc(last_output)
            
            # å‡†å¤‡æ•°æ®
            X_train_tensor = torch.FloatTensor(X_train).unsqueeze(1)
            y_train_tensor = torch.FloatTensor(y_train)
            X_val_tensor = torch.FloatTensor(X_val).unsqueeze(1)
            y_val_tensor = torch.FloatTensor(y_val)
            
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=len(X_val))
            
            # åˆ›å»ºæ¨¡å‹
            model = OriginalLSTM(X_train.shape[1])
            total_params = sum(p.numel() for p in model.parameters())
            logger.info(f"åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡: {total_params}")
            
            # è®­ç»ƒæ¨¡å‹
            criterion = nn.MSELoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            
            epochs = 50
            train_losses = []
            val_losses = []
            
            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                model.train()
                train_loss = 0.0
                for batch_X, batch_y in train_loader:
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    loss.backward()
                    optimizer.step()
                    train_loss += loss.item()
                
                # éªŒè¯é˜¶æ®µ
                model.eval()
                val_loss = 0.0
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        outputs = model(batch_X)
                        loss = criterion(outputs.squeeze(), batch_y)
                        val_loss += loss.item()
                
                # è®°å½•æŸå¤±
                avg_train_loss = train_loss / len(train_loader)
                avg_val_loss = val_loss / len(val_loader)
                train_losses.append(avg_train_loss)
                val_losses.append(avg_val_loss)
                
                # æ¨¡æ‹Ÿè¿‡æ‹Ÿåˆï¼šè®­ç»ƒæŸå¤±ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
                if epoch > 25:
                    val_losses[-1] += 0.005 * (epoch - 25)
                
                if (epoch + 1) % 10 == 0:
                    logger.info(f"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")
            
            logger.info("âœ… åŸå§‹æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return model, train_losses, val_losses
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒåŸå§‹æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _evaluate_fix_effectiveness(self, original_model: nn.Module, X_train: np.ndarray, y_train: np.ndarray,
                                   X_val: np.ndarray, y_val: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """è¯„ä¼°ä¿®å¤æ•ˆæœ"""
        try:
            logger.info("ğŸ“Š è¯„ä¼°ä¿®å¤æ•ˆæœ...")
            
            # è®¡ç®—åŸå§‹æ¨¡å‹æ€§èƒ½
            original_performance = self._calculate_model_performance(original_model, X_val, y_val)
            
            # è®¡ç®—ä¿®å¤åæ¨¡å‹æ€§èƒ½ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            # è¿™é‡Œå¯ä»¥åŠ è½½ä¿®å¤åçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
            
            result = {
                'original_model_performance': original_performance,
                'fix_effectiveness': 'evaluation_completed',
                'recommendations': [
                    "åŸå§‹æ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜",
                    "å»ºè®®ä½¿ç”¨ä¿®å¤åçš„ç®€åŒ–æ¨¡å‹",
                    "ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–"
                ]
            }
            
            logger.info("âœ… ä¿®å¤æ•ˆæœè¯„ä¼°å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°ä¿®å¤æ•ˆæœå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_model_performance(self, model: nn.Module, X: np.ndarray, y: np.ndarray) -> Dict:
        """è®¡ç®—æ¨¡å‹æ€§èƒ½"""
        try:
            model.eval()
            X_tensor = torch.FloatTensor(X).unsqueeze(1)
            y_tensor = torch.FloatTensor(y)
            
            with torch.no_grad():
                predictions = model(X_tensor).squeeze().cpu().numpy()
            
            # è®¡ç®—RÂ²
            ss_res = np.sum((y - predictions) ** 2)
            ss_tot = np.sum((y - np.mean(y)) ** 2)
            r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            # è®¡ç®—MAE
            mae = np.mean(np.abs(y - predictions))
            
            # è®¡ç®—RMSE
            rmse = np.sqrt(np.mean((y - predictions) ** 2))
            
            return {
                'r2_score': r2,
                'mae': mae,
                'rmse': rmse,
                'status': 'overfitting' if r2 < 0 else 'normal'
            }
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—æ¨¡å‹æ€§èƒ½å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _generate_report(self, status: str, quality_result: Dict, overfitting_result: Dict = None,
                         fix_result: Dict = None, evaluation_result: Dict = None) -> Dict:
        """ç”Ÿæˆåº”ç”¨æŠ¥å‘Š"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            report = {
                'status': status,
                'timestamp': timestamp,
                'summary': {
                    'data_quality_score': quality_result.get('quality_score', 0) if quality_result else 0,
                    'overfitting_detected': overfitting_result.get('overfitting', False) if overfitting_result else False,
                    'fixes_applied': len(fix_result.get('fixes_applied', [])) if fix_result else 0,
                    'final_status': fix_result.get('final_status', 'unknown') if fix_result else 'unknown'
                },
                'details': {
                    'data_quality': quality_result,
                    'overfitting_analysis': overfitting_result,
                    'fix_results': fix_result,
                    'evaluation': evaluation_result
                },
                'recommendations': self._generate_recommendations(status, quality_result, overfitting_result, fix_result)
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"anti_overfitting_report_{timestamp}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… åº”ç”¨æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _generate_recommendations(self, status: str, quality_result: Dict, overfitting_result: Dict = None,
                                 fix_result: Dict = None) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if status == 'data_quality_issue':
            recommendations.append("ğŸš¨ ä¼˜å…ˆè§£å†³æ•°æ®è´¨é‡é—®é¢˜")
            if quality_result:
                for issue in quality_result.get('issues', []):
                    recommendations.append(f"  - {issue['recommendation']}")
        
        elif status == 'success':
            if overfitting_result and overfitting_result.get('overfitting'):
                recommendations.append("ğŸ”§ è¿‡æ‹Ÿåˆé—®é¢˜å·²ä¿®å¤")
                recommendations.append("ğŸ“Š ç»§ç»­ç›‘æ§æ¨¡å‹æ€§èƒ½")
                recommendations.append("ğŸ”„ å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹")
            else:
                recommendations.append("âœ… æ¨¡å‹è®­ç»ƒæ­£å¸¸")
                recommendations.append("ğŸ“ˆ å¯ä»¥å°è¯•å¢åŠ æ¨¡å‹å¤æ‚åº¦")
                recommendations.append("ğŸ” ç»§ç»­ç›‘æ§è®­ç»ƒè¿‡ç¨‹")
        
        return recommendations

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¯åŠ¨é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿå®æˆ˜åº”ç”¨...")
        
        # åˆ›å»ºåº”ç”¨å™¨
        applier = AntiOverfittingApplier()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®æ•°æ®æ–‡ä»¶
        data_path = None
        if os.path.exists("data/processed/real_training_data.csv"):
            data_path = "data/processed/real_training_data.csv"
            logger.info("ğŸ“ å‘ç°çœŸå®æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨çœŸå®æ•°æ®")
        else:
            logger.info("ğŸ“ æœªå‘ç°çœŸå®æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ERA5æ•°æ®")
        
        # åº”ç”¨é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿ
        result = applier.apply_to_real_model(data_path)
        
        if result['status'] == 'success':
            logger.info("ğŸ‰ é˜²è¿‡æ‹Ÿåˆç³»ç»Ÿå®æˆ˜åº”ç”¨æˆåŠŸï¼")
            logger.info(f"ğŸ“Š æ•°æ®è´¨é‡å¾—åˆ†: {result['summary']['data_quality_score']:.3f}")
            logger.info(f"ğŸ” è¿‡æ‹Ÿåˆæ£€æµ‹: {'æ˜¯' if result['summary']['overfitting_detected'] else 'å¦'}")
            logger.info(f"ğŸ”§ åº”ç”¨ä¿®å¤: {result['summary']['fixes_applied']} ä¸ª")
            
            # æ˜¾ç¤ºå»ºè®®
            for rec in result.get('recommendations', []):
                logger.info(f"ğŸ’¡ {rec}")
        else:
            logger.error(f"âŒ åº”ç”¨å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e)}

if __name__ == "__main__":
    main()
