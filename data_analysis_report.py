#!/usr/bin/env python3
"""
HydrAI-SWE æ•°æ®åˆ†æžæŠ¥å‘Š
Data Analysis Report for HydrAI-SWE Project
"""

import pandas as pd
import numpy as np
from pathlib import Path
import os

def analyze_data_availability():
    """åˆ†æžæ•°æ®å¯ç”¨æ€§"""
    
    print("ðŸ” HydrAI-SWE æ•°æ®å¯ç”¨æ€§åˆ†æž")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_files = {
        "ECCCç§¯é›ªæ•°æ®": "data/processed/eccc_manitoba_snow_processed.csv",
        "HYDATå¾„æµæ•°æ®": "data/processed/hydat_streamflow_processed.csv",
        "HYDATæ•°æ®åº“": "data/raw/Hydat_with_snow.sqlite3",
        "NASA MODIS": "data/raw/nasa_modis_snow/",
        "ECCCå¤©æ°”": "data/raw/eccc_grib/",
        "ECCCè¿‘æœŸ": "data/raw/eccc_recent/"
    }
    
    print("\nðŸ“Š æ•°æ®æ–‡ä»¶çŠ¶æ€:")
    print("-" * 40)
    
    available_data = {}
    for name, path in data_files.items():
        if os.path.exists(path):
            if os.path.isfile(path):
                size = os.path.getsize(path) / 1024  # KB
                print(f"âœ… {name}: {path} ({size:.1f} KB)")
                available_data[name] = path
            else:
                files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])
                print(f"âœ… {name}: {path} ({files} æ–‡ä»¶)")
                available_data[name] = path
        else:
            print(f"âŒ {name}: {path} (ä¸å­˜åœ¨)")
    
    return available_data

def analyze_training_data_volume():
    """åˆ†æžè®­ç»ƒæ•°æ®é‡"""
    
    print("\nðŸ“ˆ è®­ç»ƒæ•°æ®é‡åˆ†æž")
    print("=" * 60)
    
    # ECCCç§¯é›ªæ•°æ®
    eccc_file = "data/processed/eccc_manitoba_snow_processed.csv"
    if os.path.exists(eccc_file):
        df_eccc = pd.read_csv(eccc_file)
        print(f"\nðŸŒ¨ï¸ ECCCç§¯é›ªæ•°æ®:")
        print(f"   - æ€»è®°å½•æ•°: {len(df_eccc):,}")
        print(f"   - æ—¶é—´èŒƒå›´: {df_eccc['date'].min()} åˆ° {df_eccc['date'].max()}")
        print(f"   - ç«™ç‚¹æ•°é‡: {df_eccc['station_name'].nunique()}")
        
        # è®¡ç®—å¹´æ•°
        df_eccc['date'] = pd.to_datetime(df_eccc['date'])
        years = df_eccc['date'].dt.year.unique()
        print(f"   - è¦†ç›–å¹´ä»½: {len(years)} å¹´ ({min(years)}-{max(years)})")
        
        # è®¡ç®—æ¯æ—¥è®°å½•æ•°
        daily_records = df_eccc.groupby('date').size()
        print(f"   - å¹³å‡æ¯æ—¥è®°å½•: {daily_records.mean():.1f}")
        print(f"   - æ•°æ®å®Œæ•´æ€§: {len(daily_records)} å¤©")
    
    # HYDATå¾„æµæ•°æ®
    hydat_file = "data/processed/hydat_streamflow_processed.csv"
    if os.path.exists(hydat_file):
        df_hydat = pd.read_csv(hydat_file)
        print(f"\nðŸŒŠ HYDATå¾„æµæ•°æ®:")
        print(f"   - æ€»è®°å½•æ•°: {len(df_hydat):,}")
        print(f"   - æ—¶é—´èŒƒå›´: {df_hydat['date'].min()} åˆ° {df_hydat['date'].max()}")
        print(f"   - ç«™ç‚¹æ•°é‡: {len(df_hydat.columns) - 1}")  # å‡åŽ»dateåˆ—
        
        # è®¡ç®—å¹´æ•°
        df_hydat['date'] = pd.to_datetime(df_hydat['date'])
        years = df_hydat['date'].dt.year.unique()
        print(f"   - è¦†ç›–å¹´ä»½: {len(years)} å¹´ ({min(years)}-{max(years)})")

def calculate_training_capacity():
    """è®¡ç®—è®­ç»ƒèƒ½åŠ›"""
    
    print("\nðŸ¤– æ¨¡åž‹è®­ç»ƒèƒ½åŠ›åˆ†æž")
    print("=" * 60)
    
    # åŸºäºŽNeuralHydrologyé…ç½®
    config_info = {
        "è®­ç»ƒæœŸ": "1979-1995 (17å¹´)",
        "éªŒè¯æœŸ": "1996-1997 (1.5å¹´)", 
        "æµ‹è¯•æœŸ": "1997-1998 (1.5å¹´)",
        "åºåˆ—é•¿åº¦": "30å¤©",
        "æ‰¹æ¬¡å¤§å°": "16",
        "è®­ç»ƒè½®æ•°": "30",
        "éšè—å±‚å¤§å°": "64"
    }
    
    print("ðŸ“‹ NeuralHydrologyé…ç½®:")
    for key, value in config_info.items():
        print(f"   - {key}: {value}")
    
    # è®¡ç®—å®žé™…å¯ç”¨è®­ç»ƒæ•°æ®
    eccc_file = "data/processed/eccc_manitoba_snow_processed.csv"
    if os.path.exists(eccc_file):
        df_eccc = pd.read_csv(eccc_file)
        df_eccc['date'] = pd.to_datetime(df_eccc['date'])
        
        # 1979-1995å¹´æ•°æ®
        train_data = df_eccc[(df_eccc['date'].dt.year >= 1979) & (df_eccc['date'].dt.year <= 1995)]
        train_days = len(train_data['date'].dt.date.unique())
        
        print(f"\nðŸ“Š å®žé™…è®­ç»ƒæ•°æ®:")
        print(f"   - è®­ç»ƒæœŸå¤©æ•°: {train_days:,}")
        print(f"   - å¯ç”¨åºåˆ—æ•°: {max(0, train_days - 30):,}")  # å‡åŽ»åºåˆ—é•¿åº¦
        print(f"   - è®­ç»ƒæ‰¹æ¬¡: {max(0, (train_days - 30) // 16):,}")
        
        # éªŒè¯æ•°æ®
        val_data = df_eccc[(df_eccc['date'].dt.year >= 1996) & (df_eccc['date'].dt.year <= 1997)]
        val_days = len(val_data['date'].dt.date.unique())
        print(f"   - éªŒè¯æœŸå¤©æ•°: {val_days:,}")
        
        # æµ‹è¯•æ•°æ®
        test_data = df_eccc[(df_eccc['date'].dt.year >= 1997) & (df_eccc['date'].dt.year <= 1998)]
        test_days = len(test_data['date'].dt.date.unique())
        print(f"   - æµ‹è¯•æœŸå¤©æ•°: {test_days:,}")

def analyze_data_source_complementarity():
    """åˆ†æžæ•°æ®æºäº’è¡¥æ€§"""
    
    print("\nðŸ”„ æ•°æ®æºäº’è¡¥æ€§åˆ†æž")
    print("=" * 60)
    
    data_sources = {
        "NASA MODIS": {
            "æ•°æ®ç±»åž‹": "å«æ˜Ÿé¥æ„Ÿç§¯é›ª",
            "ç©ºé—´åˆ†è¾¨çŽ‡": "500m",
            "æ—¶é—´åˆ†è¾¨çŽ‡": "æ¯æ—¥",
            "è¦†ç›–èŒƒå›´": "å…¨çƒ",
            "ä¼˜åŠ¿": "å¤§èŒƒå›´è¦†ç›–ã€è¿žç»­è§‚æµ‹",
            "åŠ£åŠ¿": "äº‘å±‚é®æŒ¡ã€åœ°é¢éªŒè¯éœ€æ±‚",
            "äº’è¡¥æ€§": "æä¾›å¤§å°ºåº¦ç§¯é›ªåˆ†å¸ƒ"
        },
        "ECCCç§¯é›ª": {
            "æ•°æ®ç±»åž‹": "åœ°é¢è§‚æµ‹ç§¯é›ª",
            "ç©ºé—´åˆ†è¾¨çŽ‡": "ç«™ç‚¹çº§åˆ«",
            "æ—¶é—´åˆ†è¾¨çŽ‡": "æ¯æ—¥",
            "è¦†ç›–èŒƒå›´": "åŠ æ‹¿å¤§",
            "ä¼˜åŠ¿": "é«˜ç²¾åº¦ã€è¿žç»­è®°å½•",
            "åŠ£åŠ¿": "ç©ºé—´è¦†ç›–æœ‰é™",
            "äº’è¡¥æ€§": "æä¾›åœ°é¢çœŸå€¼éªŒè¯"
        },
        "ECCCå¤©æ°”": {
            "æ•°æ®ç±»åž‹": "æ•°å€¼å¤©æ°”é¢„æŠ¥",
            "ç©ºé—´åˆ†è¾¨çŽ‡": "15km",
            "æ—¶é—´åˆ†è¾¨çŽ‡": "3å°æ—¶",
            "è¦†ç›–èŒƒå›´": "åŠ æ‹¿å¤§",
            "ä¼˜åŠ¿": "æœªæ¥é¢„æµ‹ã€å¤šå˜é‡",
            "åŠ£åŠ¿": "é¢„æµ‹ä¸ç¡®å®šæ€§",
            "äº’è¡¥æ€§": "æä¾›æœªæ¥å¤©æ°”é©±åŠ¨"
        },
        "HYDAT": {
            "æ•°æ®ç±»åž‹": "æ°´æ–‡è§‚æµ‹",
            "ç©ºé—´åˆ†è¾¨çŽ‡": "ç«™ç‚¹çº§åˆ«",
            "æ—¶é—´åˆ†è¾¨çŽ‡": "æ¯æ—¥",
            "è¦†ç›–èŒƒå›´": "åŠ æ‹¿å¤§",
            "ä¼˜åŠ¿": "é•¿æœŸè®°å½•ã€é«˜ç²¾åº¦",
            "åŠ£åŠ¿": "ç«™ç‚¹ç¨€ç–",
            "äº’è¡¥æ€§": "æä¾›å¾„æµç›®æ ‡å˜é‡"
        }
    }
    
    print("ðŸ“‹ æ•°æ®æºç‰¹æ€§:")
    for source, info in data_sources.items():
        print(f"\nðŸ”¹ {source}:")
        for key, value in info.items():
            print(f"   - {key}: {value}")
    
    print("\nðŸŽ¯ äº’è¡¥æ€§åˆ†æž:")
    print("   - NASA MODIS + ECCCç§¯é›ª: ç©ºé—´è¦†ç›– + åœ°é¢éªŒè¯")
    print("   - ECCCç§¯é›ª + ECCCå¤©æ°”: ç§¯é›ªçŠ¶æ€ + èžåŒ–é©±åŠ¨")
    print("   - æ‰€æœ‰æ•°æ®æº: å®Œæ•´çš„æ°´æ–‡å¾ªçŽ¯å»ºæ¨¡")

def analyze_local_training_feasibility():
    """åˆ†æžæœ¬åœ°è®­ç»ƒå¯è¡Œæ€§"""
    
    print("\nðŸ’» æœ¬åœ°è®­ç»ƒå¯è¡Œæ€§åˆ†æž")
    print("=" * 60)
    
    # æ•°æ®é‡ä¼°ç®—
    data_volume = {
        "ECCCç§¯é›ª": "7306æ¡è®°å½• Ã— 6åˆ— Ã— 8å­—èŠ‚ â‰ˆ 350KB",
        "HYDATå¾„æµ": "326æ¡è®°å½• Ã— 4åˆ— Ã— 8å­—èŠ‚ â‰ˆ 10KB",
        "æ€»æ•°æ®é‡": "çº¦ 360KB"
    }
    
    print("ðŸ“Š æ•°æ®é‡ä¼°ç®—:")
    for item, volume in data_volume.items():
        print(f"   - {item}: {volume}")
    
    # æ¨¡åž‹å¤æ‚åº¦
    model_complexity = {
        "LSTMéšè—å±‚": "64ä¸ªç¥žç»å…ƒ",
        "è¾“å…¥ç‰¹å¾": "12ä¸ªå˜é‡",
        "åºåˆ—é•¿åº¦": "30å¤©",
        "å‚æ•°æ•°é‡": "çº¦ 50K-100K"
    }
    
    print("\nðŸ¤– æ¨¡åž‹å¤æ‚åº¦:")
    for item, complexity in model_complexity.items():
        print(f"   - {item}: {complexity}")
    
    # è®­ç»ƒèµ„æºéœ€æ±‚
    resource_requirements = {
        "å†…å­˜éœ€æ±‚": "ä½Ž (< 2GB)",
        "å­˜å‚¨éœ€æ±‚": "ä½Ž (< 100MB)",
        "è®¡ç®—éœ€æ±‚": "ä¸­ç­‰ (CPUè®­ç»ƒå¯è¡Œ)",
        "è®­ç»ƒæ—¶é—´": "ä¼°è®¡ 1-4å°æ—¶ (CPU)"
    }
    
    print("\nðŸ’¾ èµ„æºéœ€æ±‚:")
    for item, requirement in resource_requirements.items():
        print(f"   - {item}: {requirement}")
    
    print("\nâœ… ç»“è®º: æœ¬åœ°è®­ç»ƒå®Œå…¨å¯è¡Œï¼")
    print("   - æ•°æ®é‡å°ï¼Œé€‚åˆæœ¬åœ°å¤„ç†")
    print("   - æ¨¡åž‹å¤æ‚åº¦é€‚ä¸­ï¼ŒCPUè®­ç»ƒå¯è¡Œ")
    print("   - èµ„æºéœ€æ±‚ä½Žï¼Œæ— éœ€äº‘æœåŠ¡")

def provide_training_recommendations():
    """æä¾›è®­ç»ƒå»ºè®®"""
    
    print("\nðŸ’¡ è®­ç»ƒå»ºè®®")
    print("=" * 60)
    
    recommendations = [
        {
            "é˜¶æ®µ": "ç¬¬ä¸€é˜¶æ®µ",
            "ç›®æ ‡": "éªŒè¯æ•°æ®æµç¨‹",
            "å»ºè®®": "ä½¿ç”¨çº¢æ²³æµåŸŸæ•°æ®ï¼Œè®­ç»ƒç®€å•æ¨¡åž‹",
            "é¢„æœŸç»“æžœ": "ç¡®è®¤æ•°æ®è´¨é‡å’Œæ¨¡åž‹æ¡†æž¶"
        },
        {
            "é˜¶æ®µ": "ç¬¬äºŒé˜¶æ®µ", 
            "ç›®æ ‡": "ä¼˜åŒ–æ¨¡åž‹æ€§èƒ½",
            "å»ºè®®": "å¢žåŠ ç‰¹å¾å·¥ç¨‹ï¼Œè°ƒæ•´è¶…å‚æ•°",
            "é¢„æœŸç»“æžœ": "æé«˜é¢„æµ‹ç²¾åº¦"
        },
        {
            "é˜¶æ®µ": "ç¬¬ä¸‰é˜¶æ®µ",
            "ç›®æ ‡": "æ‰©å±•åº”ç”¨èŒƒå›´",
            "å»ºè®®": "æ‰©å±•åˆ°å…¶ä»–åŒºåŸŸï¼Œé›†æˆæ›´å¤šæ•°æ®æº",
            "é¢„æœŸç»“æžœ": "å»ºç«‹å®Œæ•´çš„é¢„æµ‹ç³»ç»Ÿ"
        }
    ]
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\nðŸŽ¯ é˜¶æ®µ {i}: {rec['é˜¶æ®µ']}")
        print(f"   ç›®æ ‡: {rec['ç›®æ ‡']}")
        print(f"   å»ºè®®: {rec['å»ºè®®']}")
        print(f"   é¢„æœŸ: {rec['é¢„æœŸç»“æžœ']}")
    
    print("\nðŸš€ ç«‹å³è¡ŒåŠ¨å»ºè®®:")
    print("   1. è¿è¡Œæ•°æ®éªŒè¯: python debug_data_sources.py")
    print("   2. å¯åŠ¨è®­ç»ƒæµç¨‹: python run_full_training.py")
    print("   3. ç›‘æŽ§è®­ç»ƒè¿›åº¦: æ£€æŸ¥ runs/ ç›®å½•")
    print("   4. è¯„ä¼°æ¨¡åž‹æ€§èƒ½: ä½¿ç”¨äº¤å‰éªŒè¯")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ðŸš€ HydrAI-SWE é¡¹ç›®æ•°æ®åˆ†æžæŠ¥å‘Š")
    print("=" * 60)
    
    # æ‰§è¡Œå„é¡¹åˆ†æž
    available_data = analyze_data_availability()
    analyze_training_data_volume()
    calculate_training_capacity()
    analyze_data_source_complementarity()
    analyze_local_training_feasibility()
    provide_training_recommendations()
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®åˆ†æžæŠ¥å‘Šå®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
